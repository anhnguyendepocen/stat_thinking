[
["index.html", "Statistical Reasoning through Computation and R Preface", " Statistical Reasoning through Computation and R Brandon LeBeau and Andrew S. Zieffler 2019-09-23 Preface This book provides a modern statistical reasoning and introduction to statistics text. Computation, using the R programming language, are used instead of relying on traditional statistical theory. "],
["introduction.html", "Chapter 1 Introduction 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure", " Chapter 1 Introduction Here is an intro. And more. 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure "],
["visualization.html", "Chapter 2 Visualization 2.1 Exploring Attributes 2.2 Plot Customization 2.3 Density plots", " Chapter 2 Visualization Data scientists and statisticians visualize data to explore and understand data. Visualization can help analysts identify features in the data such as typical or extreme observations and also for describe variation. Because it is so powerful, data visualiztion is often the first step in any statistical analysis. 2.0.1 College Scorecard Data The U.S. Department of Education publishes data on institutions of higher education in their College Scorecard (https://collegescorecard.ed.gov/) to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file College-scorecard-clean.csv. To illustrate some of the common methods statisticians use to visualize data, we will examine admissions rates for 2,019 institutions of higher education. Before we begin the analysis, we will load two packages, the tidyverse package and the ggformula package. These packages include many useful functions that we will use in this chapter. library(tidyverse) library(ggformula) There are many functions in R to import data. We will use the function read_csv() since the data file we are importing (College-scorecard-clean.csv) is a comma separated value (CSV) file..1 CSV files are a common format for storing data. Since they are encoded as text files they geerally do not take up a lot of space nor computer memory. They get their name from the fact that in the text file, each variable (i.e. column in the data) is separated by a comma within each row. The syntax to import the college scorecard data is as follows: colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) In this syntax we have passed two arguments to the read_csv() function. The first argument, file=, indicates the path to the data file. The data file here is stored on GitHub, so the path is specified as a URL. The second argument, guess_max=, helps ensure that the data are read in appropriately. This argument will be described in more detail later. The syntax to the left of the read_csv() function, namely colleges &lt;-, takes the output of the function and stores it, or in the language of R, assigns it to an object named colleges. In data analysis, it is often useful to use results in later computations, so rather than continually re-running syntax to obtain these results, we can instead store those results in an object and then compute on the object. Here for example, we would like to use the data that was read by the read_csv() function to explore it. When we want to assign computational results to an object, we use the assignment operator, &lt;- . (Note that the assignment operator looks like a left-pointing arrow; it is taking the computational result produced on the right side and storing it in the object to the left side.) 2.0.2 View the Data Once we have imported and assigned the data to an object, it is quite useful to ensure that it was read in appropriately. The head() function will give us a quick snapshot of the data by printing the first six rows of data. head(colleges) ## # A tibble: 6 x 16 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alaba… Norm… AL Bachel… South… City:… 0.903 18 4824 ## 2 Unive… Birm… AL Bachel… South… City:… 0.918 25 12866 ## 3 Unive… Hunt… AL Bachel… South… City:… 0.812 28 6917 ## 4 Alaba… Mont… AL Bachel… South… City:… 0.979 18 4189 ## 5 The U… Tusc… AL Bachel… South… City:… 0.533 28 32387 ## 6 Aubur… Mont… AL Bachel… South… City:… 0.825 22 4211 ## # … with 7 more variables: costt4_a &lt;dbl&gt;, costt4_p &lt;dbl&gt;, ## # tuitionfee_in &lt;dbl&gt;, tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, ## # grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt; 2.1 Exploring Attributes Data scientists and statisticians often start analyses by exploring attributes (i.e., variables) that are of interest to them. For example, suppose we are interested in exploring the admission rates of the institutions in the college scorecard data to determine how selective the different institutions are. We will begin our exploration of admission rates by examining different visualizations of the admissions rate attribute. There is not one perfect visulaiztion for exploring the data. Each visualization has pros and cons; it may highlight some features of the attribute and mask others. It is often necessary to look at many different visualizations of the data in the exploratory phase. 2.1.1 Histograms The first viualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histrogram() function. (This function is part of the ggformula package which needs to be loaded prior to using the gf_histogram() function.) This function requires two arguments. The first argument is a formula that identifies the variables to be plotted and the second argument, data=, specifies the data object we assigned earlier. The syntax used to create a histrogram of the admission rates is: gf_histogram(~ adm_rate, data = colleges) The formula we provide in the first argument is based on the following general structure: ~ attribute name where the attribute name identified to the right of the ~ is the exact name of one of the columns in the colleges data object. 2.1.2 Interpretting Histograms Histograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color=, in the gf_histogram() function. We can also set the color for the bins themselves using the fill= argument. Here we color the bin lines black and set the bin color to yellow.2 gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) Rather than focusing on any one bin, we typically want to describe the distribution as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.5 have higher counts than the bins that are below 0.5. There are, however, some institutions that are quite selective, only admitting fewer than 25% of the students who apply. 2.1.2.1 Adjusting Number of Bins Interpretation of the distribution is sometimes influenced by the width or number of bins. It is often useful to change the number of bins to explore the impact this may have on your interpretation. This can be accomplished by either (1) changing the width of the bins via thebinwidth= argument in the gf_histogram() function, or (2) changing the number of bins using the bins= argument. Below we show both methods: gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 10) gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, binwidth = .01) In general, our interpretation remains the same, namely that most institutions admit a high proportion of applicants. When we used a bin width of 0.01, however, we were able to see that several institutions admit 100% of applicants. This was obscured in the other histograms we examined. As a data scientist these institutions might warrant a more nuanced examination. 2.2 Plot Customization There are many ways to further customize the plot we produced to make it more appealing. For example, you might want to change the label on the x-axis from adm_rate to something more informative. Or, you may want to add a descriptive title to your plot. These customizations can be specified using the gf_labs() function. 2.2.1 Axes labels To change the labels on the x- and y-axes, we can use the arguments x= and y= in the gf_labs() function. These arguments take the text for the label you want to add to each axis, respectively. Here we change the text on the x-axis to “Admission Rate” and the text on the y-axis to “Frequency”. The gf_labs() function is connected to the histogram by linking the gf_histogram() and gf_labs() functions with the pipe operator (%&gt;%). gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39; ) 2.2.2 Plot title and subtitle We can also add a title and subtitle to our plot. Similar to changing the axis labels, these are added using gf_labs(), but using the title= and subtitle= arguments. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) 2.2.3 Plot theme By default, the plot has a grey background and white grid lines. This can be modified to using the gf_theme() function. For example, in the syntax below we change the plot theme to a white background with no grid lines using theme_classic(). Again, the gf_theme() is linked to the histogram with the pipe operator. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_classic()) We have created a custom theme to use in the gf_theme() function that we will use for most of the plots in the book. The theme, theme_statthinking() is included in the statthink library, a supplemental package to the text that can be installed and loaded with the following commands: remotes::install_github(&#39;lebebr01/statthink&#39;) ## Skipping install of &#39;statthink&#39; from a github remote, the SHA1 (e9837dfb) has not changed since last install. ## Use `force = TRUE` to force installation library(statthink) We can then change the theme in a similar manner to how we changed the theme before. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_statthinking()) 2.2.3.1 Setting the default plot theme Since we will be using this theme for all of our plots, it is useful to make it the default theme (rather than the grey bckground with white gridlines). To set a different theme as the default, we will use the theme_set() function and call our theme_statthinking() within this function. theme_set(theme_statthinking()) Now when we create a plot, it will automatically use the statthinking theme without having to specify this in the gf_theme() function. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) 2.3 Density plots Another plot that is useful for exploring attributes is the density plot. This plot usually highlights similar distributional features as the histogram, but the visualization does not have the same dependency on the specification of bins. Density plots can be created with the gf_density() function which takes similar arguments as gf_histogram(), namely a formula identifying the attribute to be plotted and the data object.3 gf_density(~ adm_rate, data = colleges) Our interpretation remains that most institutions admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density. The axis labels, title, subtitle can be customized with gf_labs() in the same manner as with the histogram. The color= and fill= arguments in gf_density() will color the density curve and area under the density curve, respectively. gf_density(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Probability density&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) This function is a part of the tidyverse package, so you need to be sure to run library(tidyverse) prior to using read_csv().↩ R knows the names of 657 colors. To see these names type colors() at the command prompt.↩ The default kernel used in gf_density() is the normal kernel.↩ "],
["descriptive-statistics-applying-functions-to-columns-of-data.html", "Chapter 3 Descriptive Statistics / Applying functions to columns of data 3.1 Applying Functions to Data 3.2 Setup 3.3 Functions to columns of data 3.4 Considering Groups 3.5 Other statistics of center 3.6 Measures of Variation 3.7 Other measures of variation", " Chapter 3 Descriptive Statistics / Applying functions to columns of data 3.1 Applying Functions to Data Data visualization is often the first step on the statistical journey to explore a research question. However, this is usually not where the journey stops, instead additional analyses are often performed to learn more about the average trends seen in the data. These can often be split into two broad categories, Descriptive Statistics Inferential Statistics Descriptive Statistics help to describe the data and are particularly useful to give a single numeric summary for a single variable. We will explore this idea more fully in this section. Inferential Statistics help us to make broader statements from the data we have to the larger group of interest, commonly referred to as the population. More details on these steps later in the course. 3.2 Setup We are going to use some real data about higher education institutions from the college scorecard (https://collegescorecard.ed.gov/) to explore the types of conclusions we can make from the data. The college scorecard releases data on higher education institutions to help make the institutions more transparent and provide a place for parents, students, educators, etc can get information about specific instituations from a third party (i.e. US Department of Education). 3.2.1 Loading R packages library(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggformula) ## Loading required package: ggstance ## ## Attaching package: &#39;ggstance&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(&quot;introduction&quot;, package = &quot;ggformula&quot;) ## learnr::run_tutorial(&quot;refining&quot;, package = &quot;ggformula&quot;) library(mosaic) ## Loading required package: lattice ## Loading required package: mosaicData ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## The &#39;mosaic&#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Note: If you use the Matrix package, be sure to load it BEFORE loading mosaic. ## ## Attaching package: &#39;mosaic&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## mean ## The following objects are masked from &#39;package:dplyr&#39;: ## ## count, do, tally ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## stat ## The following objects are masked from &#39;package:stats&#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, ## prop.test, quantile, sd, t.test, var ## The following objects are masked from &#39;package:base&#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw()) 3.2.2 Read in Data The below code will read in the data for us to use in the future. The R function to read in the data is read_csv(). Function arguments are passed within the parentheses and for the read_csv() function the first argument is the path to the data. The data for this example are posted on GitHub in a comma separated file. This means the data is stored in a text format and each variable (i.e. column in the data) is separated by a comma. This is a common format data is stored. The data is stored to an object named college_score. In R (and other statistical programming languages), it is common to use objects to store results to use later. In this instance, we would like to read in the data and store it to use it later. For example, we will likely want to explore the data visually to see if we can extract some trends from the data. The assignment to an object in R is done with the &lt;- assignment operator. Finally, there is one additional argument, guess_max which helps to ensure that the data are read in appropriately. More on this later. college_score &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000) ## Parsed with column specification: ## cols( ## instnm = col_character(), ## city = col_character(), ## stabbr = col_character(), ## preddeg = col_character(), ## region = col_character(), ## locale = col_character(), ## adm_rate = col_double(), ## actcmmid = col_double(), ## ugds = col_double(), ## costt4_a = col_double(), ## costt4_p = col_double(), ## tuitionfee_in = col_double(), ## tuitionfee_out = col_double(), ## debt_mdn = col_double(), ## grad_debt_mdn = col_double(), ## female = col_double() ## ) head(college_score) ## # A tibble: 6 x 16 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alaba… Norm… AL Bachel… South… City:… 0.903 18 4824 ## 2 Unive… Birm… AL Bachel… South… City:… 0.918 25 12866 ## 3 Unive… Hunt… AL Bachel… South… City:… 0.812 28 6917 ## 4 Alaba… Mont… AL Bachel… South… City:… 0.979 18 4189 ## 5 The U… Tusc… AL Bachel… South… City:… 0.533 28 32387 ## 6 Aubur… Mont… AL Bachel… South… City:… 0.825 22 4211 ## # … with 7 more variables: costt4_a &lt;dbl&gt;, costt4_p &lt;dbl&gt;, ## # tuitionfee_in &lt;dbl&gt;, tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, ## # grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt; 3.3 Functions to columns of data Data are often stored in a tabular format where the rows of the data are the units and the columns in a data frame are the varaibles. This is shown in the college scorecard data above where the rows of the data are specific institutions of higher education and the columns represent various attributes about those higher education institutions. This is a common structure to store data where each row represents a unique unit or measurement occasion for longitudinal data. In the data visualization units, we accessed columns of data to view the distribution of the particular variable. For example, we explore histograms of admission rate. Instead of visualizing the data, now we will apply functions to these columns to calculate statistics of interest. In particular, the focus will be on the calculating statistics for variables that are numeric rather than representing categories. We will discuss this in more detail as we move along. Let’s keep talking about the admission rate as we have explored that visually already and start with an example. df_stats(~ adm_rate, data = college_score, median) ## median_adm_rate ## 1 0.7077 The df_stats() function takes a formula syntax that is similar to the syntax used for viewing a univariate distribution you saw earlier. In particular, the variable that we wish to compute a statistic on is specified after the ~. The next argument is the data argument. Finally, subsequent arguments after the data argument are functions that we want to compute for the variable specified. Here, I compute the median which happens to be, 0.708. The median is also referred to as the 50% percentile and is the location where half of the data (in this case higher education institutions) are above and below an admission rate of 70.8%. Let’s think where this shows up on the admission rate distribution we plotted earlier. gf_histogram(~ adm_rate, data = college_score, bins = 30) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~df_stats(~ adm_rate, data = college_score, median)[[1]], size = 1) You’ll notice that the line is just to the left of the main peak of the data. Does it appear that half of the data are below and half are above the blue line in the figure? The median is a special percentile, however other percentiles may be of interest. For example, maybe we’d want to know what the 20th percentile is or the 80th percentile to apply to a school that isn’t too selective or is not selective at all. We can compute these with the df_stats() function again. q &lt;- college_score %&gt;% df_stats(~ adm_rate, quantile(c(0.2, 0.5, 0.8)), nice_names = TRUE) q ## X20. X50. X80. ## 1 0.51428 0.7077 0.86932 Let’s look where these fall on our distribution. gf_histogram(~ adm_rate, data = college_score, bins = 30) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~ value, data = gather(q), size = 1) Does it appear that 20% of the data are below the first line and 20% are above the last line? Difficult to view on the histogram. An empirical distribution figure, sometimes called an ogive, can be helpful to show these. gf_ecdf(~ adm_rate, data = college_score) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~ value, data = gather(q), linetype = 2) %&gt;% gf_hline(color = &#39;darkblue&#39;, yintercept = ~c(0.2, 0.5, 0.8), data = NA, linetype = 3) %&gt;% gf_labs(y = &#39;Cumulative proportion&#39;) Here you can see that the horizontal lines cross over the vertical lines at the specified values (i.e. 20% for the first vertical line, 50% for the second vertical line, 80% for the final vertical line). 3.4 Considering Groups We’ve spent a lot of time trying to reason about other variables that may be important in explaining variation in our variable of interest. So far we have only explored the variable without considering other variables, in practice that is not that useful. Instead, it is common to compute conditional statistics based on other characteristics in the data. An example may help to show the idea more clearly. college_score %&gt;% df_stats(adm_rate ~ region, median) ## region median_adm_rate ## 1 Far West 0.70360 ## 2 Great Lakes 0.71110 ## 3 Mid East 0.73735 ## 4 New England 0.73590 ## 5 Outlying Areas 0.81160 ## 6 Plains 0.69600 ## 7 Rocky Mountains 0.82865 ## 8 Southeast 0.65965 ## 9 Southwest 0.71220 ## 10 US Service Schools 0.10740 Presented above are the conditional medians for the higher education institutions in different areas of the country. More specifically, the data are essentially split into subgroups and the median is computed for each of those subgroups instead of pooling all institutions into a single data frame. The formula syntax is now outcome ~ grouping where the variable of interest (i.e. commonly a numeric variable) and the variable to the right of the ~ is the grouping variable. This syntax is similar to the violin plots that were created earlier. Can you see differences in the admission rates across the regions? One thing that is useful to add in when computing conditional statisics, is how many data points are in each group. This is particularly useful when the groups are different sizes, which is common. To do this, we can add another function to the df_stats() function. college_score %&gt;% df_stats(adm_rate ~ region, median, length) ## region median_adm_rate length_adm_rate ## 1 Far West 0.70360 221 ## 2 Great Lakes 0.71110 297 ## 3 Mid East 0.73735 458 ## 4 New England 0.73590 167 ## 5 Outlying Areas 0.81160 35 ## 6 Plains 0.69600 200 ## 7 Rocky Mountains 0.82865 50 ## 8 Southeast 0.65965 454 ## 9 Southwest 0.71220 133 ## 10 US Service Schools 0.10740 4 This adds another columns which represents the number of observations that went into the median calculation for each group. The syntax above also shows that you can add additional functions separated by a comma in the df_stats() function and are not limited to a single function. We will take advantage of this feature later on. 3.4.1 Adding additional groups What if we thought more than one variable was important in explaining variation in the outcome variable? These can also be added to the df_stats() function for additional conditional statistics. The key is to add another variable to the right-hand side of the formula argument. More than one variable are separated with a + symbol. college_score %&gt;% df_stats(adm_rate ~ region + preddeg, median, length) ## region preddeg median_adm_rate length_adm_rate ## 1 Far West Associate Degree 0.58935 12 ## 2 Great Lakes Associate Degree 0.79515 22 ## 3 Mid East Associate Degree 0.80290 54 ## 4 New England Associate Degree 0.78430 7 ## 5 Outlying Areas Associate Degree 0.87040 6 ## 6 Plains Associate Degree 0.74610 8 ## 7 Rocky Mountains Associate Degree 0.83970 5 ## 8 Southeast Associate Degree 0.72525 34 ## 9 Southwest Associate Degree 0.42260 5 ## 10 Far West Bachelor Degree 0.69260 172 ## 11 Great Lakes Bachelor Degree 0.70560 252 ## 12 Mid East Bachelor Degree 0.70790 340 ## 13 New England Bachelor Degree 0.72980 145 ## 14 Outlying Areas Bachelor Degree 0.70905 24 ## 15 Plains Bachelor Degree 0.68330 173 ## 16 Rocky Mountains Bachelor Degree 0.82865 40 ## 17 Southeast Bachelor Degree 0.64180 393 ## 18 Southwest Bachelor Degree 0.71975 116 ## 19 US Service Schools Bachelor Degree 0.10740 4 ## 20 Far West Certificate Degree 0.76470 37 ## 21 Great Lakes Certificate Degree 0.83960 23 ## 22 Mid East Certificate Degree 0.79505 64 ## 23 New England Certificate Degree 0.81820 15 ## 24 Outlying Areas Certificate Degree 1.00000 5 ## 25 Plains Certificate Degree 0.82140 19 ## 26 Rocky Mountains Certificate Degree 0.67200 5 ## 27 Southeast Certificate Degree 0.75700 27 ## 28 Southwest Certificate Degree 0.72365 12 3.5 Other statistics of center So far we have been discussing the median. The median attempts to provide a single number summary for the center of the distribution. It is a robust statistic, but likely isn’t the most popular statistic to provide a location for the center of a distribution. The mean is often more commonly used as a measure of the center of a distribution. Part of this is due to the usage of the mean in common statistical methods and the mean also uses the values of all the data in the calculation. The median only considers the values of the middle score or scores, therefore this statistic is less sensitive to extreme values than the mean. I like to look at both statistics and this can provide some insight into the distribution of interest. We can add the mean using the df_stats() function by adding the function mean. stats_compute &lt;- college_score %&gt;% df_stats(adm_rate ~ region, median, mean, length) stats_compute ## region median_adm_rate mean_adm_rate length_adm_rate ## 1 Far West 0.70360 0.6682570 221 ## 2 Great Lakes 0.71110 0.7015263 297 ## 3 Mid East 0.73735 0.6920234 458 ## 4 New England 0.73590 0.6672838 167 ## 5 Outlying Areas 0.81160 0.7885600 35 ## 6 Plains 0.69600 0.7000135 200 ## 7 Rocky Mountains 0.82865 0.7800680 50 ## 8 Southeast 0.65965 0.6560097 454 ## 9 Southwest 0.71220 0.6696759 133 ## 10 US Service Schools 0.10740 0.1302000 4 Do you notice any trends in the direction the mean and median typically follow? More specifically, is the mean typically larger than the median or vice versa? Let’s visualize them. gf_histogram(~ adm_rate, data = college_score, bins = 30) %&gt;% gf_facet_wrap(~ region) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~ median_adm_rate, data = stats_compute, size = 1) %&gt;% gf_vline(color = &#39;lightblue&#39;, xintercept = ~ mean_adm_rate, data = stats_compute, size = 1) What is different about the distributions that have larger differences in the mean and median? 3.6 Measures of Variation So far we have focused primarily on applying functions to columns of data to provide a single numeric summary for where the center of the distribution may lie. The center of the distribution is important, however the primary goal in research and with statistics is to try to understand the variation in the distribution. One crude measure of variation that is intuitive is the range of a variable. The range is the difference between the smallest and the largest number in the data. We can compute this with the df_stats() function. college_score %&gt;% df_stats(~ adm_rate, range) ## range_adm_rate_1 range_adm_rate_2 ## 1 0 1 The details of the df_stats() function are in the previous course notes. The output for this computation returns two values, the minimum and maximum value in the data and unsurprisingly, is 0 and 1 respectively. The range is most useful as a data checking process to ensure that the variable contains values that are theoretically possible, which is true in this case. The range is known as a biased statistic in that it will almost always be smaller than the population value. Therefore, we would like a better statistic for measures of variation. 3.6.1 Robust measure of variation A robust measure of variation that often is used in tandem with the median is the interquartile range (IQR). This statistic can be calculated in two ways, either using the IQR() or quantile() function. Both are presented below. college_score %&gt;% df_stats(~ adm_rate, IQR, quantile(c(0.25, 0.75)), nice_names = TRUE) ## IQR_adm_rate X25. X75. ## 1 0.28575 0.5524 0.83815 The IQR is the difference between the 75th and 25th percentiles and in this example equals 0.285 or about 28.5%. As the IQR represents differences in percentiles, we could say that the middle 50% of the distribution is found between 55% and 84% and the middle 50% is spread out by about 28.5%. The idea behind the IQR representing differences in percentiles allows us to extend this to different percentiles that may be more directly interpretable for a given situation. For example, suppose we wanted to know how spread out the middle 80% of the distribution is. We can do this directly by computing the 90th and 10th percentiles and finding the difference between the two. mid_80 &lt;- college_score %&gt;% df_stats(~ adm_rate, quantile(c(0.1, 0.9)), nice_names = TRUE) mid_80 ## X10. X90. ## 1 0.39284 0.94706 As you can see, once you extend the amount of the distribution contained, the distance increases, now to 0.555 or 55.5% the the range of the middle 80% of the admission rate distribution. We can also visualize what this looks like. gf_histogram(~ adm_rate, data = college_score, bins = 30, color = &#39;black&#39;) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~ value, data = gather(mid_80), size = 1) We can also view the exact percentages using the empirical cumulative density function. gf_ecdf(~ adm_rate, data = college_score) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~ value, data = gather(mid_80), size = 1) 3.6.2 Variation by Group These statistics can also be calculated by different grouping variables similar to what was done with statisitcs of center. Now the variable of interest is on the left-hand side of the equation and the grouping variable is on the right hand side. iqr_groups &lt;- college_score %&gt;% df_stats(adm_rate ~ region, IQR, quantile(c(0.25, 0.75)), nice_names = TRUE) iqr_groups ## region IQR_adm_rate X25. X75. ## 1 Far West 0.306700 0.52480 0.831500 ## 2 Great Lakes 0.220300 0.60580 0.826100 ## 3 Mid East 0.279425 0.57140 0.850825 ## 4 New England 0.288000 0.54525 0.833250 ## 5 Outlying Areas 0.318600 0.64135 0.959950 ## 6 Plains 0.272200 0.56990 0.842100 ## 7 Rocky Mountains 0.281950 0.64590 0.927850 ## 8 Southeast 0.286950 0.52525 0.812200 ## 9 Southwest 0.311400 0.51900 0.830400 ## 10 US Service Schools 0.052000 0.09280 0.144800 This can also be visualized to see how these statistics vary across the groups. gf_histogram(~ adm_rate, data = college_score, bins = 30, color = &#39;black&#39;) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = ~ value, data = filter(pivot_longer(iqr_groups, IQR_adm_rate:&#39;X75.&#39;), name %in% c(&#39;X25.&#39;, &#39;X75.&#39;)), size = 1) %&gt;% gf_facet_wrap(~ region) 3.7 Other measures of variation There are many other variation measures that are used in statistics. We will apply a functional approach to these and try to visualize what they are trying to represent. The statistics discussed here represent deviations from the mean, either the average absolute deviation or the average squared deviation. college_score %&gt;% df_stats(~ adm_rate, sd, var) ## sd_adm_rate var_adm_rate ## 1 0.2113571 0.04467182 In order to compute the mean absolute error, we first need to define a new function. mae &lt;- function(x, na.rm = TRUE, ...) { avg &lt;- mean(x, na.rm = na.rm, ...) abs_avg &lt;- abs(x - avg) mean(abs_avg) } We can now use this new function just like any other function. college_score %&gt;% df_stats(~ adm_rate, sd, var, mae) ## sd_adm_rate var_adm_rate mae_adm_rate ## 1 0.2113571 0.04467182 0.1692953 "],
["multivariate-visualization.html", "Chapter 4 Multivariate Visualization 4.1 Facetting", " Chapter 4 Multivariate Visualization library(tidyverse) library(ggformula) library(statthink) # Import data colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) Real world data are never as simple exploring a distribution of a single variable, particularly when trying to understand individual variation. In most cases things interact, move in tandem, and many phenomena help to explain the variable of interest. For example, when thinking about admission rates, what may be some important factors that would explain some of the reasons why higher education institutions differ in their admission rates? Take a few minutes to brainstorm some ideas. gf_histogram(~ adm_rate, data = colleges, bins = 30, fill = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, fill = &quot;Primary Deg&quot;) Often density plots are easier to visualize when there are more than one group. To plot more than one density curve, we need to specify the color argument instead of fill. gf_density(~ adm_rate, data = colleges, color = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, color = &quot;Primary Deg&quot;) gf_density(~ adm_rate, data = colleges, fill = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, fill = &quot;Primary Deg&quot;) gf_density(~ adm_rate, data = colleges, fill = ~ preddeg, color = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, color = &quot;Primary Deg&quot;, fill = &quot;Primary Deg&quot;) gf_density(~ adm_rate, data = colleges, color = ~ preddeg, fill = &#39;gray85&#39;, size = 1) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, color = &quot;Primary Deg&quot;) ## Violin Plots Violin plots are another way to make comparisons of distributions across groups. Violin plots are also easier to show more groups on a single graph. Violin plots are density plots that are mirrored to be fully enclosed. Best to explore with an example.ArithmeticError gf_violin(adm_rate ~ preddeg, data = colleges) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;Primary Deg&quot;) Aesthetically, these figures are a bit more pleasing to look at if they include a light fill color. This is done similar to the density plots shown above with the fill = argument.ArithmeticError gf_violin(adm_rate ~ preddeg, data = colleges, fill = &#39;gray85&#39;) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;Primary Deg&quot;) Adding quantiles are useful to aid in the comparison with the violin plots. These can be added with the draw_quantiles argument. gf_violin(adm_rate ~ preddeg, data = colleges, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;Primary Deg&quot;) ### Violin Plots with many groups Many groups are more easily shown in the violin plot framework. With many groups, it is often of interest to put the long x-axis labels representing each group on the y-axis so that it reads the correct direction and the labels do not run into each other. This can be done with the gf_refine() function with coord_flip(). gf_violin(adm_rate ~ region, data = colleges, fill = &#39;gray80&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;US Region&quot;) %&gt;% gf_refine(coord_flip()) 4.1 Facetting Facetting is another way to explore distributions of two or more variables. gf_violin(adm_rate ~ region, data = colleges, fill = &#39;gray80&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;US Region&quot;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_facet_wrap(~ preddeg) "],
["classification.html", "Chapter 5 Classification 5.1 Topic: Decision Trees - bar charts of percentage of correct classification", " Chapter 5 Classification 5.1 Topic: Decision Trees - bar charts of percentage of correct classification 5.1.1 Performance Measures 5.1.2 Comparison to Baseline 5.1.2.1 Absolute vs Relative Comparison 5.1.3 Overfitting 5.1.4 Cross-validation "],
["linear-model.html", "Chapter 6 Linear Model 6.1 Simple Regression continuous predictor 6.2 Conditional Means 6.3 Categorical Predictor(s) 6.4 Multiple Regression", " Chapter 6 Linear Model 6.1 Simple Regression continuous predictor 6.2 Conditional Means 6.3 Categorical Predictor(s) 6.4 Multiple Regression "],
["estimation-bootstrap-uncertainty.html", "Chapter 7 Estimation / Bootstrap / Uncertainty", " Chapter 7 Estimation / Bootstrap / Uncertainty "],
["prediction-for-individuals.html", "Chapter 8 Prediction for individuals 8.1 Comparison of classification / linear model", " Chapter 8 Prediction for individuals 8.1 Comparison of classification / linear model "]
]
