[
["index.html", "Statistical Reasoning through Computation and R Preface", " Statistical Reasoning through Computation and R Brandon LeBeau and Andrew S. Zieffler 2019-10-17 Preface This book provides a modern statistical reasoning and introduction to statistics text. Computation, using the R programming language, are used instead of relying on traditional statistical theory. These analyses can often be categorized into two broad categories, Descriptive Statistics Inferential Statistics Descriptive Statistics help to describe the data and are particularly useful to give a single numeric summary for a single variable. We will explore this idea more fully in this section. Inferential Statistics help us to make broader statements from the data we have to the larger group of interest, commonly referred to as the population. More details on these steps later in the course. "],
["introduction.html", "Chapter 1 Introduction 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure", " Chapter 1 Introduction Here is an intro. And more. 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure "],
["visualization.html", "Chapter 2 Visualization 2.1 Exploring Attributes 2.2 Plot Customization 2.3 Density plots", " Chapter 2 Visualization Data scientists and statisticians visualize data to explore and understand data. Visualization can help analysts identify features in the data such as typical or extreme observations and also for describe variation. Because it is so powerful, data visualiztion is often the first step in any statistical analysis. 2.0.1 College Scorecard Data The U.S. Department of Education publishes data on institutions of higher education in their College Scorecard (https://collegescorecard.ed.gov/) to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file College-scorecard-clean.csv. To illustrate some of the common methods statisticians use to visualize data, we will examine admissions rates for 2,019 institutions of higher education. Before we begin the analysis, we will load two packages, the tidyverse package and the ggformula package. These packages include many useful functions that we will use in this chapter. library(tidyverse) library(ggformula) There are many functions in R to import data. We will use the function read_csv() since the data file we are importing (College-scorecard-clean.csv) is a comma separated value (CSV) file..1 CSV files are a common format for storing data. Since they are encoded as text files they geerally do not take up a lot of space nor computer memory. They get their name from the fact that in the text file, each variable (i.e. column in the data) is separated by a comma within each row. The syntax to import the college scorecard data is as follows: colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) In this syntax we have passed two arguments to the read_csv() function. The first argument, file=, indicates the path to the data file. The data file here is stored on GitHub, so the path is specified as a URL. The second argument, guess_max=, helps ensure that the data are read in appropriately. This argument will be described in more detail later. The syntax to the left of the read_csv() function, namely colleges &lt;-, takes the output of the function and stores it, or in the language of R, assigns it to an object named colleges. In data analysis, it is often useful to use results in later computations, so rather than continually re-running syntax to obtain these results, we can instead store those results in an object and then compute on the object. Here for example, we would like to use the data that was read by the read_csv() function to explore it. When we want to assign computational results to an object, we use the assignment operator, &lt;- . (Note that the assignment operator looks like a left-pointing arrow; it is taking the computational result produced on the right side and storing it in the object to the left side.) 2.0.2 View the Data Once we have imported and assigned the data to an object, it is quite useful to ensure that it was read in appropriately. The head() function will give us a quick snapshot of the data by printing the first six rows of data. head(colleges) ## # A tibble: 6 x 17 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alaba… Norm… AL Bachel… South… City:… 0.903 18 4824 ## 2 Unive… Birm… AL Bachel… South… City:… 0.918 25 12866 ## 3 Unive… Hunt… AL Bachel… South… City:… 0.812 28 6917 ## 4 Alaba… Mont… AL Bachel… South… City:… 0.979 18 4189 ## 5 The U… Tusc… AL Bachel… South… City:… 0.533 28 32387 ## 6 Aubur… Mont… AL Bachel… South… City:… 0.825 22 4211 ## # … with 8 more variables: costt4_a &lt;dbl&gt;, costt4_p &lt;dbl&gt;, ## # tuitionfee_in &lt;dbl&gt;, tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, ## # grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt;, bachelor_degree &lt;dbl&gt; 2.1 Exploring Attributes Data scientists and statisticians often start analyses by exploring attributes (i.e., variables) that are of interest to them. For example, suppose we are interested in exploring the admission rates of the institutions in the college scorecard data to determine how selective the different institutions are. We will begin our exploration of admission rates by examining different visualizations of the admissions rate attribute. There is not one perfect visulaiztion for exploring the data. Each visualization has pros and cons; it may highlight some features of the attribute and mask others. It is often necessary to look at many different visualizations of the data in the exploratory phase. 2.1.1 Histograms The first viualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histrogram() function. (This function is part of the ggformula package which needs to be loaded prior to using the gf_histogram() function.) This function requires two arguments. The first argument is a formula that identifies the variables to be plotted and the second argument, data=, specifies the data object we assigned earlier. The syntax used to create a histrogram of the admission rates is: gf_histogram(~ adm_rate, data = colleges) The formula we provide in the first argument is based on the following general structure: ~ attribute name where the attribute name identified to the right of the ~ is the exact name of one of the columns in the colleges data object. 2.1.2 Interpretting Histograms Histograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color=, in the gf_histogram() function. We can also set the color for the bins themselves using the fill= argument. Here we color the bin lines black and set the bin color to yellow.2 gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) Rather than focusing on any one bin, we typically want to describe the distribution as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.5 have higher counts than the bins that are below 0.5. There are, however, some institutions that are quite selective, only admitting fewer than 25% of the students who apply. 2.1.2.1 Adjusting Number of Bins Interpretation of the distribution is sometimes influenced by the width or number of bins. It is often useful to change the number of bins to explore the impact this may have on your interpretation. This can be accomplished by either (1) changing the width of the bins via thebinwidth= argument in the gf_histogram() function, or (2) changing the number of bins using the bins= argument. Below we show both methods: gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 10) gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, binwidth = .01) In general, our interpretation remains the same, namely that most institutions admit a high proportion of applicants. When we used a bin width of 0.01, however, we were able to see that several institutions admit 100% of applicants. This was obscured in the other histograms we examined. As a data scientist these institutions might warrant a more nuanced examination. 2.2 Plot Customization There are many ways to further customize the plot we produced to make it more appealing. For example, you might want to change the label on the x-axis from adm_rate to something more informative. Or, you may want to add a descriptive title to your plot. These customizations can be specified using the gf_labs() function. 2.2.1 Axes labels To change the labels on the x- and y-axes, we can use the arguments x= and y= in the gf_labs() function. These arguments take the text for the label you want to add to each axis, respectively. Here we change the text on the x-axis to “Admission Rate” and the text on the y-axis to “Frequency”. The gf_labs() function is connected to the histogram by linking the gf_histogram() and gf_labs() functions with the pipe operator (%&gt;%). gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39; ) 2.2.2 Plot title and subtitle We can also add a title and subtitle to our plot. Similar to changing the axis labels, these are added using gf_labs(), but using the title= and subtitle= arguments. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) 2.2.3 Plot theme By default, the plot has a grey background and white grid lines. This can be modified to using the gf_theme() function. For example, in the syntax below we change the plot theme to a white background with no grid lines using theme_classic(). Again, the gf_theme() is linked to the histogram with the pipe operator. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_classic()) We have created a custom theme to use in the gf_theme() function that we will use for most of the plots in the book. The theme, theme_statthinking() is included in the statthink library, a supplemental package to the text that can be installed and loaded with the following commands: remotes::install_github(&#39;lebebr01/statthink&#39;) ## Skipping install of &#39;statthink&#39; from a github remote, the SHA1 (98b0498e) has not changed since last install. ## Use `force = TRUE` to force installation library(statthink) ## ## Attaching package: &#39;statthink&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## colleges We can then change the theme in a similar manner to how we changed the theme before. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_statthinking()) 2.2.3.1 Setting the default plot theme Since we will be using this theme for all of our plots, it is useful to make it the default theme (rather than the grey bckground with white gridlines). To set a different theme as the default, we will use the theme_set() function and call our theme_statthinking() within this function. theme_set(theme_statthinking()) Now when we create a plot, it will automatically use the statthinking theme without having to specify this in the gf_theme() function. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) 2.3 Density plots Another plot that is useful for exploring attributes is the density plot. This plot usually highlights similar distributional features as the histogram, but the visualization does not have the same dependency on the specification of bins. Density plots can be created with the gf_density() function which takes similar arguments as gf_histogram(), namely a formula identifying the attribute to be plotted and the data object.3 gf_density(~ adm_rate, data = colleges) Our interpretation remains that most institutions admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density. The axis labels, title, subtitle can be customized with gf_labs() in the same manner as with the histogram. The color= and fill= arguments in gf_density() will color the density curve and area under the density curve, respectively. gf_density(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Probability density&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) This function is a part of the tidyverse package, so you need to be sure to run library(tidyverse) prior to using read_csv().↩ R knows the names of 657 colors. To see these names type colors() at the command prompt.↩ The default kernel used in gf_density() is the normal kernel.↩ "],
["descriptive-statistics-numerically-describing-the-sample-data.html", "Chapter 3 Descriptive Statistics: Numerically Describing the Sample Data 3.1 Summarizing Attributes 3.2 Visualizing the Median and Mean 3.3 Numerically Summarizing Variation 3.4 Summarizing Categorical Attributes 3.5 Advanced Extension: Computing Your Own Measure of Variation", " Chapter 3 Descriptive Statistics: Numerically Describing the Sample Data Data visualization is often the first step on the statistical journey to explore a research question. However, this is usually not where the journey stops, instead additional analyses are often performed to learn more about the trends and structure in the data. In this chapter we will learn about methods that useful for numerically summarizing a sample of data. These methods are commonly referred to as descriptive statistics. We will again use the data provided in the file College-scorecard-clean.csv to examine admissions rates for 2,019 institutions of higher education. As in the previous chapter, before we begin the analysis, we will load several packages that include functions we will use in the chapter. We also import the College Scorecard data using the read_csv() function. # Load packages library(tidyverse) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggformula) ## Loading required package: ggstance ## ## Attaching package: &#39;ggstance&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(&quot;introduction&quot;, package = &quot;ggformula&quot;) ## learnr::run_tutorial(&quot;refining&quot;, package = &quot;ggformula&quot;) library(mosaic) ## Loading required package: lattice ## Loading required package: mosaicData ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Registered S3 method overwritten by &#39;mosaic&#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The &#39;mosaic&#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Note: If you use the Matrix package, be sure to load it BEFORE loading mosaic. ## ## Attaching package: &#39;mosaic&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## mean ## The following objects are masked from &#39;package:dplyr&#39;: ## ## count, do, tally ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## stat ## The following objects are masked from &#39;package:stats&#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, ## prop.test, quantile, sd, t.test, var ## The following objects are masked from &#39;package:base&#39;: ## ## max, mean, min, prod, range, sample, sum library(statthink) # Set theme for plots theme_set(theme_statthinking()) # Import the data colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) ## Parsed with column specification: ## cols( ## instnm = col_character(), ## city = col_character(), ## stabbr = col_character(), ## preddeg = col_character(), ## region = col_character(), ## locale = col_character(), ## adm_rate = col_double(), ## actcmmid = col_double(), ## ugds = col_double(), ## costt4_a = col_double(), ## costt4_p = col_double(), ## tuitionfee_in = col_double(), ## tuitionfee_out = col_double(), ## debt_mdn = col_double(), ## grad_debt_mdn = col_double(), ## female = col_double(), ## bachelor_degree = col_double() ## ) # View first six cases head(colleges) ## # A tibble: 6 x 17 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alaba… Norm… AL Bachel… South… City:… 0.903 18 4824 ## 2 Unive… Birm… AL Bachel… South… City:… 0.918 25 12866 ## 3 Unive… Hunt… AL Bachel… South… City:… 0.812 28 6917 ## 4 Alaba… Mont… AL Bachel… South… City:… 0.979 18 4189 ## 5 The U… Tusc… AL Bachel… South… City:… 0.533 28 32387 ## 6 Aubur… Mont… AL Bachel… South… City:… 0.825 22 4211 ## # … with 8 more variables: costt4_a &lt;dbl&gt;, costt4_p &lt;dbl&gt;, ## # tuitionfee_in &lt;dbl&gt;, tuitionfee_out &lt;dbl&gt;, debt_mdn &lt;dbl&gt;, ## # grad_debt_mdn &lt;dbl&gt;, female &lt;dbl&gt;, bachelor_degree &lt;dbl&gt; 3.1 Summarizing Attributes Data are often stored in a tabular format where the rows of the data are the cases and the columns are attributes. For example, in the college scorecard data (displayed above) the rows each represent a specific institution of higher education (cases) and the columns represent various attributes measured on those higher education institutions. This type of tabular representation is a common structure for storing and analyzing data. In the previous chapter, we visualized different attributes by referencing those attributes in the function we used to create a plot of the distribution. For example, when we wanted to plot a histogram of the distribution of admission rates, we referenced the adm_rate attribute in the gf_histogram() function. In a similar vein, we will obtain numerical summaries of an attribute by referencing that attribute in the df_stats() function. Below, we obtain numerical summaries for the admissions rate attribute: df_stats(~ adm_rate, data = colleges, median) ## median_adm_rate ## 1 0.7077 The df_stats() function takes a formula syntax that is the same as the formula syntax we introduced in the previous chapter. In particular, the variable that we wish to compute a statistic on is specified after the ~. We also specify the data object with the data= argument. Finally, we include additional arguments indicating the name of the particular numerical summary (or summaries) that we want to compute.4 In the syntax above, we compute the median admission rate. The median is also referred to as the 50th percentile, and is the value at which half of the admission rates in the data are above and half are below. In our data, the medain admission rate is 70.8%. In our data 1,009 institutions have an admission rate below 70.8% and 1,009 have an admission rate above 70.8%. In the histogram below, we add a vertical line at the median admission rate to help you visualize what this means. Another common numerical summary that is often used to describe a distribution is the mean. To compute the mean admission rate we again use the df_stats() function, but include mean as our additional argument. df_stats(~ adm_rate, data = colleges, mean) ## mean_adm_rate ## 1 0.6827355 The mean (or average) admission rate for the 2,019 institutions of higher education is 68.3%. 3.2 Visualizing the Median and Mean In your previous educational experiences with the mean and median, you may have learned the formulas or algorithms that produce these values. For example: Mean: Add up all the values of the attribute and divide by the number of values; Median: Order all the values of the attribute from smallest to largest and find the one in the middle. If there is an even number of observations, find the mean of the middle two values. To better understand these summaries, we will visualize them on the distirbution of admission rates. ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified Figure 3.1: Distribution of admission rates for thw 2,019 institutions of higher education. The mean admission rate is displayed as a red, dashed line. The mean (displayed as a red, dashed line) represents the “balance point” of the distribution. If the distribution were a physical entity, it is the location where you would put you finger underneath the distribution to balance it. In a statistical sense, we balance the distribution by “balancing” the deviations. To explain this, let’s examine a toy data set of five observations: \\[ Y = \\begin{pmatrix}10\\\\ 10\\\\ 20\\\\ 30\\\\ 50\\end{pmatrix} \\] The mean of these five values is 24. Each of these values has a deviation which is computed as the difference between the observed value and the mean value. For the toy data, \\[ Y = \\begin{pmatrix}10 - 24\\\\ 10-24\\\\ 20-24\\\\ 30-24\\\\ 50-24\\end{pmatrix} = \\begin{pmatrix}-14\\\\ -14\\\\ -4\\\\ 6\\\\ 26\\end{pmatrix} \\] Notice that some of the deviations are negative (the observation was below the mean) and some are positive (the observation was above the mean). The mean “balances” these deviations since the sum of the deviations is 0. What if we had instead looked at the deviations from the median, which is 20? \\[ Y = \\begin{pmatrix}10 - 20\\\\ 10-20\\\\ 20-20\\\\ 30-20\\\\ 50-20\\end{pmatrix} = \\begin{pmatrix}-10\\\\ -10\\\\ 0\\\\ 10\\\\ 30\\end{pmatrix} \\] The median does not balance the deviations; the sum is not zero (it is \\(+20\\)). The mean is the only value we can use to “balance” the deviations. What about the median? gf_histogram(~ adm_rate, data = colleges, bins = 30) %&gt;% gf_vline(color = &#39;blue&#39;, xintercept = 0.708, size = 1) ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified In the figure, half of the observations in the histogram have an admission rate below the blue line and half have an admission rate above the blue line. The median splits the distribution into two equal areas. Note that the median is not necessarily in the middle of the values represented on the \\(x\\)-axis; that would be 0.50 rather than 0.708. It is the area under the curve (or embodied by the histogram) that is halved. 3.2.1 Summarize with the Mean or Median? The goal of summarizing the distribution numerically is to provide a value that typifies or represents the observed values of the attribute. In our example, we need a value that summarizes the 2,019 admission rates. Since the mean balances the deviations, it is the representative because it is the value that is “closest” (at least on average) to all of the observations. (It is the value that produces the smallest average deviation—since the sum of deviations is 0, the average deviation is also 0.) The median is representative because half of the distribution is smaller than that value and the other half is larger. But, does one represent the distribution better than the other? ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified Figure 3.2: Distribution of the admission rates for 2,019 institutions of higher education. The mean admission rate is displayed as a red, dashed line. The median admission rate is displayed as a blue, solid line. In this example, both values are quite similar, so either would send a simlar message about the distribution of admission rates, namely that a typical admission rate for these 2,019 institutions of higher education is around 70%. Looking at the plot, we see that the mean admission rate is lower than the median admission rate. In a left-skewed distribution this will often be the case. The mean is “pulled toward the tail” of the distribution. This is because the mean is influenced by extreme values (which in a skewed distribution are in the tail). The median is not influenced by extreme values; we say it is robust to these values. This is because in calculating the median, only the middle score (or middle two scores) are used, so its value is not informed by the extreme values in the distribution.5 In practice, it is a good idea to compute both the mean and the median and explore whether one is more representative than the other (perhaps by plotting them on the distribution). The choice of one over the other should also be guided by substantive knowledge. 3.3 Numerically Summarizing Variation In the distribution of admission rates, both the mean and median seems to offer a representative admission rate since both are close to the modal clump of the distribution. (There are several colleges that have an admission rate close to 70%.) But, you will also notice that an admission rate of 70% does not do a great job representing all of the institutions’ admissions rates. This is true for any single statistic we pick to summarize the distribution. To more fully summarize the distribution we need to summarize the variability in the distribution in addition to a “typical” or representative value. There are several summaries that statisticians and data scientists use to describe the variation in a distribution. And, like the representative summary measures, each of the summaries of variation provide slightly different information by highlighting different aspects of the variability. We will explore some of these measures below. 3.3.1 Range One measure of variation that you have almost surely encountered before is the range. This numerical measure is the difference between the maximum and minimum values in the data. To compute this we provide the df_stats() function with two additional arguments, min and max. Then we can compute the difference between these values. # Obtain minimum and maximum admission rate df_stats(~ adm_rate, data = colleges, min, max) ## min_adm_rate max_adm_rate ## 1 0 1 # Compute range 1 - 0 ## [1] 1 The range of the admission rates is 1. When people colloquially describe the range, they typically provide the limits of the data rather than actually providing the range. For example, they may describe the range of the admission rates as: “the admission rates range from 0 to 1”. While this is technially not the range (which is a single number), it is probably more descriptive as it also gives a sense of the lower- and upper-limits for the observations. One problem with the range is that if there are extreme values, the range will not give an accurate picture of the variation encompassing most observations. For example, consider the following five test scores: \\[ Y = \\begin{pmatrix}30\\\\ 35\\\\ 36\\\\ 37\\\\ 100\\end{pmatrix} \\] The range of these data is 70, indicating that the variation between the lowest and highest score is 70 points, suggesting a lot of variation in the scores. The range, however, is clearly influenced by the score of 100. Were it not for that score, we would have a much different take on the score variability; the other scores are between 30 and 37 (a range of 7), suggesting that there is not a lot of differenes in the test scores.6 While the range is perhaps not the best measure of variation, it is quite useful as a validity check on the data to ensure that the attribute’s values are theoretically possible. In this case the values are all between 0 and 1, which are values that are theoretically plausible for admission rate. 3.3.2 Percentile Range One way to deal with extreme values in the sample is simply to not include them when we calculate the range. For example, instead of computing the difference between the maximum and minimum value in the data (which includes extreme values), truncate the bottom 10% and upper 10% of the data and calculate the range between the remaining maximum and minimum values. This is essentially the range of the middle 80% of the data. To compute the endpoint after truncatng the lower- and upper 10% we will use the quantile() function. This function finds the data value for an associated percentile provided to the function. If we wnat to truncate the lower- and upper 10% of a distribution we are interested in finding the values associated with the 10th and 90th percentiles. The syntax below shows two manners for obtaining these values for the admissions rate attribute. # Provide both percentiles separately colleges %&gt;% df_stats(~ adm_rate, quantile(0.10), quantile(.90)) ## 10% 90% ## 1 0.39284 0.94706 # Provide both percentiles in a single quantile() call colleges %&gt;% df_stats(~ adm_rate, quantile(c(0.1, 0.9))) ## 10% 90% ## 1 0.39284 0.94706 # Compute the range ofthe middle 80% of the data 0.94706 - 0.39284 ## [1] 0.55422 The range of admissions rates for 80% of the 2,019 institutions of higher education is 0.554. We can visualize this by adding the percentiles on the plot of the distribution of admission rates. These values seem to visually correspond to where most of the data are concentrated. ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified Figure 3.3: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 10th and 90th percentiles, respectively. 3.3.3 Interquartile Range (IQR) One percentile range that statisticians and data scientists use a great deal is the interquartile range (IQR). This range demarcates the middle 50% of the distribution; it truncates the lower and upper 25% of the values. In other words it is based on finding the range between the 25th- and the 75th-percentiles. # Obtain values for the 25th- and 75th percentiles colleges %&gt;% df_stats(~ adm_rate, quantile(c(0.25, 0.75))) ## 25% 75% ## 1 0.5524 0.83815 # Compute the IQR 0.83815 - 0.5524 ## [1] 0.28575 The range of admission rates for the middle 50% of the distribution is 28.5%. Since it is based on only 50% of the observations, the IQR no longer gives the range for “most” of the data, but, as shown in the plot below, this range encompasses the modal clump of institutions’ admission rates and can be useful for describing the variation. ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified Figure 3.4: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 25th and 75th percentiles, respectively. Since the IQR describes the range for half of the observations, it can also be useful to compare this range with the entire range of the data. Below we compute these values and visualize them on a histogram of the distribution. # Obtain values for the 25th- and 75th percentiles colleges %&gt;% df_stats(~ adm_rate, min, quantile(c(0.25, 0.75)), max) ## min_adm_rate 25% 75% max_adm_rate ## 1 0 0.5524 0.83815 1 # Compute the IQR 0.83815 - 0.5524 ## [1] 0.28575 # Compute the range 1 - 0 ## [1] 1 ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified ## Warning in (function (mapping = NULL, data = NULL, ..., xintercept, na.rm = FALSE, : Using both `xintercept` and `mapping` may not have the desired result as mapping is overwritten if `xintercept` is specified Figure 3.5: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 25th and 75th percentiles, respectively. The dashed, blue lines are placed at the minimum and maximum values, respectively. Although our sample of 2,019 institutions of higher education have wildy varying admissions rates (from 0% to 100%), the middle half of those institutions have admissions rates between 55% and 84%. We also note that the 25% of institutions with the lowest admissions rate range from 0% to 55%, while the 25% of institutions with the highest admissions rate range from only 84% to 100%. This means that there is more variation in the admissions rates in the institutions with the lowest admissions rate than in the institutions with the highest admissions rates. Understanding how similar the range of variation is in these areas of the distribution can give us information about the shape f the distribution. For example, the bigger range in the lowest 25% of the data suggests that the distribution has a tail on the left side. Seventy-five percent of the institutions have admissions rates higher than 50%. These two features suggest that the distribtion is left-skewed (which we also see in the histogram). When we describe the shape of a distribution, we are actually describing the variability in the data! Examining the lowest 25%, highest 25%, and middle 50% of the data is so common that a statistician named John Tukey invented a visualization technique called the box-and-whiskers plot to show these ranges. To create a box-and-whiskers plot we use the gf_boxploth() function.7 This function takes a formula that is slightly different than we have been using, namely 0 ~ attribute name. (Note that the 0 in the formula is where the box-and-whiskers plot is centered on the y-axis.) gf_boxploth(0 ~ adm_rate, data = colleges, fill = &quot;skyblue&quot;) %&gt;% gf_labs(x = &quot;Admission rate&quot;) The “box”, etxending from 0.55 to 0.84, depicts the interuartile range; the middle 50% of the distribution. The line near the middle of the box is the median value. The “whiskers” extend to either the end of the range, or the next closest observation that is not an extreme value. (There are several extreme values on the left-hand side of the distribution representing institutions with extremely low admission rates.) The length of the whisker denotes the range of the lowest 25% of the distribution and the highest 25% of the distribution. 3.3.4 Empirical Cumulative Density The percentile range plots and the boxplot indicated the values of the distribution that demarcated a particular proportion of the distribution. For example, the boxplot visually showed the admission rates that were at the 25th, 50th, and 75th percentiles. Another plot that can be useful for understanding how much of a distribution is at or below a particular value is a plot of the empirical cumulative density. To create this plot we use the gf_ecdf() function from the ggformula package. gf_ecdf(~ adm_rate, data = colleges) %&gt;% gf_labs(x = &quot;Admission rate&quot;, y = &#39;Cumulative proportion&#39;) To read this plot, we can map admission rates to their associated cumulative proportion. For example, one-quarter of the admission rates in the distribution are at or below 0.55; that is the admission rate of 0.55 has an associated cumulative proportion of 0.25. Similarly, an admission rate of 0.71 is associated with a cumulative proportion of 0.50; one-half of the admission rates in the distribution are at or below the value of 0.71. 3.3.5 Variance and Standard Deviation Two measures of variation that are commonly used by statisticians and datd scientists are the variance and the standard deviation. These can be obtained by including var and sd, respectively, in the df_stats() function. # Compute variance and standard deviation colleges %&gt;% df_stats(~ adm_rate, var, sd) ## var_adm_rate sd_adm_rate ## 1 0.04467182 0.2113571 The variance and standard deviation are related to each other in that if we square the value of the standard deviation we obtain the variance. # Square the standard deviation 0.2113571 ^ 2 ## [1] 0.04467182 In general, the standard deviation is more useful for describing the variation in a sample because it is in the same metric as the data. In our example, the metric of the data is proportion of students admitted, and the standard deviation is also in this metric. The variance, as the square of the standard deviation, is in the squared metric—in our example, proportion of students admitted squared. While this is not a useful metric in description, it does have some nice mathematical properties, so it is also a useful measure of the variation. 3.3.5.1 Understanding the Standard Deviation To understand how we interpret the standard deviation, it is useful to see how it is calculated. To do so, we will return to our toy data set: \\[ Y = \\begin{pmatrix}10 \\\\ 10\\\\ 20\\\\ 30\\\\ 50\\end{pmatrix} \\] Recall that earlier we computed the deviation from the mean for each of these observations, and that these deviations was a measure of how far above or below the mean each observation was. \\[ Y = \\begin{pmatrix}10 - 24\\\\ 10-24\\\\ 20-24\\\\ 30-24\\\\ 50-24\\end{pmatrix} = \\begin{pmatrix}-14\\\\ -14\\\\ -4\\\\ 6\\\\ 26\\end{pmatrix} \\] A useful measure of the variation in the data would be the average of these deviations. This would tell us, on average, how far from the mean the data are. Unfortunately, if we were to compute the mean we would get zero because the sum of the deviations is zero. (That was a property of the mean discussed earlier in the chapter!) To alleviate this, we square the deviations before summing them. \\[ \\begin{pmatrix}-14^2\\\\ -14^2\\\\ -4^2\\\\ 6^2\\\\ 26^2\\end{pmatrix} = \\begin{pmatrix}196\\\\ 196\\\\ 16\\\\ 36\\\\ 676\\end{pmatrix} \\] The sum of these squared deviations is 1,120. And the average squared deviation is then \\(1120/5 = 224\\). If we take the square root of 224, which is 14.97, we now have the average deviation for the five observations. On average, the observations in the distribution are 14.97 units from the mean value of 24. The standard deviation is interpreted as the average deviation from the mean.8 3.3.5.2 Using the Standard Deviation In our example, the mean admission rate for the 2,019 institutions of higher education was 68.2%, and the standard deviation was 21.1%. We can combine these two pieces of information to make a statement about the admission rates for most of the institutions in our sample. In general, most observations in a distribution fall within one standard deviation of the mean. So, for our example, most institutions have an admission rate that is between 47.1% and 89.3%.9 # 1 SD below the mean 0.682 - 0.211 ## [1] 0.471 # 1 SD above the mean 0.682 + 0.211 ## [1] 0.893 3.4 Summarizing Categorical Attributes Categorical attributes are attributes that have values that represent categories. For example, the attribute region indicates the region in the United States where the institution is located (e.g., Midwest). The attribute bachelor_degree is a categorical value indicating whether ir not the institution offers a Bachelor’s degree. Sometimes statisticians anddata scientists use the terms dichotmous (two categories) and polychotomous (more than two categories) to further define categorical variables. Using this nomenclature, region is a polychotomous categorical variable and bachelor_degree is a dichotomous categorical variable. Sometimes analysts use numeric values to encode the categories of a categorical attribute. For example, the attribute bachelor_degree is encoded using the values of 0 and 1. It is important to note that these values just indicate whether the institution offers a Bachelor’s degree (1) or not (0). The values are not necessarily ordinal in the sense that a 1 means more of the attribute than a 0. Since the values just refer to categories, an analyst might have reversed the coding and used 0 to encode institutions that offer a Bachelor’s degree and 1 to encode those institutions that do not. Similarly, the values of 0 and 1 are not sancrosanct; any two numers could have been used to represent the categories.10 Most of the time, the numerical summaries we computed earlier in the chapter do not work so well for categorical attributes. For example, it would not make sense to compute the mean region for the institutions. In general, it suffices to compute counts and proportions for the categories included in these attributes. To compute the category counts we use the tally() function. This function takes a formula indicating the name of the categorical attribute and the name of the data object. To find the category counts for the region attribute: # Compute category counts tally(~region, data = colleges) ## region ## Far West Great Lakes Mid East ## 221 297 458 ## New England Outlying Areas Plains ## 167 35 200 ## Rocky Mountains Southeast Southwest ## 50 454 133 ## US Service Schools ## 4 To find the proportion of institutions in each region, we can divide each of the counts by 2,019. # Compute category proportions tally(~region, data = colleges) / 2019 ## region ## Far West Great Lakes Mid East ## 0.109460129 0.147102526 0.226844973 ## New England Outlying Areas Plains ## 0.082714215 0.017335315 0.099058940 ## Rocky Mountains Southeast Southwest ## 0.024764735 0.224863794 0.065874195 ## US Service Schools ## 0.001981179 You could also compute the proportions directly with the tally() function by specifying the argument format = &quot;proportion&quot;. # Compute category proportions tally(~region, data = colleges, format = &quot;proportion&quot;) ## region ## Far West Great Lakes Mid East ## 0.109460129 0.147102526 0.226844973 ## New England Outlying Areas Plains ## 0.082714215 0.017335315 0.099058940 ## Rocky Mountains Southeast Southwest ## 0.024764735 0.224863794 0.065874195 ## US Service Schools ## 0.001981179 3.5 Advanced Extension: Computing Your Own Measure of Variation If you have another non-standard measure of variation that you want to compute, you can always write your own function to compute it. For example, say you wanted to compute the mean absolute error (the mean of the absolute values of the deviations). To compute the mean absolute error, we first need to define a new function. mae &lt;- function(x, na.rm = TRUE, ...) { avg &lt;- mean(x, na.rm = na.rm, ...) abs_avg &lt;- abs(x - avg) mean(abs_avg) } We can now use this new function by employing it as an argument in the df_stats() function. colleges %&gt;% df_stats(~ adm_rate, mae) ## mae_adm_rate ## 1 0.1692953 Note that the names of the summaries we include in the df_stats() function need to be the actual names of functions that R recognizes.↩ The downside of using the median is that it is only informed by one or two observations in the data. The mean is informed by all of the observations. This property of the mean makes it a more useful than the median in some mathematical and theoretical applications.↩ A second issue with range is that it is a biased statistic. If we use it as an estimate of the population range, it will almose inevitably be too small. The population range will almost always be larger since the sampling process will often not select extreme population values.↩ The gf_boxplot() function creates vertical a box-and-whiskers plot, and the gf_boxploth() function creates a horizontal box-and-whiskers plot.↩ Technically, after summing the squared deviations, we divide this sum by \\(n-1\\) rather than \\(n\\). But, when the sample size is even somewhat large, this difference is trivial.↩ If we know the exact shape of the distribution we can be more specific about the proportion of the distribution that fall within one standard deviation of the mean.↩ Using 0’s and 1’s for the encoding does have some advantages over other coding schemes which we will explore later when fitting statistical models.↩ "],
["multivariate-visualization.html", "Chapter 4 Multivariate Visualization 4.1 Facetting", " Chapter 4 Multivariate Visualization library(tidyverse) library(ggformula) library(statthink) # Import data colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) Real world data are never as simple exploring a distribution of a single variable, particularly when trying to understand individual variation. In most cases things interact, move in tandem, and many phenomena help to explain the variable of interest. For example, when thinking about admission rates, what may be some important factors that would explain some of the reasons why higher education institutions differ in their admission rates? Take a few minutes to brainstorm some ideas. gf_histogram(~ adm_rate, data = colleges, bins = 30, fill = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, fill = &quot;Primary Deg&quot;) Often density plots are easier to visualize when there are more than one group. To plot more than one density curve, we need to specify the color argument instead of fill. gf_density(~ adm_rate, data = colleges, color = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, color = &quot;Primary Deg&quot;) gf_density(~ adm_rate, data = colleges, fill = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, fill = &quot;Primary Deg&quot;) gf_density(~ adm_rate, data = colleges, fill = ~ preddeg, color = ~ preddeg) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, color = &quot;Primary Deg&quot;, fill = &quot;Primary Deg&quot;) gf_density(~ adm_rate, data = colleges, color = ~ preddeg, fill = &#39;gray85&#39;, size = 1) %&gt;% gf_labs(x = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, color = &quot;Primary Deg&quot;) ## Violin Plots Violin plots are another way to make comparisons of distributions across groups. Violin plots are also easier to show more groups on a single graph. Violin plots are density plots that are mirrored to be fully enclosed. Best to explore with an example.ArithmeticError gf_violin(adm_rate ~ preddeg, data = colleges) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;Primary Deg&quot;) Aesthetically, these figures are a bit more pleasing to look at if they include a light fill color. This is done similar to the density plots shown above with the fill = argument.ArithmeticError gf_violin(adm_rate ~ preddeg, data = colleges, fill = &#39;gray85&#39;) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;Primary Deg&quot;) Adding quantiles are useful to aid in the comparison with the violin plots. These can be added with the draw_quantiles argument. gf_violin(adm_rate ~ preddeg, data = colleges, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;Primary Deg&quot;) ### Violin Plots with many groups Many groups are more easily shown in the violin plot framework. With many groups, it is often of interest to put the long x-axis labels representing each group on the y-axis so that it reads the correct direction and the labels do not run into each other. This can be done with the gf_refine() function with coord_flip(). gf_violin(adm_rate ~ region, data = colleges, fill = &#39;gray80&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;US Region&quot;) %&gt;% gf_refine(coord_flip()) 4.1 Facetting Facetting is another way to explore distributions of two or more variables. gf_violin(adm_rate ~ region, data = colleges, fill = &#39;gray80&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &#39;Admission Rate (in %)&#39;, title = &#39;Multivariate distribution of higher education admission rates by degree type&#39;, x = &quot;US Region&quot;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_facet_wrap(~ preddeg) "],
["classification.html", "Chapter 5 Classification 5.1 Topic: Decision Trees", " Chapter 5 Classification In this example, we will explore data from the titanic that comes from Kaggle (https://www.kaggle.com/c/titanic/data). You can view the attributes in the data from the link previous. The following set of code will install a couple a new packages that we will utilize for this section of the course, the titanic package has the data we will use and the rpart package includes functions to perform the tree based models we will employ. 5.1 Topic: Decision Trees library(tidyverse) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggformula) ## Loading required package: ggstance ## ## Attaching package: &#39;ggstance&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(&quot;introduction&quot;, package = &quot;ggformula&quot;) ## learnr::run_tutorial(&quot;refining&quot;, package = &quot;ggformula&quot;) library(mosaic) ## Loading required package: lattice ## Loading required package: mosaicData ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Registered S3 method overwritten by &#39;mosaic&#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The &#39;mosaic&#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Note: If you use the Matrix package, be sure to load it BEFORE loading mosaic. ## ## Attaching package: &#39;mosaic&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## mean ## The following objects are masked from &#39;package:dplyr&#39;: ## ## count, do, tally ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## stat ## The following objects are masked from &#39;package:stats&#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, ## prop.test, quantile, sd, t.test, var ## The following objects are masked from &#39;package:base&#39;: ## ## max, mean, min, prod, range, sample, sum library(titanic) library(rpart) library(rsample) library(rpart.plot) library(statthink) theme_set(theme_statthinking()) titanic &lt;- bind_rows(titanic_train, titanic_test) %&gt;% mutate(survived = ifelse(Survived == 1, &#39;Survived&#39;, &#39;Died&#39;)) %&gt;% select(-Survived) %&gt;% drop_na(survived) head(titanic) ## PassengerId Pclass Name ## 1 1 3 Braund, Mr. Owen Harris ## 2 2 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) ## 3 3 3 Heikkinen, Miss. Laina ## 4 4 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) ## 5 5 3 Allen, Mr. William Henry ## 6 6 3 Moran, Mr. James ## Sex Age SibSp Parch Ticket Fare Cabin Embarked survived ## 1 male 22 1 0 A/5 21171 7.2500 S Died ## 2 female 38 1 0 PC 17599 71.2833 C85 C Survived ## 3 female 26 0 0 STON/O2. 3101282 7.9250 S Survived ## 4 female 35 1 0 113803 53.1000 C123 S Survived ## 5 male 35 0 0 373450 8.0500 S Died ## 6 male NA 0 0 330877 8.4583 Q Died count(titanic, survived) ## # A tibble: 2 x 2 ## survived n ## &lt;chr&gt; &lt;int&gt; ## 1 Died 549 ## 2 Survived 342 gf_bar(~ survived, data = titanic) 5.1.1 Fitting a Classification Tree Let’s class_tree our first classification tree to predict the dichotomous attribute, survival. For this, we will use the rpart() function from the rpart package. The first argument to the rpart() function is a formula where the outcome of interest is specified to the left of the ~ and the attributes that are predictive of the outcome are specified to the right of the ~ separated with + signs. The second argument specifies the method for which we want to run the analysis, in this case we want to classify individuals based on the values in the data, therefore we specify method = ‘class’. The final argument is the data element, in this case titanic. In this example, I picked a handful of attributes that would seem important. These can either be numeric or represent categories, the method does not care the type of attributes that are included in the analysis. Notice that I save the computation to the object, class_tree. class_tree &lt;- rpart(survived ~ Pclass + Sex + Age + Fare + Embarked + SibSp + Parch, method = &#39;class&#39;, data = titanic) rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3) rpart.rules(class_tree, cover = TRUE) ## survived cover ## 0.11 when Sex is male &amp; Age &lt; 6.5 &amp; SibSp &gt;= 3 1% ## 0.11 when Sex is female &amp; Pclass &gt;= 3 &amp; Fare &gt;= 23 3% ## 0.17 when Sex is male &amp; Age &gt;= 6.5 62% ## 0.30 when Sex is female &amp; Pclass &gt;= 3 &amp; Fare is 18 to 23 &amp; Embarked is S 1% ## 0.41 when Sex is female &amp; Pclass &gt;= 3 &amp; Fare &lt; 11 &amp; Embarked is S 4% ## 0.70 when Sex is female &amp; Pclass &gt;= 3 &amp; Fare &lt; 23 &amp; Embarked is C or Q 6% ## 0.81 when Sex is female &amp; Pclass &gt;= 3 &amp; Fare is 11 to 18 &amp; Embarked is S 2% ## 0.95 when Sex is female &amp; Pclass &lt; 3 19% ## 1.00 when Sex is male &amp; Age &lt; 6.5 &amp; SibSp &lt; 3 2% 5.1.2 Pruning Trees One downside of decision trees, is that they can tend to overfit the data and capitalize on chance variation in our sample that we can not generalize to another sample. This means that there are features in the current sample that would not be present in another sample of data. There are a few ways to overcome this, one is to prune the tree to only include the attributes that are most important and improve the classification accuracy. One measure of this can be used is called the complexity parameter (CP) and this statistic attempts to balance the tree complexity related to how strongly the levels of the tree improve the classification accuracy. We can view these statistics with the printcp() and plotcp() functions where the only argument to be specified is the classification tree computation that was saved in the previous step. printcp(class_tree) ## ## Classification tree: ## rpart(formula = survived ~ Pclass + Sex + Age + Fare + Embarked + ## SibSp + Parch, data = titanic, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] Age Embarked Fare Pclass Sex SibSp ## ## Root node error: 342/891 = 0.38384 ## ## n= 891 ## ## CP nsplit rel error xerror xstd ## 1 0.444444 0 1.00000 1.00000 0.042446 ## 2 0.030702 1 0.55556 0.55556 0.035750 ## 3 0.023392 3 0.49415 0.51754 0.034823 ## 4 0.020468 4 0.47076 0.50585 0.034524 ## 5 0.010234 5 0.45029 0.50585 0.034524 ## 6 0.010000 8 0.41813 0.50000 0.034372 plotcp(class_tree) prune_class_tree &lt;- prune(class_tree, cp = .02) rpart.plot(prune_class_tree, roundint = FALSE, type = 3, branch = .3) 5.1.3 Accuracy titanic_predict &lt;- titanic %&gt;% mutate(tree_predict = predict(prune_class_tree, type = &#39;class&#39;)) %&gt;% cbind(predict(prune_class_tree, type = &#39;prob&#39;)) head(titanic_predict, n = 20) ## PassengerId Pclass ## 1 1 3 ## 2 2 1 ## 3 3 3 ## 4 4 1 ## 5 5 3 ## 6 6 3 ## 7 7 1 ## 8 8 3 ## 9 9 3 ## 10 10 2 ## 11 11 3 ## 12 12 1 ## 13 13 3 ## 14 14 3 ## 15 15 3 ## 16 16 2 ## 17 17 3 ## 18 18 2 ## 19 19 3 ## 20 20 3 ## Name Sex Age ## 1 Braund, Mr. Owen Harris male 22 ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 ## 3 Heikkinen, Miss. Laina female 26 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 ## 5 Allen, Mr. William Henry male 35 ## 6 Moran, Mr. James male NA ## 7 McCarthy, Mr. Timothy J male 54 ## 8 Palsson, Master. Gosta Leonard male 2 ## 9 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27 ## 10 Nasser, Mrs. Nicholas (Adele Achem) female 14 ## 11 Sandstrom, Miss. Marguerite Rut female 4 ## 12 Bonnell, Miss. Elizabeth female 58 ## 13 Saundercock, Mr. William Henry male 20 ## 14 Andersson, Mr. Anders Johan male 39 ## 15 Vestrom, Miss. Hulda Amanda Adolfina female 14 ## 16 Hewlett, Mrs. (Mary D Kingcome) female 55 ## 17 Rice, Master. Eugene male 2 ## 18 Williams, Mr. Charles Eugene male NA ## 19 Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele) female 31 ## 20 Masselmani, Mrs. Fatima female NA ## SibSp Parch Ticket Fare Cabin Embarked survived ## 1 1 0 A/5 21171 7.2500 S Died ## 2 1 0 PC 17599 71.2833 C85 C Survived ## 3 0 0 STON/O2. 3101282 7.9250 S Survived ## 4 1 0 113803 53.1000 C123 S Survived ## 5 0 0 373450 8.0500 S Died ## 6 0 0 330877 8.4583 Q Died ## 7 0 0 17463 51.8625 E46 S Died ## 8 3 1 349909 21.0750 S Died ## 9 0 2 347742 11.1333 S Survived ## 10 1 0 237736 30.0708 C Survived ## 11 1 1 PP 9549 16.7000 G6 S Survived ## 12 0 0 113783 26.5500 C103 S Survived ## 13 0 0 A/5. 2151 8.0500 S Died ## 14 1 5 347082 31.2750 S Died ## 15 0 0 350406 7.8542 S Died ## 16 0 0 248706 16.0000 S Survived ## 17 4 1 382652 29.1250 Q Died ## 18 0 0 244373 13.0000 S Survived ## 19 1 0 345763 18.0000 S Died ## 20 0 0 2649 7.2250 C Survived ## tree_predict Died Survived ## 1 Died 0.83182640 0.1681736 ## 2 Survived 0.05294118 0.9470588 ## 3 Survived 0.41025641 0.5897436 ## 4 Survived 0.05294118 0.9470588 ## 5 Died 0.83182640 0.1681736 ## 6 Died 0.83182640 0.1681736 ## 7 Died 0.83182640 0.1681736 ## 8 Died 0.88888889 0.1111111 ## 9 Survived 0.41025641 0.5897436 ## 10 Survived 0.05294118 0.9470588 ## 11 Survived 0.41025641 0.5897436 ## 12 Survived 0.05294118 0.9470588 ## 13 Died 0.83182640 0.1681736 ## 14 Died 0.83182640 0.1681736 ## 15 Survived 0.41025641 0.5897436 ## 16 Survived 0.05294118 0.9470588 ## 17 Died 0.88888889 0.1111111 ## 18 Died 0.83182640 0.1681736 ## 19 Survived 0.41025641 0.5897436 ## 20 Survived 0.41025641 0.5897436 titanic_predict %&gt;% count(survived, tree_predict) ## # A tibble: 4 x 3 ## survived tree_predict n ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 Died Died 492 ## 2 Died Survived 57 ## 3 Survived Died 97 ## 4 Survived Survived 245 gf_bar(~ survived, fill = ~tree_predict, data = titanic_predict) gf_bar(~ survived, fill = ~tree_predict, data = titanic_predict, position = &quot;fill&quot;) %&gt;% gf_labs(y = &#39;Proportion&#39;) %&gt;% gf_refine(scale_y_continuous(breaks = seq(0, 1, .1))) titanic_predict %&gt;% mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## mean_same_class sum_same_class ## 1 0.8271605 737 5.1.4 Comparison to Baseline titanic_predict &lt;- titanic_predict %&gt;% mutate(tree_predict_full = predict(class_tree, type = &#39;class&#39;)) titanic_predict %&gt;% count(survived, tree_predict_full) ## # A tibble: 4 x 3 ## survived tree_predict_full n ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 Died Died 521 ## 2 Died Survived 28 ## 3 Survived Died 115 ## 4 Survived Survived 227 gf_bar(~ survived, fill = ~tree_predict_full, data = titanic_predict, position = &quot;fill&quot;) %&gt;% gf_labs(y = &quot;proportion&quot;) %&gt;% gf_refine(scale_y_continuous(breaks = seq(0, 1, .1))) titanic_predict %&gt;% mutate(same_class = ifelse(survived == tree_predict_full, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## mean_same_class sum_same_class ## 1 0.8395062 748 5.1.4.1 Absolute vs Relative Comparison 5.1.5 Training/Test Data So far we have used the entire data to make our classification. This is not best practice and we will explore this is a bit more detail. First, take a minute to hypothesize why using the entire data to make our classification prediction may not be the best? It is common to split the data prior to fitting a classification/prediction model into a training data set in which the model makes a series of predictions on the data, learns which data attributes are the most important, etc. Then, upon successfully identifying a useful model with the training data, test these model predictions on data that the model has not seen before. This is particularly important as the algorithms to make the predictions are very good at understanding and exploiting small differences in the data used to fit the model. Therefore, exploring the extent to which the model does a good job on data the model has not seen is a better test to the utility of the model. We will explore in more detail the impact of not using the training/test data split later, but first, let’s refit the classification tree to the titanic data by splitting the data into 70% training and 30% test data. Why 70% training and 30% test? This is a number that is sometimes used as the splitting, an 80/20 split is also common. The main idea behind the making the test data smaller is so that the model has more data to train on initially to understand the attributes from the data. Secondly, the test data does not need to be quite as large, but we would like it to be representative. Here, the data are not too large, about 1000 passengers with available survival data, therefore, withholding more data helps to ensure the test data is representative of the 1000 total passengers. Splitting the data into training/test This is done with the rsample package utilizing three functions, initial_split(), training(), and test(). The initial_split() function helps to take the initial random sample and the proportion of data to use for the training data is initially identified. The random sample is done without replacement meaning that the data are randomly selected, but can not show up in the data more than once. Then, after using the initial_split() function, the training() and test() functions are used on the resulting output from initial_split() to obtain the training and test data respectively. It is good practice to use the set.seed() function to save the seed that was used as this is a random process. Without using the set.seed() function, the same split of data would likely not be able to be recreated in the code was ran again. Let’s do the data splitting. titanic &lt;- bind_rows(titanic_train, titanic_test) %&gt;% mutate(survived = ifelse(Survived == 1, &#39;Survived&#39;, &#39;Died&#39;)) %&gt;% drop_na(survived) set.seed(2019) titanic_split &lt;- initial_split(titanic, prop = .7) titanic_train &lt;- training(titanic_split) titanic_test &lt;- testing(titanic_split) class_tree &lt;- rpart(survived ~ Pclass + Sex + Age + Fare + Embarked + SibSp + Parch, method = &#39;class&#39;, data = titanic_train) rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3) prune_class_tree &lt;- prune(class_tree, cp = .02) rpart.plot(prune_class_tree, roundint = FALSE, type = 3, branch = .3) This seems like a reasonable model. Let’s check the model accuracy. titanic_predict &lt;- titanic_train %&gt;% mutate(tree_predict = predict(prune_class_tree, type = &#39;class&#39;)) titanic_predict %&gt;% mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## mean_same_class sum_same_class ## 1 0.8445513 527 This is actually slightly better accuracy compared to the model last time, about xxx compared to about xxx prediction accuracy. But, let’s test the model out on the test data to see the prediction accuracy for the test data, the real test. titanic_predict_test &lt;- titanic_test %&gt;% mutate(tree_predict = predict(prune_class_tree, newdata = titanic_test, type = &#39;class&#39;)) titanic_predict_test %&gt;% mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## mean_same_class sum_same_class ## 1 0.7827715 209 For the test data, prediction accuracy was quite a bit lower, about xxx. 5.1.6 Introduction to resampling/bootstrap To explore these ideas in more detail, it will be helpful to use a statistical technique called resampling or the bootstrap. We will use these ideas a lot going forward in this course. In very simple terminology, resampling or the bootstrap can help us understand uncertainty in our estimates and also allow us to be more flexible in the statistics that we run. The main drawback of resampling and bootstrap methods is that they can be computationally heavy, therefore depending on the situation, more time is needed to come to the conclusion desired. Resampling and bootstrap methods use the sample data we have and perform the sampling procedure again treating the sample we have data for as the population. Generating the new samples is done with replacement (more on this later). This resampling is done many times (100, 500, 1000, etc.) with more in general being better. As an example with the titanic data, let’s take the titanic data, assume this is the population of interest, and resample from this population 1000 times (with replacement) and each time we will calculate the proportion that survived the disaster in each sample. Before we write the code for this, a few questions to consider. Would you expect the proportion that survived to be the same in each new sample? Why or why not? Sampling with replacement keeps coming up, what do you think this means? Hypothesize why sampling with replacement would be a good idea? Let’s now try the resampling with the calculation of the proportion that survived. We will then save these 1000 survival proportions and create a visualization. resample_titanic &lt;- function(...) { titanic %&gt;% sample_n(nrow(titanic), replace = TRUE) %&gt;% df_stats(~ Survived, mean) } survival_prop &lt;- map(1:1000, resample_titanic) %&gt;% bind_rows() gf_density(~ mean_Survived, data = survival_prop) 5.1.6.1 Bootstrap variation in prediction accuracy We can apply these same methods to evaluate the prediction accuracy based on the classification model above. When using the bootstrap, we can get an estimate for how much variation there is in the classification accuracy based on the sample that we have. In addition, we can explore how different the prediction accuracy would be for many samples when using all the data and by splitting the data into training and test sets. Bootstrap full data. Let’s first explore the full data to see how much variation there is in the prediction accuracy using all of the data. Here we will again use the sample_n() function to sample with replacement, then fit the classification model to each of these samples, then calculate the prediction accuracy. First, I’m going to write a function to do all of these steps one time. calc_predict_acc &lt;- function(data) { rsamp_titanic &lt;- titanic %&gt;% sample_n(nrow(titanic), replace = TRUE) class_model &lt;- rpart(survived ~ Pclass + Sex + Age + Fare + SibSp + Parch, method = &#39;class&#39;, data = rsamp_titanic, cp = .02) titanic_predict &lt;- rsamp_titanic %&gt;% mutate(tree_predict = predict(class_model, type = &#39;class&#39;)) titanic_predict %&gt;% mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) } calc_predict_acc() ## mean_same_class sum_same_class ## 1 0.8451178 753 To do the bootstrap, this process can be replicated many times. In this case, I’m going to do 500. In practice, we would likely want to do a few more. predict_accuracy_fulldata &lt;- map(1:2000, calc_predict_acc) %&gt;% bind_rows() gf_density(~ mean_same_class, data = predict_accuracy_fulldata) calc_predict_acc_split &lt;- function(data) { titanic_split &lt;- initial_split(titanic, prop = .7) titanic_train &lt;- training(titanic_split) titanic_test &lt;- testing(titanic_split) class_model &lt;- rpart(survived ~ Pclass + Sex + Age + Fare + SibSp + Parch, method = &#39;class&#39;, data = titanic_train, cp = .02) titanic_predict &lt;- titanic_test %&gt;% mutate(tree_predict = predict(class_model, newdata = titanic_test, type = &#39;class&#39;)) titanic_predict %&gt;% mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) } calc_predict_acc_split() ## mean_same_class sum_same_class ## 1 0.8277154 221 predict_accuracy_traintest &lt;- map(1:2000, calc_predict_acc_split) %&gt;% bind_rows() gf_density(~ mean_same_class, data = predict_accuracy_traintest) bind_rows( mutate(predict_accuracy_fulldata, type = &quot;Full Data&quot;), mutate(predict_accuracy_traintest, type = &quot;Train/Test&quot;) ) %&gt;% gf_density(~ mean_same_class, color = ~ type, fill = NA, size = 1.25) 5.1.7 Cross-validation "],
["linear-model.html", "Chapter 6 Linear Model 6.1 Simple Regression continuous predictor 6.2 Conditional Means 6.3 Categorical Predictor(s) 6.4 Multiple Regression", " Chapter 6 Linear Model 6.1 Simple Regression continuous predictor 6.2 Conditional Means 6.3 Categorical Predictor(s) 6.4 Multiple Regression "],
["estimation-bootstrap-uncertainty.html", "Chapter 7 Estimation / Bootstrap / Uncertainty", " Chapter 7 Estimation / Bootstrap / Uncertainty "],
["prediction-for-individuals.html", "Chapter 8 Prediction for individuals 8.1 Comparison of classification / linear model", " Chapter 8 Prediction for individuals 8.1 Comparison of classification / linear model "]
]
