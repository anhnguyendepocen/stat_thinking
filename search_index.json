[
["index.html", "Statistical Reasoning through Computation and R Preface", " Statistical Reasoning through Computation and R Brandon LeBeau and Andrew S. Zieffler September 25, 2020 Preface This book provides a modern statistical reasoning and introduction to statistics text. Computation, using the R programming language, are used instead of relying on traditional statistical theory. These analyses can often be categorized into two broad categories, Descriptive Statistics Inferential Statistics Descriptive Statistics help to describe the data and are particularly useful to give a single numeric summary for a single variable. We will explore this idea more fully in this section. Inferential Statistics help us to make broader statements from the data we have to the larger group of interest, commonly referred to as the population. More details on these steps later in the course. "],
["introduction.html", "Chapter 1 Introduction 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure", " Chapter 1 Introduction Here is an intro. And more. 1.1 Statistics vs Data Science 1.2 Experiments vs Observations 1.3 Data Structure "],
["visualization.html", "Chapter 2 Visualization 2.1 Exploring Attributes 2.2 Plot Customization 2.3 Density plots", " Chapter 2 Visualization Data scientists and statisticians visualize data to explore and understand data. Visualization can help analysts identify features in the data such as typical or extreme observations and also for describe variation. Because it is so powerful, data visualiztion is often the first step in any statistical analysis. 2.0.1 College Scorecard Data The U.S. Department of Education publishes data on institutions of higher education in their College Scorecard (https://collegescorecard.ed.gov/) to facilitate transparency and provide information for interested stakeholders (e.g., parents, students, educators). A subset of this data is provided in the file College-scorecard-clean.csv. To illustrate some of the common methods statisticians use to visualize data, we will examine admissions rates for 2,019 institutions of higher education. Before we begin the analysis, we will load two packages, the tidyverse package and the ggformula package. These packages include many useful functions that we will use in this chapter. library(tidyverse) library(ggformula) There are many functions in R to import data. We will use the function read_csv() since the data file we are importing (College-scorecard-clean.csv) is a comma separated value (CSV) file..1 CSV files are a common format for storing data. Since they are encoded as text files they geerally do not take up a lot of space nor computer memory. They get their name from the fact that in the text file, each variable (i.e. column in the data) is separated by a comma within each row. The syntax to import the college scorecard data is as follows: colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) In this syntax we have passed two arguments to the read_csv() function. The first argument, file=, indicates the path to the data file. The data file here is stored on GitHub, so the path is specified as a URL. The second argument, guess_max=, helps ensure that the data are read in appropriately. This argument will be described in more detail later. The syntax to the left of the read_csv() function, namely colleges &lt;-, takes the output of the function and stores it, or in the language of R, assigns it to an object named colleges. In data analysis, it is often useful to use results in later computations, so rather than continually re-running syntax to obtain these results, we can instead store those results in an object and then compute on the object. Here for example, we would like to use the data that was read by the read_csv() function to explore it. When we want to assign computational results to an object, we use the assignment operator, &lt;- . (Note that the assignment operator looks like a left-pointing arrow; it is taking the computational result produced on the right side and storing it in the object to the left side.) 2.0.2 View the Data Once we have imported and assigned the data to an object, it is quite useful to ensure that it was read in appropriately. The head() function will give us a quick snapshot of the data by printing the first six rows of data. head(colleges) ## [90m# A tibble: 6 x 17[39m ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m Alaba… Norm… AL Bachel… South… City:… 0.903 18 [4m4[24m824 [4m2[24m[4m2[24m886 ## [90m2[39m Unive… Birm… AL Bachel… South… City:… 0.918 25 [4m1[24m[4m2[24m866 [4m2[24m[4m4[24m129 ## [90m3[39m Unive… Hunt… AL Bachel… South… City:… 0.812 28 [4m6[24m917 [4m2[24m[4m2[24m108 ## [90m4[39m Alaba… Mont… AL Bachel… South… City:… 0.979 18 [4m4[24m189 [4m1[24m[4m9[24m413 ## [90m5[39m The U… Tusc… AL Bachel… South… City:… 0.533 28 [4m3[24m[4m2[24m387 [4m2[24m[4m8[24m836 ## [90m6[39m Aubur… Mont… AL Bachel… South… City:… 0.825 22 [4m4[24m211 [4m1[24m[4m9[24m892 ## [90m# … with 7 more variables: costt4_p [3m[90m&lt;dbl&gt;[90m[23m, tuitionfee_in [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# tuitionfee_out [3m[90m&lt;dbl&gt;[90m[23m, debt_mdn [3m[90m&lt;dbl&gt;[90m[23m, grad_debt_mdn [3m[90m&lt;dbl&gt;[90m[23m, female [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bachelor_degree [3m[90m&lt;dbl&gt;[90m[23m[39m We can also include an interactive version for viewing the book on the web using the DT package. DT::datatable(colleges) 2.1 Exploring Attributes Data scientists and statisticians often start analyses by exploring attributes (i.e., variables) that are of interest to them. For example, suppose we are interested in exploring the admission rates of the institutions in the college scorecard data to determine how selective the different institutions are. We will begin our exploration of admission rates by examining different visualizations of the admissions rate attribute. There is not one perfect visulaiztion for exploring the data. Each visualization has pros and cons; it may highlight some features of the attribute and mask others. It is often necessary to look at many different visualizations of the data in the exploratory phase. 2.1.1 Histograms The first viualization we will examine is a histogram. We can create a histogram of the admission rates using the gf_histrogram() function. (This function is part of the ggformula package which needs to be loaded prior to using the gf_histogram() function.) This function requires two arguments. The first argument is a formula that identifies the variables to be plotted and the second argument, data=, specifies the data object we assigned earlier. The syntax used to create a histrogram of the admission rates is: gf_histogram(~ adm_rate, data = colleges) The formula we provide in the first argument is based on the following general structure: ~ attribute name where the attribute name identified to the right of the ~ is the exact name of one of the columns in the colleges data object. 2.1.2 Interpretting Histograms Histograms are created by collapsing the data into bins and then counting the number of observations that fall into each bin. To show this more clearly in the figure created previously, we can color the bin lines to highlight the different bins. To do this we include an additional argument, color=, in the gf_histogram() function. We can also set the color for the bins themselves using the fill= argument. Here we color the bin lines black and set the bin color to yellow.2 gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) Rather than focusing on any one bin, we typically want to describe the distribution as a whole. For example, it appears as though most institutions admit a high proportion of applicants since the bins to the right of 0.5 have higher counts than the bins that are below 0.5. There are, however, some institutions that are quite selective, only admitting fewer than 25% of the students who apply. 2.1.2.1 Adjusting Number of Bins Interpretation of the distribution is sometimes influenced by the width or number of bins. It is often useful to change the number of bins to explore the impact this may have on your interpretation. This can be accomplished by either (1) changing the width of the bins via thebinwidth= argument in the gf_histogram() function, or (2) changing the number of bins using the bins= argument. Below we show both methods: gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 10) gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, binwidth = .01) In general, our interpretation remains the same, namely that most institutions admit a high proportion of applicants. When we used a bin width of 0.01, however, we were able to see that several institutions admit 100% of applicants. This was obscured in the other histograms we examined. As a data scientist these institutions might warrant a more nuanced examination. 2.2 Plot Customization There are many ways to further customize the plot we produced to make it more appealing. For example, you might want to change the label on the x-axis from adm_rate to something more informative. Or, you may want to add a descriptive title to your plot. These customizations can be specified using the gf_labs() function. 2.2.1 Axes labels To change the labels on the x- and y-axes, we can use the arguments x= and y= in the gf_labs() function. These arguments take the text for the label you want to add to each axis, respectively. Here we change the text on the x-axis to “Admission Rate” and the text on the y-axis to “Frequency”. The gf_labs() function is connected to the histogram by linking the gf_histogram() and gf_labs() functions with the pipe operator (%&gt;%). gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39; ) 2.2.2 Plot title and subtitle We can also add a title and subtitle to our plot. Similar to changing the axis labels, these are added using gf_labs(), but using the title= and subtitle= arguments. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) 2.2.3 Plot theme By default, the plot has a grey background and white grid lines. This can be modified to using the gf_theme() function. For example, in the syntax below we change the plot theme to a white background with no grid lines using theme_classic(). Again, the gf_theme() is linked to the histogram with the pipe operator. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_classic()) We have created a custom theme to use in the gf_theme() function that we will use for most of the plots in the book. The theme, theme_statthinking() is included in the statthink library, a supplemental package to the text that can be installed and loaded with the following commands: remotes::install_github(&#39;lebebr01/statthink&#39;) ## Using bundled GitHub PAT. Please add your own PAT to the env var `GITHUB_PAT` ## Skipping install of &#39;statthink&#39; from a github remote, the SHA1 (c3291533) has not changed since last install. ## Use `force = TRUE` to force installation library(statthink) ## ## Attaching package: &#39;statthink&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## colleges We can then change the theme in a similar manner to how we changed the theme before. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) %&gt;% gf_theme(theme_statthinking()) 2.2.3.1 Setting the default plot theme Since we will be using this theme for all of our plots, it is useful to make it the default theme (rather than the grey bckground with white gridlines). To set a different theme as the default, we will use the theme_set() function and call our theme_statthinking() within this function. theme_set(theme_statthinking()) Now when we create a plot, it will automatically use the statthinking theme without having to specify this in the gf_theme() function. gf_histogram(~ adm_rate, data = colleges, color = &#39;black&#39;, bins = 25) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Frequency&#39;, title = &#39;Distribution of admission rates for 2,000 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) 2.3 Density plots Another plot that is useful for exploring attributes is the density plot. This plot usually highlights similar distributional features as the histogram, but the visualization does not have the same dependency on the specification of bins. Density plots can be created with the gf_density() function which takes similar arguments as gf_histogram(), namely a formula identifying the attribute to be plotted and the data object.3 gf_density(~ adm_rate, data = colleges) Our interpretation remains that most institutions admit a high proportion of applicants. In fact, colleges that admit around 75% of their applicants have the highest probability density. The axis labels, title, subtitle can be customized with gf_labs() in the same manner as with the histogram. The color= and fill= arguments in gf_density() will color the density curve and area under the density curve, respectively. gf_density(~ adm_rate, data = colleges, color = &#39;black&#39;, fill = &#39;yellow&#39;) %&gt;% gf_labs( x = &#39;Admission Rate&#39;, y = &#39;Probability density&#39;, title = &#39;Distribution of admission rates for 2,019 institutions of higher education.&#39;, subtitle = &#39;Data Source: U.S. Department of Education College Scorecard&#39; ) This function is a part of the tidyverse package, so you need to be sure to run library(tidyverse) prior to using read_csv().↩︎ R knows the names of 657 colors. To see these names type colors() at the command prompt.↩︎ The default kernel used in gf_density() is the normal kernel.↩︎ "],
["descriptive-statistics-numerically-describing-the-sample-data.html", "Chapter 3 Descriptive Statistics: Numerically Describing the Sample Data 3.1 Summarizing Attributes 3.2 Understanding the Median and Mean 3.3 Numerically Summarizing Variation 3.4 Summarizing Categorical Attributes 3.5 Advanced Extension: Computing Your Own Measure of Variation", " Chapter 3 Descriptive Statistics: Numerically Describing the Sample Data Data visualization is often the first step on the statistical journey to explore a research question. However, this is usually not where the journey stops, instead additional analyses are often performed to learn more about the trends and structure in the data. In this chapter we will learn about methods that useful for numerically summarizing a sample of data. These methods are commonly referred to as descriptive statistics. We will again use the data provided in the file College-scorecard-clean.csv to examine admissions rates for 2,019 institutions of higher education. As in the previous chapter, before we begin the analysis, we will load several packages that include functions we will use in the chapter. We also import the College Scorecard data using the read_csv() function. # Load packages library(tidyverse) library(ggformula) library(mosaic) library(statthink) # Set theme for plots theme_set(theme_statthinking()) # Import the data colleges &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000 ) # View first six cases DT::datatable(colleges) 3.1 Summarizing Attributes Data are often stored in a tabular format where the rows of the data are the cases and the columns are attributes. For example, in the college scorecard data (displayed above) the rows each represent a specific institution of higher education (cases) and the columns represent various attributes measured on those higher education institutions. This type of tabular representation is a common structure for storing and analyzing data. In the previous chapter, we visualized different attributes by referencing those attributes in the function we used to create a plot of the distribution. For example, when we wanted to plot a histogram of the distribution of admission rates, we referenced the adm_rate attribute in the gf_histogram() function. In a similar vein, we will obtain numerical summaries of an attribute by referencing that attribute in the df_stats() function. Below, we obtain numerical summaries for the admissions rate attribute: df_stats(~ adm_rate, data = colleges, median) ## response median ## 1 adm_rate 0.7077 The df_stats() function takes a formula syntax that is the same as the formula syntax we introduced in the previous chapter. In particular, the variable that we wish to compute a statistic on is specified after the ~. We also specify the data object with the data= argument. Finally, we include additional arguments indicating the name of the particular numerical summary (or summaries) that we want to compute.4 In the syntax above, we compute the median admission rate. The median is also referred to as the 50th percentile, and is the value at which half of the admission rates in the data are above and half are below. In our data, the medain admission rate is 70.8%. In our data 1,009 institutions have an admission rate below 70.8% and 1,009 have an admission rate above 70.8%. In the histogram below, we add a vertical line at the median admission rate to help you visualize what this means. Another common numerical summary that is often used to describe a distribution is the mean. To compute the mean admission rate we again use the df_stats() function, but include mean as our additional argument. df_stats(~ adm_rate, data = colleges, mean) ## response mean ## 1 adm_rate 0.6827355 The mean (or average) admission rate for the 2,019 institutions of higher education is 68.3%. 3.2 Understanding the Median and Mean In your previous educational experiences with the mean and median, you may have learned the formulas or algorithms that produce these values. For example: Mean: Add up all the values of the attribute and divide by the number of values; Median: Order all the values of the attribute from smallest to largest and find the one in the middle. If there is an even number of observations, find the mean of the middle two values. To better understand these summaries, we will visualize them on the distirbution of admission rates. Figure 3.1: Distribution of admission rates for thw 2,019 institutions of higher education. The mean admission rate is displayed as a red, dashed line. The mean (displayed as a red, dashed line) represents the “balance point” of the distribution. If the distribution were a physical entity, it is the location where you would put you finger underneath the distribution to balance it. In a statistical sense, we balance the distribution by “balancing” the deviations. To explain this, let’s examine a toy data set of five observations: \\[ Y = \\begin{pmatrix}10\\\\ 10\\\\ 20\\\\ 30\\\\ 50\\end{pmatrix} \\] The mean of these five values is 24. Each of these values has a deviation which is computed as the difference between the observed value and the mean value. For the toy data, \\[ Y = \\begin{pmatrix}10 - 24\\\\ 10-24\\\\ 20-24\\\\ 30-24\\\\ 50-24\\end{pmatrix} = \\begin{pmatrix}-14\\\\ -14\\\\ -4\\\\ 6\\\\ 26\\end{pmatrix} \\] Notice that some of the deviations are negative (the observation was below the mean) and some are positive (the observation was above the mean). The mean “balances” these deviations since the sum of the deviations is 0. What if we had instead looked at the deviations from the median, which is 20? \\[ Y = \\begin{pmatrix}10 - 20\\\\ 10-20\\\\ 20-20\\\\ 30-20\\\\ 50-20\\end{pmatrix} = \\begin{pmatrix}-10\\\\ -10\\\\ 0\\\\ 10\\\\ 30\\end{pmatrix} \\] The median does not balance the deviations; the sum is not zero (it is \\(+20\\)). The mean is the only value we can use to “balance” the deviations. What about the median? In the figure, half of the observations in the histogram have an admission rate below the blue line and half have an admission rate above the blue line. The median splits the distribution into two equal areas. Note that the median is not necessarily in the middle of the values represented on the \\(x\\)-axis; that would be 0.50 rather than 0.708. It is the area under the curve (or embodied by the histogram) that is halved. 3.2.1 Summarize with the Mean or Median? The goal of summarizing the distribution numerically is to provide a value that typifies or represents the observed values of the attribute. In our example, we need a value that summarizes the 2,019 admission rates. Since the mean balances the deviations, it is the representative because it is the value that is “closest” (at least on average) to all of the observations. (It is the value that produces the smallest average deviation—since the sum of deviations is 0, the average deviation is also 0.) The median is representative because half of the distribution is smaller than that value and the other half is larger. But, does one represent the distribution better than the other? Figure 3.2: Distribution of the admission rates for 2,019 institutions of higher education. The mean admission rate is displayed as a red, dashed line. The median admission rate is displayed as a blue, solid line. In this example, both values are quite similar, so either would send a simlar message about the distribution of admission rates, namely that a typical admission rate for these 2,019 institutions of higher education is around 70%. Looking at the plot, we see that the mean admission rate is lower than the median admission rate. In a left-skewed distribution this will often be the case. The mean is “pulled toward the tail” of the distribution. This is because the mean is influenced by extreme values (which in a skewed distribution are in the tail). The median is not influenced by extreme values; we say it is robust to these values. This is because in calculating the median, only the middle score (or middle two scores) are used, so its value is not informed by the extreme values in the distribution.5 In practice, it is a good idea to compute both the mean and the median and explore whether one is more representative than the other (perhaps by plotting them on the distribution). The choice of one over the other should also be guided by substantive knowledge. 3.3 Numerically Summarizing Variation In the distribution of admission rates, both the mean and median seems to offer a representative admission rate since both are close to the modal clump of the distribution. (There are several colleges that have an admission rate close to 70%.) But, you will also notice that an admission rate of 70% does not do a great job representing all of the institutions’ admissions rates. This is true for any single statistic we pick to summarize the distribution. To more fully summarize the distribution we need to summarize the variability in the distribution in addition to a “typical” or representative value. There are several summaries that statisticians and data scientists use to describe the variation in a distribution. And, like the representative summary measures, each of the summaries of variation provide slightly different information by highlighting different aspects of the variability. We will explore some of these measures below. 3.3.1 Range One measure of variation that you have almost surely encountered before is the range. This numerical measure is the difference between the maximum and minimum values in the data. To compute this we provide the df_stats() function with two additional arguments, min and max. Then we can compute the difference between these values. # Obtain minimum and maximum admission rate df_stats(~ adm_rate, data = colleges, min, max) ## response min max ## 1 adm_rate 0 1 # Compute range 1 - 0 ## [1] 1 The range of the admission rates is 1. When people colloquially describe the range, they typically provide the limits of the data rather than actually providing the range. For example, they may describe the range of the admission rates as: “the admission rates range from 0 to 1”. While this is technially not the range (which is a single number), it is probably more descriptive as it also gives a sense of the lower- and upper-limits for the observations. One problem with the range is that if there are extreme values, the range will not give an accurate picture of the variation encompassing most observations. For example, consider the following five test scores: \\[ Y = \\begin{pmatrix}30\\\\ 35\\\\ 36\\\\ 37\\\\ 100\\end{pmatrix} \\] The range of these data is 70, indicating that the variation between the lowest and highest score is 70 points, suggesting a lot of variation in the scores. The range, however, is clearly influenced by the score of 100. Were it not for that score, we would have a much different take on the score variability; the other scores are between 30 and 37 (a range of 7), suggesting that there is not a lot of differenes in the test scores.6 While the range is perhaps not the best measure of variation, it is quite useful as a validity check on the data to ensure that the attribute’s values are theoretically possible. In this case the values are all between 0 and 1, which are values that are theoretically plausible for admission rate. 3.3.2 Percentile Range One way to deal with extreme values in the sample is simply to not include them when we calculate the range. For example, instead of computing the difference between the maximum and minimum value in the data (which includes extreme values), truncate the bottom 10% and upper 10% of the data and calculate the range between the remaining maximum and minimum values. This is essentially the range of the middle 80% of the data. To compute the endpoint after truncatng the lower- and upper 10% we will use the quantile() function. This function finds the data value for an associated percentile provided to the function. If we wnat to truncate the lower- and upper 10% of a distribution we are interested in finding the values associated with the 10th and 90th percentiles. The syntax below shows two manners for obtaining these values for the admissions rate attribute. # Provide both percentiles separately colleges %&gt;% df_stats(~ adm_rate, quantile(0.10), quantile(.90)) ## response 10% 90% ## 1 adm_rate 0.39284 0.94706 # Provide both percentiles in a single quantile() call colleges %&gt;% df_stats(~ adm_rate, quantile(c(0.1, 0.9))) ## response 10% 90% ## 1 adm_rate 0.39284 0.94706 # Compute the range ofthe middle 80% of the data 0.94706 - 0.39284 ## [1] 0.55422 The range of admissions rates for 80% of the 2,019 institutions of higher education is 0.554. We can visualize this by adding the percentiles on the plot of the distribution of admission rates. These values seem to visually correspond to where most of the data are concentrated. Figure 3.3: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 10th and 90th percentiles, respectively. 3.3.3 Interquartile Range (IQR) One percentile range that statisticians and data scientists use a great deal is the interquartile range (IQR). This range demarcates the middle 50% of the distribution; it truncates the lower and upper 25% of the values. In other words it is based on finding the range between the 25th- and the 75th-percentiles. # Obtain values for the 25th- and 75th percentiles colleges %&gt;% df_stats(~ adm_rate, quantile(c(0.25, 0.75))) ## response 25% 75% ## 1 adm_rate 0.5524 0.83815 # Compute the IQR 0.83815 - 0.5524 ## [1] 0.28575 The range of admission rates for the middle 50% of the distribution is 28.5%. Since it is based on only 50% of the observations, the IQR no longer gives the range for “most” of the data, but, as shown in the plot below, this range encompasses the modal clump of institutions’ admission rates and can be useful for describing the variation. Figure 3.4: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 25th and 75th percentiles, respectively. Since the IQR describes the range for half of the observations, it can also be useful to compare this range with the entire range of the data. Below we compute these values and visualize them on a histogram of the distribution. # Obtain values for the 25th- and 75th percentiles colleges %&gt;% df_stats(~ adm_rate, min, quantile(c(0.25, 0.75)), max) ## response min 25% 75% max ## 1 adm_rate 0 0.5524 0.83815 1 # Compute the IQR 0.83815 - 0.5524 ## [1] 0.28575 # Compute the range 1 - 0 ## [1] 1 Figure 3.5: Distribution of the admission rates for 2,019 institutions of higher education. The solid, red lines are placed at the 25th and 75th percentiles, respectively. The dashed, blue lines are placed at the minimum and maximum values, respectively. Although our sample of 2,019 institutions of higher education have wildy varying admissions rates (from 0% to 100%), the middle half of those institutions have admissions rates between 55% and 84%. We also note that the 25% of institutions with the lowest admissions rate range from 0% to 55%, while the 25% of institutions with the highest admissions rate range from only 84% to 100%. This means that there is more variation in the admissions rates in the institutions with the lowest admissions rate than in the institutions with the highest admissions rates. Understanding how similar the range of variation is in these areas of the distribution can give us information about the shape f the distribution. For example, the bigger range in the lowest 25% of the data suggests that the distribution has a tail on the left side. Seventy-five percent of the institutions have admissions rates higher than 50%. These two features suggest that the distribtion is left-skewed (which we also see in the histogram). When we describe the shape of a distribution, we are actually describing the variability in the data! Examining the lowest 25%, highest 25%, and middle 50% of the data is so common that a statistician named John Tukey invented a visualization technique called the box-and-whiskers plot to show these ranges. To create a box-and-whiskers plot we use the gf_boxploth() function.7 This function takes a formula that is slightly different than we have been using, namely 0 ~ attribute name. (Note that the 0 in the formula is where the box-and-whiskers plot is centered on the y-axis.) gf_boxploth(0 ~ adm_rate, data = colleges, fill = &quot;skyblue&quot;) %&gt;% gf_labs(x = &quot;Admission rate&quot;) The “box”, etxending from 0.55 to 0.84, depicts the interuartile range; the middle 50% of the distribution. The line near the middle of the box is the median value. The “whiskers” extend to either the end of the range, or the next closest observation that is not an extreme value. (There are several extreme values on the left-hand side of the distribution representing institutions with extremely low admission rates.) The length of the whisker denotes the range of the lowest 25% of the distribution and the highest 25% of the distribution. We can also display both the histogram and the box-and-whiskers plot. In the syntax below, we center the box-and-whiskers plot at \\(y=170\\). We also make the box slightly wider to display better in the plot. gf_histogram(~ adm_rate, data = colleges, bins = 30) %&gt;% gf_boxploth(170 ~ adm_rate, data = colleges, fill = &quot;skyblue&quot;, width = 10) %&gt;% gf_labs(x = &quot;Admission rate&quot;, y = &quot;Frequency&quot;) 3.3.4 Empirical Cumulative Density The percentile range plots and the boxplot indicated the values of the distribution that demarcated a particular proportion of the distribution. For example, the boxplot visually showed the admission rates that were at the 25th, 50th, and 75th percentiles. Another plot that can be useful for understanding how much of a distribution is at or below a particular value is a plot of the empirical cumulative density. To create this plot we use the gf_ecdf() function from the ggformula package. gf_ecdf(~ adm_rate, data = colleges) %&gt;% gf_labs(x = &quot;Admission rate&quot;, y = &#39;Cumulative proportion&#39;) To read this plot, we can map admission rates to their associated cumulative proportion. For example, one-quarter of the admission rates in the distribution are at or below 0.55; that is the admission rate of 0.55 has an associated cumulative proportion of 0.25. Similarly, an admission rate of 0.71 is associated with a cumulative proportion of 0.50; one-half of the admission rates in the distribution are at or below the value of 0.71. 3.3.5 Variance and Standard Deviation Two measures of variation that are commonly used by statisticians and datd scientists are the variance and the standard deviation. These can be obtained by including var and sd, respectively, in the df_stats() function. # Compute variance and standard deviation colleges %&gt;% df_stats(~ adm_rate, var, sd) ## response var sd ## 1 adm_rate 0.04467182 0.2113571 The variance and standard deviation are related to each other in that if we square the value of the standard deviation we obtain the variance. # Square the standard deviation 0.2113571 ^ 2 ## [1] 0.04467182 In general, the standard deviation is more useful for describing the variation in a sample because it is in the same metric as the data. In our example, the metric of the data is proportion of students admitted, and the standard deviation is also in this metric. The variance, as the square of the standard deviation, is in the squared metric—in our example, proportion of students admitted squared. While this is not a useful metric in description, it does have some nice mathematical properties, so it is also a useful measure of the variation. 3.3.5.1 Understanding the Standard Deviation To understand how we interpret the standard deviation, it is useful to see how it is calculated. To do so, we will return to our toy data set: \\[ Y = \\begin{pmatrix}10 \\\\ 10\\\\ 20\\\\ 30\\\\ 50\\end{pmatrix} \\] Recall that earlier we computed the deviation from the mean for each of these observations, and that these deviations was a measure of how far above or below the mean each observation was. \\[ Y = \\begin{pmatrix}10 - 24\\\\ 10-24\\\\ 20-24\\\\ 30-24\\\\ 50-24\\end{pmatrix} = \\begin{pmatrix}-14\\\\ -14\\\\ -4\\\\ 6\\\\ 26\\end{pmatrix} \\] A useful measure of the variation in the data would be the average of these deviations. This would tell us, on average, how far from the mean the data are. Unfortunately, if we were to compute the mean we would get zero because the sum of the deviations is zero. (That was a property of the mean discussed earlier in the chapter!) To alleviate this, we square the deviations before summing them. \\[ \\begin{pmatrix}-14^2\\\\ -14^2\\\\ -4^2\\\\ 6^2\\\\ 26^2\\end{pmatrix} = \\begin{pmatrix}196\\\\ 196\\\\ 16\\\\ 36\\\\ 676\\end{pmatrix} \\] The sum of these squared deviations is 1,120. And the average squared deviation is then \\(1120/5 = 224\\). If we take the square root of 224, which is 14.97, we now have the average deviation for the five observations. On average, the observations in the distribution are 14.97 units from the mean value of 24. The standard deviation is interpreted as the average deviation from the mean.8 3.3.5.2 Using the Standard Deviation In our example, the mean admission rate for the 2,019 institutions of higher education was 68.2%, and the standard deviation was 21.1%. We can combine these two pieces of information to make a statement about the admission rates for most of the institutions in our sample. In general, most observations in a distribution fall within one standard deviation of the mean. So, for our example, most institutions have an admission rate that is between 47.1% and 89.3%.9 # 1 SD below the mean 0.682 - 0.211 ## [1] 0.471 # 1 SD above the mean 0.682 + 0.211 ## [1] 0.893 3.4 Summarizing Categorical Attributes Categorical attributes are attributes that have values that represent categories. For example, the attribute region indicates the region in the United States where the institution is located (e.g., Midwest). The attribute bachelor_degree is a categorical value indicating whether ir not the institution offers a Bachelor’s degree. Sometimes statisticians anddata scientists use the terms dichotmous (two categories) and polychotomous (more than two categories) to further define categorical variables. Using this nomenclature, region is a polychotomous categorical variable and bachelor_degree is a dichotomous categorical variable. Sometimes analysts use numeric values to encode the categories of a categorical attribute. For example, the attribute bachelor_degree is encoded using the values of 0 and 1. It is important to note that these values just indicate whether the institution offers a Bachelor’s degree (1) or not (0). The values are not necessarily ordinal in the sense that a 1 means more of the attribute than a 0. Since the values just refer to categories, an analyst might have reversed the coding and used 0 to encode institutions that offer a Bachelor’s degree and 1 to encode those institutions that do not. Similarly, the values of 0 and 1 are not sancrosanct; any two numers could have been used to represent the categories.10 Most of the time, the numerical summaries we computed earlier in the chapter do not work so well for categorical attributes. For example, it would not make sense to compute the mean region for the institutions. In general, it suffices to compute counts and proportions for the categories included in these attributes. To compute the category counts we use the tally() function. This function takes a formula indicating the name of the categorical attribute and the name of the data object. To find the category counts for the region attribute: # Compute category counts tally(~region, data = colleges) ## region ## Far West Great Lakes Mid East New England ## 221 297 458 167 ## Outlying Areas Plains Rocky Mountains Southeast ## 35 200 50 454 ## Southwest US Service Schools ## 133 4 To find the proportion of institutions in each region, we can divide each of the counts by 2,019. # Compute category proportions tally(~region, data = colleges) / 2019 ## region ## Far West Great Lakes Mid East New England ## 0.109460129 0.147102526 0.226844973 0.082714215 ## Outlying Areas Plains Rocky Mountains Southeast ## 0.017335315 0.099058940 0.024764735 0.224863794 ## Southwest US Service Schools ## 0.065874195 0.001981179 You could also compute the proportions directly with the tally() function by specifying the argument format = \"proportion\". # Compute category proportions tally(~region, data = colleges, format = &quot;proportion&quot;) ## region ## Far West Great Lakes Mid East New England ## 0.109460129 0.147102526 0.226844973 0.082714215 ## Outlying Areas Plains Rocky Mountains Southeast ## 0.017335315 0.099058940 0.024764735 0.224863794 ## Southwest US Service Schools ## 0.065874195 0.001981179 3.5 Advanced Extension: Computing Your Own Measure of Variation If you have another non-standard measure of variation that you want to compute, you can always write your own function to compute it. For example, say you wanted to compute the mean absolute error (the mean of the absolute values of the deviations). To compute the mean absolute error, we first need to define a new function. mae &lt;- function(x, na.rm = TRUE, ...) { avg &lt;- mean(x, na.rm = na.rm, ...) abs_avg &lt;- abs(x - avg) mean(abs_avg) } We can now use this new function by employing it as an argument in the df_stats() function. colleges %&gt;% df_stats(~ adm_rate, mae) ## response mae ## 1 adm_rate 0.1692953 Note that the names of the summaries we include in the df_stats() function need to be the actual names of functions that R recognizes.↩︎ The downside of using the median is that it is only informed by one or two observations in the data. The mean is informed by all of the observations. This property of the mean makes it a more useful than the median in some mathematical and theoretical applications.↩︎ A second issue with range is that it is a biased statistic. If we use it as an estimate of the population range, it will almose inevitably be too small. The population range will almost always be larger since the sampling process will often not select extreme population values.↩︎ The gf_boxplot() function creates vertical a box-and-whiskers plot, and the gf_boxploth() function creates a horizontal box-and-whiskers plot.↩︎ Technically, after summing the squared deviations, we divide this sum by \\(n-1\\) rather than \\(n\\). But, when the sample size is even somewhat large, this difference is trivial.↩︎ If we know the exact shape of the distribution we can be more specific about the proportion of the distribution that fall within one standard deviation of the mean.↩︎ Using 0’s and 1’s for the encoding does have some advantages over other coding schemes which we will explore later when fitting statistical models.↩︎ "],
["multivariate-visualization.html", "Chapter 4 Multivariate Visualization 4.1 Multivariate Distributions 4.2 Faceting 4.3 Multivariate Descriptive Statistics", " Chapter 4 Multivariate Visualization library(tidyverse) library(ggformula) library(statthink) # Add plot theme theme_set(theme_statthinking()) Real world data are never as simple as exploring a distribution of a single variable, particularly when trying to understand individual variation. In most cases attributes interact with one another, move in tandem, and many phenomena help to explain the attribute of interest. For example, when thinking about admission rates of higher education institutions, what may be some important attributes that would explain some of the reasons why higher education institutions differ in their admission rates? When thinking about the high temperature for the given day, what attributes would be helpful to understand why the high temperature on a day is different? Take a few minutes to brainstorm some ideas. Building off of the high temperature example, we will use weather data from two seasons of cooler months of the year, October through April of 2018-2019 and 2019-2020, of various locations around the United States. This data was downloaded from the National Centers for Environmental Information (NCEI) Climate Data Online portal and is part of the companion package, statthink. The locations extracted in these data are found in the northern part of the United States and include: c(“Buffalo, NY”, “Iowa City, IA”, “Chicago, IL”, “Boston, MA”, “Portland, ME”, “Minneapolis, MN”, “Duluth, MN”, “Detroit, MI”). The first few rows of the data are shown below. DT::datatable(us_weather) 4.1 Multivariate Distributions Before moving to multivariate distributions, first, we will explore a univariate distribution of the average daily temperature. The average daily temperature takes the recorded temperatures from a single day and averages those, therefore this value would fall somewhere between the high and low temperature for the day. What are some key characteristics of the average daily temperature distribution? Take a few minutes to summarize the key characteristics. gf_histogram(~ drybulbtemp_avg, data = us_weather, bins = 30) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Univariate distribution of avg daily temperature&quot;) From the univariate figure of the average daily temperature, you may have noticed that there is variation in these values, with a fairly wide range considering the values. For example, there are some average daily temperatures that are below zero and some of 60 degree Fahrenheit. However, most values are between 20 and 45 or so, which is reasonable given these are cooler months of the year and mostly northern location in the United States. Finally, the distribution is unimodal and somewhat symmetric, at least not skewed enough to be too concerned. Earlier the question was asked what other attributes may help to explain why there are variations in high temperatures. If you did not think about that question then, we encourage you to think about this question now. What attributes could help to understand why there is such a wide range of values in the average daily temperature? There are many answers that could be informative to this discussion, the one we will explore first is whether it snowed on a given day. This could influence the average temperature for a few reasons, first when it snows it must be cold enough for the precipitation to stay in frozen form rather than melting as it falls being at or below freezing (32 degrees Fahrenheit);11 also, if it snowed during the day it would also be less likely for the sun to be out further making the temperature less variable and likely lower during the winter months. 4.1.1 Histograms Below is the multivariate distribution of the average daily temperature by whether it snowed or not at some point during that day. Whether it snowed or not is depicted by color in the figure, the blue color is showing the distribution of average daily temperature for days where it snowed and red is otherwise. Before we interpret the figure in more detail, the code for making this change was done by adding the snow attribute to the fill aesthetic using the formula syntax: fill = ~ snow. The fill aesthetic is telling the histogram bars to be colored by the different categories of the attribute of interest, here snow. The fill attribute for histrograms is best used when the attribute only has a few categories. gf_histogram(~ drybulbtemp_avg, data = us_weather, bins = 30, fill = ~ snow) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, fill = &quot;Snow?&quot;) If you were to compare the distribution for the average daily temperature for days when it snowed compared to when it did not snow, what similarities and differences do you notice? Take a few minutes to try and interpret this figure, focusing on characteristics that are similar or different across days when it snowed or not. You may notice first that the bars are higher for the days that did not snow compared to days that it did snow. Why might this be occurring? If you said the reason is that it tends to snow less frequently, you would be correct. The bars are higher for days when it did not snow due to more data (i.e., days) where it did not snow. This characteristic of the histogram in this example makes the histogram more difficult to interpret and can even mislead when making comparisons across the two groups. We will explore a solution to this problem soon. Another observation you may have noticed is that it appears the temperature tends to be lower for days in which it snows compared to those that it did not snow. On average, if you estimated the location of the mean for the two groups, the average daily temperature for days where it snowed would likely be just under 30 degrees where as it would be around 45 degrees Fahrenheit for days it did not snow. You may have also noticed that the variability for days when it snowed was also lower compared to days where it did snow. Why might this occur? This is likely due to the temperature ranges where snow forms readily. You may wonder, why are there days where the average daily temperature was about 45 degrees Fahrenheit when it generally needs to be lower than 32 degrees Fahrenheit for it to snow? There could be many explanations for this, but since the distribution is showing the average daily temperature over a 24 hour period, the weather can change quite drastically over this period. For example, it could snow at the beginning or end of the day and the rest of the day could be quite different and much warmer than when it snowed. Finally, you may also notice that the distribution for days when it snowed is somewhat more left skewed compared to days in which it does not snow. As snow readily forms when it is cooler, typically less than 32 degrees Fahrenheit, the temperature would commonly fall below this value when it snows. Therefore, the average temperature would be restricted by those colder values, pulling the average lower, even if it was warmer earlier or later in the day. This restriction is not found for days when it does not snow, therefore the temperatures are able to be found over a wider range of temperature values. 4.1.2 Density Curves Often density plots are easier to visualize when there are more than one group. The other benefit of moving to a density plot is that any sample size differences across the groups are normalized automatically, which does not occur by default with the histogram. To start, we will explore the average daily temperature based on whether it snowed that day or not. For the first density curve, we will use almost the same code as the multivariate histogram. The only change in the code is to use gf_density() instead of gf_histogram(). Take a few minutes to summarize important differences in this figure compared to the multivariate histogram. gf_density(~ drybulbtemp_avg, data = us_weather, size = 1, fill = ~ snow) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, fill = &quot;Snow?&quot;) Notice that there are two density curves, each shaded a different color, light blue for days in which it snowed, and light red for days where it did not snow. One primary difference, between the density and histogram is that the curves are normalized for sample size differences, therefore differences in heights for the two density curves can be interpreted as there being more data at that location. For example, the curve for days it snowed has a higher peak, around 30 degrees Fahrenheit, compared to the peak for days it did not snow, around 45 degrees. The higher peak here means there is more data clustered around 30 degrees for the snowy days compared to the amount of data clustered around 45 degrees for the days it did not snow. The normalization of sample size across the groups helps to accentuate the key differences. Namely, there are differences in where the center and amount of variation between the two groups. When plotting two groups with a histogram, the groups are plotted over top of each other which can further mask differences in the group that is plotted first. The density curve with some transparency, as depicted above, does not suffer from these problems. There is another plotting aesthetic that is useful to know about when using density curves. This is the color aesthetic which changes the colors of the line of the density curve. Similar to the fill aesthetic, it is possible to have the lines change color based on the snow attribute as such: color = ~ snow. The code below does this and also makes the lines slightly larger to view easier with the size = 1 global aesthetic. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ snow, size = 1) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, color = &quot;Snow?&quot;) Without specifying the fill aesthetic, both groups have the same fill color below the lines, but notice that now the lines are colored instead of the entire curve being filled in. Areas that are darker gray are areas where the two groups overlap. The rest of the density curves are the same besides the appearance differences. It is also possible to combine the color and fill aesthetics, even with the same attribute. For example, the code below using the snow attribute for both the color and fill aesthetics. The primary difference here is that the lines are a bit larger compared to the first density figure shown. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ snow, size = 1, fill = ~ snow) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, fill = &quot;Snow?&quot;, color = &quot;Snow?&quot;) It is also possible to use both aesthetics, but set one to be constant rather than adding the attribute to it. When there are many groups, this can be a good way to visualize more groups and not have too much color happening. In the code below, the snow attribute is specified to the color aesthetic, but now the fill aesthetic is set to a specific color, this time a shade of gray. The grayscale ranges from 0 to 100, where 0 is black and 100 is white, therefore in this case when setting fill = 'gray75' this would be a lighter gray as the number is closer to 100. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ snow, size = 1, fill = &#39;gray75&#39;) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, title = &quot;Multivariate distribution of avg daily temperature by whether it snowed&quot;, color = &quot;Snow?&quot;) This results in a figure that is a bit lighter than the default gray color used in density plots. This lighter gray makes it a bit easier to see the density curves that are plotted behind. When there are more than two groups, this can be an important consideration to take into account to ensure the visualization is easier to interpret. Take the example below which instead of showing whether it snowed it shows the different locations. gf_density(~ drybulbtemp_avg, data = us_weather, color = ~ location, size = 1, fill = &#39;gray85&#39;) %&gt;% gf_labs(x = &quot;Average daily temperature, in Fahrenheit&quot;, color = &quot;&quot;) Figure 4.1: Multivariate distribution of avg daily temperature by whether it snowed In this figure, the grayscale was changed from 75 to 85 as well, but you’ll notice that it is still difficult to view all of the locations. This is made more difficult here due to the locations having similar distributional centers. The next section will show an alternative visualization that can help when density curves are difficult to interpret. 4.1.3 Violin Plots Violin plots are another way to make comparisons of distributions across groups. Violin plots are also easier to interpret when there are more than two groups on a single graph. Violin plots are density plots that are mirrored to be fully enclosed. Procedurally, the density curve that was shown above is mirrored or flipped across the x-axis. Let’s explore an example that creates a violin plot with the gf_violin() function and a two-sided formula, drybulbtemp_avg ~ snow which will visualize the average daily temperature by whether it has snowed or not. gf_violin(drybulbtemp_avg ~ snow, data = us_weather) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, title = &#39;Multivariate distribution of avg daily temperature by whether it snowed&#39;, x = &quot;Snow?&quot;) By default, the violin plots are oriented vertically, where the temperature is on the y-axis and whether it snowed or not is on the x-axis. This is opposite of what we have been using with histograms and density curves. Fortunately, we can change this behavior by adding a single line of code, gf_refine(coord_flip()). This command flips the x- and y- axes to place the average daily temperature on the x-axis. Throughout the rest of the book, any violin plots shown will have the default axes flipped for consistency with the histograms and density plots. gf_violin(drybulbtemp_avg ~ snow, data = us_weather) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, title = &#39;Multivariate distribution of avg daily temperature by whether it snowed&#39;, x = &quot;Snow?&quot;) %&gt;% gf_refine(coord_flip()) Violin plots are depicted for each group separately. This means that there will be the same number of violin plots as the number of groups that are in the attribute on the right hand side of the equation specified inside gf_violin(). In the example above, there is a violin plot for days it snowed and second one for days it did not snow and these violin plots are shown separately and never stacked on top of one another. This is the feature that makes these figures able to handle a larger number of groups more efficiently compared to density of histograms. From the figure, similar findings discussed earlier can be articulated. For example, there is a higher center and more variation on days that it did not snow. There is also evidence that the distribution for days that did snow is left-skewed whereas the distribution for days it did not snow is more symmetric. Aesthetically, these figures are a bit more pleasing to look at if they include a light fill color. This is done similar to the density plots shown above with the fill = argument, specified as fill = 'gray85'. gf_violin(drybulbtemp_avg ~ snow, data = us_weather, fill = &#39;gray85&#39;) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, title = &#39;Multivariate distribution of avg daily temperature by whether it snowed&#39;, x = &quot;Snow?&quot;) %&gt;% gf_refine(coord_flip()) Percentiles are another useful feature to aid in the comparison across groups with violin plots. These can be added with the draw_quantiles argument. In the below code, three percentiles are shown, the 10th, 50th, and 90th percentiles are added with the code, draw_quantiles = c(0.1, 0.5, 0.9). Notice that the percentiles are represented as a proportion instead of as a percentage. gf_violin(drybulbtemp_avg ~ snow, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Snow?&quot;) %&gt;% gf_refine(coord_flip()) Figure 4.2: Multivariate distribution of average daily temperature by whether it snowed When the percentiles are added, it is now easier to compare the center of the two distributions by using the middle vertical line in each violin plot which represents the 50th percentile or median. In this violin plot, the 50th percentile is lower for days it did snow compared to days it did not snow. Furthermore, the 90th percentile is also lower than the 50th percentile for days that it did snow. This helps to provide evidence that days that it did snow tend to be much colder than typical days, as shown by the 50th percentile, when it did not snow. Comparing the distance between the 10th and 90th percentiles for each group can also give a sense of the range of typical values. Although this isn’t the interquartile range,12 the difference between the 10th and 90th percentiles reflects a similar quantity. For days that it snowed, the 10th percentile is around 10 to 12 degrees Fahrenheit and the 90th percentile is around 37 to 40 degrees Fahrenheit. Therefore the difference would be about 25 to 30 degrees Fahrenheit. Doing the same process for days it did not snow, the 10th percentile would be around 24 degrees Fahrenheit and the 90th percentile would be around 55 degrees Fahrenheit. The difference would then be around 31 degrees Fahrenheit. This suggests that the spread of the middle 80% of the data in each group are similar, with days in which it snowed showing some evidence of being slightly less spread out over this middle 80%. 4.1.3.1 Violin Plots with many groups As discussed earlier, visualizing many groups can be done more easily using violin plots compared to density or histograms. This is shown below where the average daily temperature is shown for each of the 8 locations in the US weather data. The 10th, 50th, and 90th percentiles are shown as well for comparison. Take a minute to compare this figure to the one with overlapping density curves in Figure 4.1. gf_violin(drybulbtemp_avg ~ location, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Location&quot;) %&gt;% gf_refine(coord_flip()) Figure 4.3: Average daily temperature by the location of weather measurement. 4.2 Faceting Faceting is a graphical procedure to create multiple figures side by side. This is helpful when there are more than 2 attributes to explore. For example, we have explored the distribution of average daily temperature by whether it snowed or not and also the location separately. Perhaps, there are differences in the distribution of average daily temperatures for whether it snowed or not based on the location. This multivariate visualization now considers two attributes that may help to understand why the average daily temperature may differ. Below is an example of adding the faceting to a violin plot. The violin plot will explore the average daily temperature by the location. This figure was explored initially in Figure 4.3. This new figure (Figure 4.4) adds one line of code to do the faceting, gf_facet_wrap(~ snow). You may notice that inside the gf_facet_wrap() function, a similar formula notation used by other plotting functions is used. Within this function, a one-sided formula is used where the attribute name is specified after the ~. gf_violin(drybulbtemp_avg ~ location, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Location&quot;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_facet_wrap(~ snow) Figure 4.4: Average daily temperature by the location of weather measurement and whether it snowed on a given day. From the figure, the primary difference between Figure 4.3 is the creation of two side-by-side plots, one representing observations when it did not snow (left-side plot) and the other representing observations when it did snow that day (right-side plot). If the two figures were combined, the same figure as in Figure 4.3 happen. The faceting is splitting the observations based on this new attribute, therefore each panel of the figure has less data compared to the overall that did not split the figure by whether it snowed or not. There are a few differences shown in the figure, primarily related to variation differences. For instance, the variation of average daily temperatures is much smaller for Detroit when it snows compared to when it does not snow. These variation differences are not as pronounced for a city like Duluth or Minneapolis. Similar to previous discussions, there does not seem to be large differences in average daily temperature across the locations. As a contrast, one instance that may have a larger impact than location, would be month of the year. Below is a faceted figure that instead of location, adds month of year. gf_violin(drybulbtemp_avg ~ month, data = us_weather, fill = &#39;gray85&#39;, draw_quantiles = c(.1, .5, .9)) %&gt;% gf_labs(y = &quot;Average daily temperature, in Fahrenheit&quot;, x = &quot;Month&quot;) %&gt;% gf_refine(coord_flip()) %&gt;% gf_facet_wrap(~ snow) Figure 4.5: Average daily temperature by the month of weather measurement and whether it snowed on a given day. 4.3 Multivariate Descriptive Statistics We’ve spent a lot of time trying to reason about other attributes that may be important in explaining variation in our attribute of interest. We started this chapter by exploring visually other attributes that help to understand or explain variation in the primary outcome. For example, we visually explored attributes that may help us understand why average daily temperatures may differ. We found that if it snows on a given day has an impact and month has an impact, but location does not seem to have as large of an effect. In this section, we will revisit descriptive statistics, or single number summaries of key elements of a distribution. The idea of descriptive statistics will now be generalized to be multivariate in nature. Practically, this means that we will no longer use a single numeric quantity as was done in Chapter 3, instead, we will consider multiple numeric quantities for a single distribution. More accurately, we are now calculating conditional descriptive statistics for conditional distributions. For example, if you look back at Figure 4.5, we conditioned the distribution on month and whether it snowed or not. The conditioning means that the distribution of the average daily temperature was split into separate distributions for each unique combination of the month and snow attributes. In the case of descriptive statistics, we will now compute single number summaries for those conditional or separate distributions. Let’s jump in with a concrete example that computes the multivariate (conditional) descriptive statistics that we explored in Figure 4.2. Recall from Chapter 3, to compute descriptive statistics the df_stats() function is used. The primary arguments to this function are a formula indicating the attributes to consider followed by the statistics to be calculated. In Chapter 3, the formula used for the df_stats() function were all one-sided as these were univariate descriptive statistics. The formula when calculating multivariate (conditional) descriptive statistics will be two-sided of the following form primary-attribute ~ conditional-attributes. In this formula specification, the primary attribute would represent the attribute the descriptive statistics will be computed for or the attribute of interest. The conditional attribute(s) represent the attribute that the data will be conditioned or split on. Looking at the next code chunk, notice that the formula specification is drybulbtemp_avg ~ snow which means that the descriptive statistics will be computed for the average daily temperature or the primary attribute and this computation will be done for each unique value of the snow attribute. As we know, the snow attribute takes on two unique values, either it snowed or it did not snow. Therefore, the resulting output will contain two rows, one for days in which it did snow and one for when it did not snow. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow, median) ## response snow median ## 1 drybulbtemp_avg No 41 ## 2 drybulbtemp_avg Yes 29 Presented above are the conditional medians for the average daily temperature for days in which it snowed (Yes) compared to days it did not snow (No). These are the middle lines shown in Figure 4.2, but now explicitly calculated. The data were split into subgroups and the median was computed for each of those subgroups instead of pooling all observations. One thing that is useful to add in when computing conditional statistics, is how many data points are in each group. This is particularly useful when the groups are different sizes, which commonly occurs. To do this, we can add another function, length, to the df_stats() function. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow, median, length) ## response snow median length ## 1 drybulbtemp_avg No 41 2392 ## 2 drybulbtemp_avg Yes 29 1008 This adds another columns which represents the number of observations that went into the median calculation for each group and that there are many more days for these locations where it did not snow. The syntax above also shows that you can add additional functions separated by a comma in the df_stats() function and are not limited to a single function. We will take advantage of this feature later on. 4.3.1 Adding additional groups What if we thought more than one attribute was important in explaining variation in the outcome? These can also be added to the df_stats() function for additional conditional statistics. The key is to add another attribute to the right-hand side of the formula argument. Each attribute on the right-hand side of the ~ are separated with a + symbol. The following example computes the median and number of observations for the average daily temperature based on the unique values of whether it snowed by each month. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow + month, median, length) ## response snow month median length ## 1 drybulbtemp_avg No Oct 50.0 466 ## 2 drybulbtemp_avg Yes Oct 37.0 30 ## 3 drybulbtemp_avg No Nov 38.0 309 ## 4 drybulbtemp_avg Yes Nov 30.0 171 ## 5 drybulbtemp_avg No Dec 33.0 335 ## 6 drybulbtemp_avg Yes Dec 29.0 161 ## 7 drybulbtemp_avg No Jan 31.0 238 ## 8 drybulbtemp_avg Yes Jan 24.0 258 ## 9 drybulbtemp_avg No Feb 30.0 256 ## 10 drybulbtemp_avg Yes Feb 23.5 200 ## 11 drybulbtemp_avg No Mar 40.0 381 ## 12 drybulbtemp_avg Yes Mar 32.0 115 ## 13 drybulbtemp_avg No Apr 47.0 407 ## 14 drybulbtemp_avg Yes Apr 36.0 73 From the output, there are 14 rows which coincide with the number of months times 2. The 2 here represents the number of categories for the snow attribute, being if it snowed or not. In this case each month by snow combination has data, but if a combination of the attributes on the right-hand side of the equation do not have any data, those rows would not show up in the descriptive statistic summary table. These multivariate descriptive statistics coincide with Figure 4.5, specifically the middle line in these violin plots. 4.3.2 Adding more descriptive statistics Chapter 3 discussed how to add more descriptive statistics to the df_stats() function. This section serves as a reminder for how to do that. In this chapter, we have only computed the median and number of observations, these are useful statistics, but it is often useful to compute descriptive statistics of variation or other location based statistics such as percentiles or means. To do this, we can continue adding function separated by commas directly at the end of the df_stats() function. For example, we can add the mean and standard deviation to the previous multivariate descriptive statistics tables by adding , mean, sd to the end of the previous df_stats() function. us_weather %&gt;% df_stats(drybulbtemp_avg ~ snow + month, median, length, mean, sd) ## response snow month median length mean sd ## 1 drybulbtemp_avg No Oct 50.0 466 51.25751 8.739614 ## 2 drybulbtemp_avg Yes Oct 37.0 30 37.03333 4.278925 ## 3 drybulbtemp_avg No Nov 38.0 309 37.52427 9.009927 ## 4 drybulbtemp_avg Yes Nov 30.0 171 28.77778 7.512952 ## 5 drybulbtemp_avg No Dec 33.0 335 32.60000 9.318451 ## 6 drybulbtemp_avg Yes Dec 29.0 161 27.20497 8.892495 ## 7 drybulbtemp_avg No Jan 31.0 238 28.51681 13.316250 ## 8 drybulbtemp_avg Yes Jan 24.0 258 20.96899 12.657990 ## 9 drybulbtemp_avg No Feb 30.0 256 28.01172 12.269676 ## 10 drybulbtemp_avg Yes Feb 23.5 200 22.74500 9.545447 ## 11 drybulbtemp_avg No Mar 40.0 381 38.30446 10.005878 ## 12 drybulbtemp_avg Yes Mar 32.0 115 29.31304 9.812600 ## 13 drybulbtemp_avg No Apr 47.0 407 47.91646 7.817367 ## 14 drybulbtemp_avg Yes Apr 36.0 73 36.31507 4.918437 One thing to note when adding more statistics to the df_stats() function is the order with which these show up. The statistics computed show up in the order specified, therefore the statistics from the following command will be median, number of observations (length), mean, and then standard deviation. For more information on why it snows instead of rains, this is an informative description from the National Snow &amp; Ice Data Center↩︎ The interquartile range is the difference between the 25th and 75th percentiles↩︎ "],
["classification.html", "Chapter 5 Classification 5.1 Topic: Decision Trees", " Chapter 5 Classification Classification is a task that tries to predict group membership using the data attributes available. For example, one could try to predict if an individual is left or right handed based on their preferences to music or hobbies. In this situation, the classification technique would look for patterns in the music or hobby preferences to help differentiate between those that are left or right handed. Perhaps the data would show that left handed individuals would be more likely to be artistic, therefore those that rated more highly artistic tasks would be more likely to be classified as left handed. To perform the classification tasks in this chapter, we are going to consider a group of statistical models called decision trees, or more specifically in this case, classification trees. Statistical models are used to help us as humans understand patterns in the data and estimate uncertainty. Uncertainty comes from the variation in the data. For example, those that are left handed are likely not all interested or like artistic hobbies or tasks, but on average maybe they are more likely to enjoy these tasks compared to right handed individuals. Statistical models help us to understand if the differences shown in our sample of data are due to signal (true differences) or noise (uncertainty). In the remaining sections of this chapter, we will build off of this idea of statistical models to understanding how these work with classification trees to classify. Furthermore, we will aim to develop heuristics to understand if our statistical model is practically useful. That is, does our model help us to do our classification task above just randomly guessing. We will use a few additional packages to perform the classification tasks, including rpart, rpart.plot, and rsample. The following code chunk loads all the packages that will be used in the current chapter. library(tidyverse) library(ggformula) library(statthink) library(rpart) library(rpart.plot) library(rsample) remotes::install_github(&quot;grantmcdermott/parttree&quot;) library(parttree) # Add plot theme theme_set(theme_statthinking()) us_weather &lt;- mutate(us_weather, snow_factor = factor(snow)) 5.1 Topic: Decision Trees We will continue to use the United States weather data introduced in Chapter 4. Given that this data was for the winter months in the United States, the classification task we will attempt to perform is to correct predict if it will snow on a particular day that precipitation occurs. To get a sense of how often it rains vs snows in these data, we can use the count() function to do this. For the count() function, the first argument is the name of the data, followed by the attributes we wish to count the number of observations in the unique values of those attributes. count(us_weather, rain, snow) ## [90m# A tibble: 4 x 3[39m ## rain snow n ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m No No [4m1[24m571 ## [90m2[39m No Yes 728 ## [90m3[39m Yes No 821 ## [90m4[39m Yes Yes 280 The following table counts the number of times that it rains or snows in the data. You may notice that there are days in which it does not rain or snow as shown by the row with No for both the rain and snow columns. There are also days in which it both rains and snows as shown in the row with Yes in both the rain and snow columns. Not surprisingly, a majority of the days it does not rain or snow, occurring about 46% of the time (\\(1571 / (1571 + 728 + 821 + 280) = 46.2%\\)). Using similar logic, about 8% of the days in the data have both snow and rain. 5.1.1 Fitting a Classification Tree Let’s class_tree our first classification tree to predict whether it snowed on a particular day. For this, we will use the rpart() function from the rpart package. The first argument to the rpart() function is a formula where the outcome of interest is specified to the left of the ~ and the attributes that are predictive of the outcome are specified to the right of the ~ separated with + signs. The second argument specifies the method for which we want to run the analysis, in this case we want to classify days based on the values in the data, therefore we specify method = 'class'. The final argument is the data element, in this case us_weather. Before we fit the model, what attributes do you think would be predictive of whether it will rain or snow on a particular day during the winter months? Take a few minutes to brainstorm some ideas. In this example, a handful of attributes to explore, including the average, minimum, and maximum temperature for the day. These happen to be all continuous attributes, meaning that these attributes can take many data values. The model is not limited to those types of data attributes, but that is where we will start the classification journey. Notice that the fitted model is saved to the object, class_tree. This will allow for easier interaction with the model results later. Then after fitting the model, the model is visualized using the rpart.plot() function. The primary argument to the rpart.plot() function is the fitted model object from the rpart() function, here that would be class_tree. The additional arguments passed below adjust the appearance of the visualization. class_tree &lt;- rpart(snow_factor ~ drybulbtemp_min + drybulbtemp_max, method = &#39;class&#39;, data = us_weather) rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3) Figure 5.1: Classification tree predicting whether it will snow or rain The visualization shown in Figure 5.1 produces the decision rules for the classification tree. The decision rules start from the top of the tree and proceed down the branches to the leaf nodes at the bottom that highlight the predictions. By default, the rpart() algorithm assumes that each split should go in two directions. For example, the first split occurs with the maximum temperature is less than 42 degrees Fahrenheit or greater than or equal to 42 degrees Fahrenheit. If the maximum temperature for the day is greater than or equal to 42 degrees Fahrenheit, the first split in the decision tree follows the left-most branch and proceeds to the left-most leaf node. This results in the prediction for those days as being days in which it does not snow (i.e., a category prediction of “No”). The numbers below the “No” label indicate that the probability of it snowing on a day where the maximum temperature was greater than or equal to 42 degrees Fahrenheit is 0.09 or about 9%. Furthermore, this category represents about 53% of the total number of data cases inputted. Following the right-hand split of the first decision, which occurs for days when the maximum temperature is less than 42 degrees, we come to another split. This split is again for the maximum temperature, but now the split comes at 36 degrees Fahrenheit. In this case, if the temperature is greater than or equal to 36 degrees Fahrenheit, the decision leads to the next leaf node and a prediction that it will not snow that day. For this leaf node, there is more uncertainty in the prediction, where on average the probability of it snowing would be 0.42 or about 42%. This value is less than 50%, therefore the “No” category is chosen. This occurs for about 16% of the data. For days in which the maximum temperature is less than 36 degrees Fahrenheit, the decision tree moves to the right further and comes to another split. The third split in the decision tree is for the minimum daily temperature and occurs at 23 degrees Fahrenheit. For days where the minimum temperature is greater than 23 degrees Fahrenheit (but also had a maximum temperature less than 36 degree Fahrenheit), the right-most leaf node is predicted. For these data cases, about 8% of the total data, the prediction is that it will snow (i.e., “Yes” category) and the probability of it snowing in those conditions is about 71%. Finally, if the minimum temperature is less than 23 degrees Fahrenheit (but also had a maximum temperature less than 36 degree Fahrenheit), then one last split occurs on the maximum temperature at 29 degrees Fahrenheit. This leads to the last two leaf node in the middle of Figure 5.1. One prediction states it will snow, for maximum temperature less than 29 degrees and one predicting it will not snow, for those greater than or equal to 29 degrees. Both of these leaf nodes have more uncertainty in the predictions, being close to 50% probability. Note, that the average daily temperature was included in the model fitting procedure, but was not included in the results shown in Figure 5.1. Why do you think this happened? The model results show the attributes that were helpful in making the prediction of whether it snowed or not. For this task, the model found that the maximum and minimum temperature attributes were more useful and adding the average daily temperature did not appreciably improve the predictions. For this reason, it did not show up in the decision tree. Furthermore, the attributes that are most informative in making the prediction are at the top of the decision tree. In the results shown in Figure 5.1, the maximum daily temperature was the most helpful attribute in making the snow or not prediction. The decision tree rules can also be requested in text form using the rpart.rules() function and are shown below. The rows in the output are the leaf nodes from 5.1 and the columns represent the probability of it snowing, the decision rules that are applicable, and the percentage of data found in each row. For example, for the first row, it is predicted to snow about 9% of the time when the maximum temperature for the day is greater than 42 and this occurred in 53% of the original data. Since the probability is less than 50%, the prediction would be that it would not snow on days with those characteristics. In rows where there are &amp; symbols, these separate different data attributes that are useful in the classification model. rpart.rules(class_tree, cover = TRUE) ## snow_factor cover ## 0.09 when drybulbtemp_max &gt;= 42 53% ## 0.42 when drybulbtemp_max is 36 to 42 16% ## 0.45 when drybulbtemp_max is 29 to 36 &amp; drybulbtemp_min &lt; 23 9% ## 0.63 when drybulbtemp_max &lt; 29 &amp; drybulbtemp_min &lt; 23 14% ## 0.71 when drybulbtemp_max &lt; 36 &amp; drybulbtemp_min &gt;= 23 8% 5.1.1.1 Visualizing Results To get another view of what the classification model is doing in this scenario, we will visualize the study results. First, the gf_point() function is used to create a scatterplot where the maximum temperature is shown on the x-axis and the minimum temperature is shown on the y-axis, shown in Figure 5.2. There is a positive relationship between maximum and minimum temperatures and on days with lower maximum temperatures are where it tends to snow. However, there is not perfect separation, meaning that there are days that have similar minimum and maximum temperatures where it does snow and other where it does not snow. temperature_scatter &lt;- gf_point(drybulbtemp_min ~ drybulbtemp_max, color = ~ snow_factor, alpha = .75, data = us_weather) %&gt;% gf_labs(x = &quot;Maximum Temperature (in F)&quot;, y = &quot;Minimum Temperature (in F)&quot;, color = &quot;Snow?&quot;) temperature_scatter Figure 5.2: Scatterplot of the minimum and maximum daily temperatures and if it snows or not The next figure will make use of the parttree R package to visualize what the classification model is doing. The geom_parttree() function is used where the primary argument is the saved classification model object that was save earlier, named class_tree. The other two arguments to add are the fill aesthetic that is the outcome of the classification tree and to control how transparent the backgroud fill color is. In this example, this is set using alpha = .25 where the transparency is set at 75% (i.e., 1 - 0.25 = 0.75). Setting a higher alpha value would reduce the amount of transparency, whereas setting a smaller value would increase the transparency. Figure 5.3 gives a sense as to what the classification model is doing to the data. The classification model breaks the data into quadrants and makes a single uniform prediction for those quadrants. For example, the areas of the figure that are shaded as red are days in which the model predicts it will not snow whereas the blue/green color are days in which the model predicts it will snow. The data points are the real data cases, therefore there are instances inside each of the quadrants in which the model did not correctly predict or classify the case. Each of the quadrants in the figure represent different leaf nodes shown in 5.1 and each represent a different likelihood of it snowing. temperature_scatter + geom_parttree(data = class_tree, aes(fill = snow_factor), alpha = .25) + scale_fill_discrete(&quot;Snow?&quot;) Figure 5.3: Showing the predictions based on the classification tree with the raw data 5.1.2 Accuracy Evaluating the model accuracy helps to understand how well the model performed the classification. If you recall, the classification model is making a prediction about whether it is going to snow on a given day based on the observed data where it was recorded if it snowed that day or not. Therefore, the data has for each day if it snowed or not. With this information, how could we evaluate how well the model performed in classifying whether it snows on a given day? To do this, the observation of whether it snowed or not can be compared to the model prediction of whether it snowed or not. Better classification accuracy would occur when the observed snow or no snow attribute is the same as the model prediction of snow or not. That is, when the same category is predicted as what is observed, this would result in better classification accuracy, a good thing. If there are cases where different categories between the observed and predicted categories or classes, this would be an example of poor classification accuracy. In the data so far, there is the observed data value on whether it snowed or not, this is the attribute that was used to fit the classification model, named snow_factor. To add the predicted classes based on the classification model shown in Figure 5.1, the predict() function can be used. To use the predict() function, the primary argument is a model object, in this case the classification model object named class_tree. To get the predicted classes, that is the leaf nodes at the bottom of Figure 5.1, a second argument is needed, type = 'class' which tells the predict function to report the top line of the leaf nodes in Figure 5.1. These predicted classes are saved into a new attribute named snow_predict. Another element is also added that represent the probability of a particular day not snowing or snowing, these are reported in the columns No and Yes in the resulting output. us_weather_predict &lt;- us_weather %&gt;% mutate(snow_predict = predict(class_tree, type = &#39;class&#39;)) %&gt;% cbind(predict(class_tree, type = &#39;prob&#39;)) head(us_weather_predict, n = 20) ## station date dewpoint_avg drybulbtemp_avg ## 1 72528014733 2018-10-01 23:59:00 51 52 ## 2 72528014733 2018-10-02 23:59:00 59 60 ## 3 72528014733 2018-10-03 23:59:00 55 62 ## 4 72528014733 2018-10-04 23:59:00 56 60 ## 5 72528014733 2018-10-05 23:59:00 43 51 ## 6 72528014733 2018-10-06 23:59:00 62 63 ## 7 72528014733 2018-10-07 23:59:00 58 60 ## 8 72528014733 2018-10-08 23:59:00 61 68 ## 9 72528014733 2018-10-09 23:59:00 66 77 ## 10 72528014733 2018-10-10 23:59:00 64 74 ## 11 72528014733 2018-10-11 23:59:00 56 62 ## 12 72528014733 2018-10-12 23:59:00 36 47 ## 13 72528014733 2018-10-13 23:59:00 36 46 ## 14 72528014733 2018-10-14 23:59:00 39 51 ## 15 72528014733 2018-10-15 23:59:00 43 49 ## 16 72528014733 2018-10-16 23:59:00 32 45 ## 17 72528014733 2018-10-17 23:59:00 34 45 ## 18 72528014733 2018-10-18 23:59:00 30 40 ## 19 72528014733 2018-10-19 23:59:00 38 50 ## 20 72528014733 2018-10-20 23:59:00 42 48 ## relativehumidity_avg sealevelpressure_avg stationpressure_avg ## 1 95 30.26 29.50 ## 2 96 30.01 29.26 ## 3 86 30.05 29.31 ## 4 77 29.97 29.18 ## 5 75 30.17 29.41 ## 6 90 30.03 29.28 ## 7 97 30.24 29.44 ## 8 84 30.23 29.49 ## 9 72 30.13 29.39 ## 10 70 29.89 29.18 ## 11 77 29.66 28.91 ## 12 66 29.82 29.05 ## 13 74 29.95 29.15 ## 14 69 30.12 29.34 ## 15 79 29.94 29.16 ## 16 61 30.06 29.31 ## 17 66 30.02 29.21 ## 18 68 30.37 29.59 ## 19 63 30.00 29.28 ## 20 86 29.68 28.90 ## wetbulbtemp_avg windspeed_avg cooling_degree_days ## 1 51 10.9 0 ## 2 60 8.5 0 ## 3 57 5.5 0 ## 4 59 12.5 0 ## 5 47 9.6 0 ## 6 63 8.1 0 ## 7 58 9.4 0 ## 8 63 7.9 3 ## 9 69 11.4 12 ## 10 68 10.6 9 ## 11 59 15.7 0 ## 12 42 12.5 0 ## 13 41 8.4 0 ## 14 45 6.5 0 ## 15 47 12.8 0 ## 16 39 15.8 0 ## 17 40 15.3 0 ## 18 36 11.2 0 ## 19 45 18.0 0 ## 20 45 12.3 0 ## departure_from_normal_temperature heating_degree_days drybulbtemp_max ## 1 -4.6 13 54 ## 2 3.8 5 69 ## 3 6.2 3 70 ## 4 4.6 5 74 ## 5 -4.0 14 58 ## 6 8.4 2 74 ## 7 5.7 5 67 ## 8 14.1 0 82 ## 9 23.5 0 83 ## 10 20.9 0 81 ## 11 9.2 3 74 ## 12 -5.4 18 51 ## 13 -6.1 19 51 ## 14 -0.7 14 60 ## 15 -2.4 16 58 ## 16 -6.0 20 52 ## 17 -5.7 20 53 ## 18 -10.4 25 47 ## 19 0.0 15 57 ## 20 -1.7 17 55 ## drybulbtemp_min peak_wind_direction peak_wind_speed precipitation snow_depth ## 1 50 50 24 0.090 0 ## 2 51 320 24 1.000 0 ## 3 53 210 25 0.005 0 ## 4 46 220 39 0.450 0 ## 5 44 100 21 0.000 0 ## 6 51 250 26 0.730 0 ## 7 53 50 21 0.020 0 ## 8 53 70 20 0.010 0 ## 9 70 210 30 0.000 0 ## 10 67 190 25 0.005 0 ## 11 50 220 39 0.010 0 ## 12 42 280 27 0.010 0 ## 13 40 250 24 0.140 0 ## 14 41 250 17 0.000 0 ## 15 40 220 37 0.090 0 ## 16 38 210 40 0.005 0 ## 17 36 290 36 0.050 0 ## 18 33 250 28 0.030 0 ## 19 43 210 48 0.005 0 ## 20 40 220 49 0.470 0 ## snowfall wind_direction wind_speed weather_occurances sunrise sunset month ## 1 0.000 60 20 RA DZ BR 612 1757 Oct ## 2 0.000 320 21 RA DZ BR 613 1755 Oct ## 3 0.000 200 20 DZ BR 614 1753 Oct ## 4 0.000 220 32 TS RA BR 615 1751 Oct ## 5 0.000 70 16 &lt;NA&gt; 616 1750 Oct ## 6 0.000 200 20 TS RA BR 618 1748 Oct ## 7 0.000 60 16 RA DZ BR 619 1746 Oct ## 8 0.000 70 16 RA 620 1744 Oct ## 9 0.000 210 23 &lt;NA&gt; 621 1743 Oct ## 10 0.000 190 21 &lt;NA&gt; 622 1741 Oct ## 11 0.000 240 29 RA 623 1739 Oct ## 12 0.000 260 21 RA 625 1738 Oct ## 13 0.000 240 18 RA BR 626 1736 Oct ## 14 0.000 250 14 &lt;NA&gt; 627 1734 Oct ## 15 0.000 290 28 RA BR 628 1733 Oct ## 16 0.000 220 30 RA 629 1731 Oct ## 17 0.005 290 28 GR RA SN 631 1729 Oct ## 18 0.005 240 21 RA SN HZ 632 1728 Oct ## 19 0.000 240 35 RA 633 1726 Oct ## 20 0.100 240 36 TS GR RA BR PL 634 1725 Oct ## month_numeric year day winter_group location fog mist drizzle rain snow ## 1 10 2018 1 18_19 Buffalo, NY No Yes Yes Yes No ## 2 10 2018 2 18_19 Buffalo, NY No Yes Yes Yes No ## 3 10 2018 3 18_19 Buffalo, NY No Yes Yes No No ## 4 10 2018 4 18_19 Buffalo, NY No Yes No Yes No ## 5 10 2018 5 18_19 Buffalo, NY No No No No No ## 6 10 2018 6 18_19 Buffalo, NY No Yes No Yes No ## 7 10 2018 7 18_19 Buffalo, NY No Yes Yes Yes No ## 8 10 2018 8 18_19 Buffalo, NY No No No Yes No ## 9 10 2018 9 18_19 Buffalo, NY No No No No No ## 10 10 2018 10 18_19 Buffalo, NY No No No No No ## 11 10 2018 11 18_19 Buffalo, NY No No No Yes No ## 12 10 2018 12 18_19 Buffalo, NY No No No Yes No ## 13 10 2018 13 18_19 Buffalo, NY No Yes No Yes No ## 14 10 2018 14 18_19 Buffalo, NY No No No No No ## 15 10 2018 15 18_19 Buffalo, NY No Yes No Yes No ## 16 10 2018 16 18_19 Buffalo, NY No No No Yes No ## 17 10 2018 17 18_19 Buffalo, NY No No No Yes Yes ## 18 10 2018 18 18_19 Buffalo, NY No No No Yes Yes ## 19 10 2018 19 18_19 Buffalo, NY No No No Yes No ## 20 10 2018 20 18_19 Buffalo, NY No Yes No Yes No ## snow_factor snow_predict No Yes ## 1 No No 0.9130676 0.08693245 ## 2 No No 0.9130676 0.08693245 ## 3 No No 0.9130676 0.08693245 ## 4 No No 0.9130676 0.08693245 ## 5 No No 0.9130676 0.08693245 ## 6 No No 0.9130676 0.08693245 ## 7 No No 0.9130676 0.08693245 ## 8 No No 0.9130676 0.08693245 ## 9 No No 0.9130676 0.08693245 ## 10 No No 0.9130676 0.08693245 ## 11 No No 0.9130676 0.08693245 ## 12 No No 0.9130676 0.08693245 ## 13 No No 0.9130676 0.08693245 ## 14 No No 0.9130676 0.08693245 ## 15 No No 0.9130676 0.08693245 ## 16 No No 0.9130676 0.08693245 ## 17 Yes No 0.9130676 0.08693245 ## 18 Yes No 0.9130676 0.08693245 ## 19 No No 0.9130676 0.08693245 ## 20 No No 0.9130676 0.08693245 The first 20 rows of the resulting data are shown. Notice that for all of these 20 rows, the predicted class, shown in the attribute, snow_predict, are represented as “No” indicating that these days it was not predicted to snow. Notice toward the bottom however, that there were two days in which it did in fact snow, shown in the column named, snow_factor. These would represent two cases of misclassification as the observed data is not the same as the model predicted class. Finally, the probabilities shown in the last two attribute columns are all the same here. These are all the same as the maximum dry bulb temperature was greater than 42 degrees Fahrenheit in all of these days. Therefore, all 20 of the cases shown in the data here represent the left-most leaf node shown in Figure 5.1. Now that the observed data and the model predicted classes are in the data, it is possible to produce a table that shows how many observations were correctly predicted (indicating better model accuracy). To do this, the count() function can be used where the observed and predicted class attributes are passed as arguments. us_weather_predict %&gt;% count(snow_factor, snow_predict) ## snow_factor snow_predict n ## 1 No No 2147 ## 2 No Yes 245 ## 3 Yes No 529 ## 4 Yes Yes 479 The resulting table shows the observed data values in the left-most column (snow_factor) followed by the predicted class (snow_predict) in the middle column. The final column represents the number of rows or observations that were in each combination of the first two columns. For example, the first row shows that 2,147 observations were counted that had the combination where it was observed and predicted to have not snowed that day. These 2,147 observations would be instances of correct classification. The second row shows that 245 observations occurred where it was observed to not have snowed, but the model predicted it would snow that day. All of the 245 observations were misclassified based on the classification model. From this table, the overall model accuracy can be calculated by summing up the cases that matched and dividing by the total number of observations. This computation would look like: \\[ accuracy = \\frac{\\textrm{matching predictions}}{\\textrm{total observations}} = \\frac{(2147 + 479)}{(2147 + 245 + 529 + 479)} = .772 = 77.2% \\] This means that the overall classification accuracy for this example was just over 77%, meaning that about 77% of days the model was able to correctly classify whether it snowed or not. This computation can also be done programmatically. To do this, a new attribute named, same_class, can be added to the data that is given a value of 1 if the observed data matches the predicted class and a value of 0 otherwise. Descriptive statistics, such as the mean and sum, can be computed on this new vector to represent the accuracy as a proportion and the number of matching predictions (the numerator shown in the equation above). us_weather_predict %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0)) %&gt;% df_stats(~ same_class, mean, sum) ## response mean sum ## 1 same_class 0.7723529 2626 Notice that the same model accuracy was found, about 77.2%, and the number of observations (i.e., days) that the correct classification was found was 2,626 days. Is correctly predicting 77.2% of the days good? That is, would you say this model is doing a good job at accurately predicting if it will snow or not on that day? 5.1.2.1 Conditional Accuracy One potential misleading element of simply computing the overall model accuracy as done above, is that the accuracy will likely differ based on the which class. This could occur for a few reasons, one it could be more difficult to predict one of the classes due to similarities in data across the two classes. The two classes are also often unbalanced, therefore exploring the overall model accuracy will give more weight to the group/class that has more data. In addition, this group has more data so it could make it a bit easier for the model to predict, these issues could be exacerbated in small sample conditions. Therefore, similar to earlier discussion in the book about multivariate distributions, it is often important to consider conditional or multivariate accuracy instead of the overall model accuracy. Let’s explore this a different way than simply computing a percentage, instead we could use a bar graph to explore the model accuracy. Figure 5.4 shows the number of correct classifications for the two observed data classes (i.e., snow or did not snow) on the x-axis by the predicted classes shown with the fill color in the bars. The fill color are red for days that the model predicts it will not show and green/blue for days in which it will not snow. Therefore, accuracy would be represented in the left-bar by the red portion of the bar and the right-bar by the green/blue portion of the bar. gf_bar(~ snow_factor, fill = ~snow_predict, data = us_weather_predict) %&gt;% gf_labs(x = &quot;Observed Snow Status&quot;, fill = &quot;Predicted Snow Status&quot;) Figure 5.4: A bar graph showing the conditional prediction accuracy represented as counts. Figure 5.4 is not a very good picture to depict accuracy as the two groups have different numbers of observations so comparisons between the bars is difficult. Secondly, the count metric makes it difficult to estimate how many are in each group, for example, it is difficult from the figure alone to know how many were incorrectly classified in the left-most bar represented by the blue/green color. These issues can be fixed by adding an additional argument, position = 'fill' which will scale each bar as a proportion, ranging from 0 to 1. The bar graph is now scaling each bar based on the sample size to normalize sample size differences. gf_bar(~ snow_factor, fill = ~snow_predict, data = us_weather_predict, position = &quot;fill&quot;) %&gt;% gf_labs(x = &quot;Observed Snow Status&quot;, fill = &quot;Predicted Snow Status&quot;, y = &#39;Proportion&#39;) %&gt;% gf_refine(scale_y_continuous(breaks = seq(0, 1, .1))) Figure 5.5: A bar graph showing the conditional prediction accuracy represented as a proportion. From this new figure (Figure 5.5), it is much easier to estimate the prediction accuracy from the figure. For example, the green/blue portion of the left-most bar is at about 0.10, meaning that about 10% of the cases are misclassified and 90% would be correctly classified. Therefore the classification accuracy for days in which it did not snow would be about 90%. Compare this to days in which it did not snow (the right bar), where the prediction accuracy represented by the green/blue color is about 48%, meaning that the misclassification rate is about 52%. Let’s recalibrate how we think the model is doing? If you were just given the overall classification rate of about 77%, how did you feel about the model? Now that we know the model accurate predicts it won’t snow about 90% of the time, but can only identify that it will snow about 48% of the time, how well do you feel the model is performing now? Would you feel comfortable using this model in the real world? One last note, we can also compute the conditional model accuracy more directly using the df_stats() function as was done for the overall model accuracy. The primary difference in the code is to specify the same_class attribute to the left of the ~. This represent the attribute to compute the statistics of interest with. Another attribute is added to the right of the ~ to represent the attribute to condition on, in this case the observed data point of whether it snowed or not. us_weather_predict %&gt;% mutate(same_class = ifelse(snow_factor == snow_predict, 1, 0)) %&gt;% df_stats(same_class ~ snow_factor, mean, sum, length) ## response snow_factor mean sum length ## 1 same_class No 0.8975753 2147 2392 ## 2 same_class Yes 0.4751984 479 1008 The output returns the conditional model accuracy as a proportion, the number of correct classifications for each class/group, and the total number of observations (both correct and incorrect classifications) for each class/group. The estimated values we had from the figure were very close to the actual calculated values, but we find the figure to be more engaging than just the statistics. "],
["linear-model.html", "Chapter 6 Linear Model 6.1 Regression Trees 6.2 Simple Regression continuous predictor 6.3 Conditional Means", " Chapter 6 Linear Model 6.1 Regression Trees In this example, data on major league baseball players that comes with an R package, ISLR. These data contain information about hitters in major league baseball for the 1986 season and also contain information about their starting salary for the 1987 season. Missing data related to salary information was dropped from the data. ### Loading R packages hen the packages can be loaded and some processing is done on the Hitters data to drop any missing data elements from the salary data attribute. Finally, the first few rows of the data are shown with the head() function. library(tidyverse) library(ggformula) library(mosaic) library(ISLR) library(rpart) library(rsample) library(rpart.plot) library(statthink) # Set theme for plots theme_set(theme_statthinking()) Hitters &lt;- Hitters %&gt;% drop_na(Salary) head(Hitters) ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun ## -Alan Ashby 315 81 7 24 38 39 14 3449 835 69 ## -Alvin Davis 479 130 18 66 72 76 3 1624 457 63 ## -Andre Dawson 496 141 20 65 78 37 11 5628 1575 225 ## -Andres Galarraga 321 87 10 39 42 30 2 396 101 12 ## -Alfredo Griffin 594 169 4 74 51 35 11 4408 1133 19 ## -Al Newman 185 37 1 23 8 21 2 214 42 1 ## CRuns CRBI CWalks League Division PutOuts Assists Errors ## -Alan Ashby 321 414 375 N W 632 43 10 ## -Alvin Davis 224 266 263 A W 880 82 14 ## -Andre Dawson 828 838 354 N E 200 11 3 ## -Andres Galarraga 48 46 33 N E 805 40 4 ## -Alfredo Griffin 501 336 194 A W 282 421 25 ## -Al Newman 30 9 24 N E 76 127 7 ## Salary NewLeague ## -Alan Ashby 475.0 N ## -Alvin Davis 480.0 A ## -Andre Dawson 500.0 N ## -Andres Galarraga 91.5 N ## -Alfredo Griffin 750.0 A ## -Al Newman 70.0 A 6.1.1 Visualize distributions Exploring the distribution of the variable of interest is often the first step in an analysis. Here, we are interested in exploring the distribution of salaries of major league baseball players and seeing if data attributes can help to predict the salary for the player. The first plot explored is the density of the salary variable. gf_density(~ Salary, data = Hitters) What are some features of the distribution above? Could there be some concerns about this distribution if we are looking to do some analysis on this? In general, symmetric distributions are preferred over skewed distributions and some models make an assumption of normality, a special type of symmetric distribution. One way to help make a skewed distribution more symmetric is to transform the data. For a positively skewed distribution, such as income, rent, salary, etc, a log transformation is a common transformation that is used by econometricians and is a meaningful transformation. The largest downside of the transformation is that the original metric is lost and the analysis is done on the transformed metric. The log transformation identified above is often referred to as a non-linear transformation such that it alters values differently based on where these are on the scale. For example, the log transformation will minimize gaps that are higher up on the scale and spread out gaps in small values, this is why this type of transformation is common for right or positively skewed data. Below is a figure applying the log transformation to the salary attribute. gf_density(~ log(Salary), data = Hitters) ### Explore relationships between two quantitative attributes So far, the course has focused on exploring relationships between a quantitative and various qualitative (i.e. categorical or grouping) attributes. It is also common to want to explore relationships between two quantitative attributes. One way to visualize this type of relationship is with a scatterplot and this can be done with the gf_point() function. Similar to other multivariate figures, this function takes a formula as the input where the attribute of interest (log of salary here) is placed on the left hand side of the equation and the second attribute is placed on the right hand side of the equation. Below, the equation log(Salary) ~ HmRun means that the log salary is the attribute of interest (placed on the y-axis) is going to be plotted in conjunction with the number of home runs the player hit (placed on the x-axis). Let’s look at the figure. gf_point(log(Salary) ~ HmRun, data = Hitters) Another measure of association between two attributes is the correlation. This statistic gives a single number summary about the linear relationship between two quantitative attributes. The correlation ranges between -1 and +1 where 0 means no relationship. The closer the correlation gets to -1 or +1 indicates a stronger linear relationship between the two attributes. A correlation of -1 means the two attributes are inversely related, more specifically this means that as one goes up the other will tend to decrease. The opposite is true for a correlation of +1 indicating a positive relationship, as one attribute increases the other tends to increase as well. cor(log(Salary) ~ HmRun, data = Hitters) ## [1] 0.3398543 6.1.2 Decision Tree - Regression Tree Another way to explore the relationship between two quantitative attributes is through the fitting a regression tree. A regression tree is similar to a classification tree, however now the output is a numeric or continuous type variable that takes on many different values. In the classification tree example, the focus in this class was predicting if a case belonged to one of two classes. In this case, the regression tree will predict the numeric variable with many potential values rather than just two. The syntax for fitting a regression tree is very similar in R compared to the classification tree. The same function, rpart() is used and the function rpart.plot() will be used to visualize the fitted regression tree similar to before. The primary argument to the rpart() function is a formula where the left-hand side is the attribute of interest and the right hand side contains attributes that help predict the outcome. In the example below, the log of salary is the outcome and the number of home runs hit during the previous season is used as the sole continuous attribute used to predict the log of the salary. The data argument is also specified and the only difference here between a classification tree and the regression tree here is the method argument. In the regression tree the method argument should be set to method = 'anova'. This tells the rpart() function that the outcome is numeric and that an anova method should be used in the model fitting. The anova stands for Analysis of Variance and we will discuss this in more detail moving forward. hit_reg &lt;- rpart(log(Salary) ~ HmRun, data = Hitters, method = &quot;anova&quot;) rpart.plot(hit_reg, roundint = FALSE, type = 3, branch = .3) The output from the regression tree is similar to that from a classification tree. One major difference however is that the predicted values in the end are numeric quantities instead of classes and the probabilities that were shown previously are not shown here as there is not a probability of being in a class. The percentage of cases in the predicted nodes at the end of the tree are still shown. The logic for following the tree is the same as before where each split can have two new paths to follow and then the variable(s) are re-evaluated after the first split to see if additional splits can help predict the outcome of interest. Below is a figure that builds on the scatterplot we saw above. Vertical lines are shown that indicate the two splits that were established from the above regression tree. These splits are where the end buckets lie and all of the data points residing in a single area have the same predicted log salary. gf_point(log(Salary) ~ HmRun, data = Hitters, color = &#39;gray55&#39;) %&gt;% gf_vline(xintercept = c(8.5, 19), size = 1) %&gt;% gf_text(x = 2, y = 4.2, label = &quot;5.6&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_text(x = 12, y = 4.2, label = &quot;6.1&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_text(x = 32, y = 4.2, label = &quot;6.4&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_labs(y = &quot;Log of player salary&quot;, x = &quot;Number of Home Runs&quot;, title = &quot;Log salary by number of home runs&quot;) ## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided. #### Explore another attribute Let’s explore another attribute, the number of hits in the previous season and how this may be related to the log of the salary. First a scatterplot is shown then the correlation is computed. gf_point(log(Salary) ~ Hits, data = Hitters) cor(log(Salary) ~ Hits, data = Hitters) ## [1] 0.4495841 Updating the regression tree with another variable is similar to a classification tree. More than one attribute used to help predict the outcome are separated by + signs. In addition, I specified the model to terminate when the complexity parameter (CP) gets smaller than .012. hit_reg &lt;- rpart(log(Salary) ~ HmRun + Hits, data = Hitters, method = &quot;anova&quot;, cp = .012) rpart.plot(hit_reg, roundint = FALSE, type = 3, branch = .3) The figure below attempts to show the regression tree in a scatterplot. Now there are more predicted buckets and these are represented by the square areas of the figure below. All of the data points within each square would receive the same predicted score. gf_point(HmRun ~ Hits, data = Hitters, color = ~ log(Salary)) %&gt;% gf_vline(xintercept = c(118, 146), size = 1) %&gt;% gf_segment(8.5 + 8.5 ~ 0 + 118, size = 0.75, color = &quot;black&quot;) %&gt;% gf_segment(8.5 + 8.5 ~ 146 + 238, size = 0.75, color = &quot;black&quot;) %&gt;% gf_text(x = 1, y = 3, label = &quot;5.4&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_text(x = 128, y = 3, label = &quot;6.3&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_text(x = 170, y = 3, label = &quot;6.1&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_text(x = 50, y = 35, label = &quot;5.9&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_text(x = 200, y = 35, label = &quot;6.7&quot;, color = &quot;red&quot;, size = 5) %&gt;% gf_labs(x = &quot;Number of Hits&quot;, y = &quot;Number of Home Runs&quot;, title = &quot;Log salary by number of home runs and hits&quot;) ## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided. One thing that can help with interpretation when the data are transformed, in this case by taking the log of the salary variable, is to back-transform to the original salary metric. In this case, the inverse of the log is the exponential function. This can be achieved in R with the exp() function. The predicted values from the regression tree are back-transformed to show the salaries in their original metric, in thousands of dollars. exp(c(5.4, 5.9, 6.3, 6.1, 6.7)) ## [1] 221.4064 365.0375 544.5719 445.8578 812.4058 6.1.3 Evaluating accuracy In the classification tree example, a natural metric to evaluate how well the model was doing was the classification accuracy. This was most useful being computed individually for each class that was predicted instead of solely overall. In the regression tree example, we do not have class membership, instead we have the original observed salary and the predicted salary. One measure that could be used for accuracy is on average how far do the predicted scores deviate from the observed scores. The below code chunk computes those variables for use, one on the log scale and another on the original back-transformed scale. Hitters &lt;- Hitters %&gt;% mutate(log_salary_pred = predict(hit_reg), log_salary = log(Salary), log_error = log_salary - log_salary_pred, salary_pred = exp(log_salary_pred), error = Salary - salary_pred) head(Hitters) ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks ## 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 ## 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 ## 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 ## 4 321 87 10 39 42 30 2 396 101 12 48 46 33 ## 5 594 169 4 74 51 35 11 4408 1133 19 501 336 194 ## 6 185 37 1 23 8 21 2 214 42 1 30 9 24 ## League Division PutOuts Assists Errors Salary NewLeague log_salary_pred ## 1 N W 632 43 10 475.0 N 5.375205 ## 2 A W 880 82 14 480.0 A 6.260120 ## 3 N E 200 11 3 500.0 N 6.260120 ## 4 N E 805 40 4 91.5 N 5.872782 ## 5 A W 282 421 25 750.0 A 6.112038 ## 6 N E 76 127 7 70.0 A 5.375205 ## log_salary log_error salary_pred error ## 1 6.163315 0.78810957 215.9842 259.01581 ## 2 6.173786 -0.08633342 523.2815 -43.28148 ## 3 6.214608 -0.04551143 523.2815 -23.28148 ## 4 4.516339 -1.35644260 355.2357 -263.73572 ## 5 6.620073 0.50803537 451.2574 298.74263 ## 6 4.248495 -1.12670999 215.9842 -145.98419 hen, the df_stats() function is used to compute summary statistics for the log_error attribute which represented the difference between the observed and predicted log salaries. After this, the same statistics are computed for the error after back-transforming the data. Both of these are not quite what we want here, any idea why? Hitters %&gt;% df_stats(~ log_error, mean, median, sd, min, max) ## response mean median sd min max ## 1 log_error 1.09754e-15 0.118683 0.7493933 -1.612228 2.287419 Hitters %&gt;% df_stats(~ error, mean, median, sd, min, max) ## response mean median sd min max ## 1 error 115.4757 34.01581 386.1183 -609.801 1911.349 Instead of computing the average deviation, we first want to take the absolute value of the difference between the observed and predicted scores then compute the summary statistics. This now represents the mean absolute error that was computed earlier when discussing variation and the interpretation of the mean statistic below would be the average distance the predicted scores are from the observed scores, on the log salary scale. In general, lower average distances means the model did a better job of predicting the numeric quantity. However, this value is scale dependent, therefore if the scales of two outcomes are different, the mean absolute error is not directly comparable without some prior standardization. Hitters %&gt;% df_stats(~ abs(log_error), mean, median, sd, min, max) ## response mean median sd min max ## 1 abs(log_error) 0.6128455 0.527838 0.4296245 0.0002568557 2.287419 6.2 Simple Regression continuous predictor 6.2.1 Description of the Data These data contain information on mother’s and baby’s health for 1,174 pregnant women. baby &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/baby.csv&quot;) ## Parsed with column specification: ## cols( ## birth_weight = [32mcol_double()[39m, ## gestational_days = [32mcol_double()[39m, ## maternal_age = [32mcol_double()[39m, ## maternal_height = [32mcol_double()[39m, ## maternal_pregnancy_weight = [32mcol_double()[39m, ## maternal_smoker = [33mcol_logical()[39m ## ) head(baby) ## [90m# A tibble: 6 x 6[39m ## birth_weight gestational_days maternal_age maternal_height maternal_pregna… ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m 120 284 27 62 100 ## [90m2[39m 113 282 33 64 135 ## [90m3[39m 128 279 28 64 115 ## [90m4[39m 108 282 23 67 125 ## [90m5[39m 136 286 25 62 93 ## [90m6[39m 138 244 33 62 178 ## [90m# … with 1 more variable: maternal_smoker [3m[90m&lt;lgl&gt;[90m[23m[39m 6.2.2 Scatterplots As we’ve explored before, scatterplots help to explore the relationship between two continuous, quantitative data attributes. These are created with the gf_point() function and adding lines to the figure to provide some guidance to the relationship can be done with the gf_smooth() function. Below, a scatterplot is created that explores the relationship between birth weight and gestational days. gf_point(birth_weight ~ gestational_days, data = baby, size = 3, alpha = .2) %&gt;% gf_smooth(method = &#39;lm&#39;, linetype = 2, size = 1) %&gt;% gf_smooth(size = 1) ## `geom_smooth()` using method = &#39;gam&#39; The figure shows two types of lines, the dashed line is assuming a linear relationship (specified with gf_smooth(method = 'lm')) and the solid line is allowing the relationship to be more flexible to account for any non-linearity. There does appear to be some evidence of non-linearity, particularly in the tails of gestational days distribution. We can attempt to summarize this relationship in a single numeric value by computing the correlation coefficient. The correlation was initially explored when fitting regression trees. The correlation can be calculated with the cor() function with the primary argument being a formula depicting the two variables to compute the correlation on. cor(birth_weight ~ gestational_days, data = baby) ## [1] 0.4075428 Here the correlation represents the degree of linear relationship between the two variables. Values closer to 1 in absolute value (i.e. +1 or -1) show a stronger linear relationship and values closer to 0 indicate no relationship or weaker relationship. The correlation between the two variables above was about 0.41 indicating that there is a moderate positive linear relationship between birth weight and gestational days. The correlation is shown to be positive due to the coefficient being positive and the general trend from the scatterplot shows a direction of relationship moving from the lower left of the figure to the upper right of the figure. A negative correlation would have a negative sign associated with it and would trend from the upper left to the lower right of a scatterplot. 6.2.3 Fitting a linear regression model Now that the correlation was computed, we have evidence that there is a relationship between the baby birth weight and the gestational days. To provide some more evidence about the strength of this relationship and how much error is involved, fitting a linear regression model is often done. This can be done with the lm() function where the two arguments that need to be specified are a formula and the data to use for the model fitting. The formula takes the following form: birth_weight ~ gestational_days, where birth weight is the outcome of interest (in language we’ve used previously, this is the attribute we want to predict) and gestational days is the attribute we want to use to do the predicting of birth weight. Another way to think about what these variables represent is to explain variation in the birth weight with gestational days. In other words, the assumption is made that gestational days impacts or explains differences in the baby birth weight. baby_reg &lt;- lm(birth_weight ~ gestational_days, data = baby) coef(baby_reg) ## (Intercept) gestational_days ## -10.7541389 0.4665569 he following coefficients represent the linear regression equation that more generally can be show as: \\[\\begin{equation} birth\\_weight = -10.8 + 0.47 gestational\\_days + \\epsilon \\end{equation}\\] The equation can also be represented without the error, \\(\\epsilon\\) as: begin{equation} = -10.8 + 0.47 gestational_days \\end{equation} where now the birth weight outcome has a hat (i.e. \\(\\hat{y}\\)) that denotes mathematically that the equation predicts a value of birth weight given solely the number of gestational days. The first equation above says that the original observed birth weight is a function of gestational days plus some error. Using the equation above, the predicted birth weight can be obtained by including a value inserted for gestational days. Let’s pick a few values for gestational days to try. -10.8 + 0.47 * 200 ## [1] 83.2 -10.8 + 0.47 * 275 ## [1] 118.45 -10.8 + 0.47 * 276 ## [1] 118.92 You may notice that the predicted value of birth weight increases by 0.47 grams for every one day increase in gestational days, often referred to as the linear slope. The predicted values would fit on the dashed line shown in the scatterplot shown above. This highlights the assumption made here from the linear regression model above in which the relationship between birth weight and gestational days is assumed to be linear. It is possible to relax this assumption with a more complicated model, however this is the assumption being made currently. 6.2.4 Explore the y-intercept So far the discussion has focused on the linear slope, often a term that is of most interest. However, the y-intercept can also be made to be more interesting by adjusting the range of gestational days. 6.2.4.1 Mean center gestational days First, mean centering the x attribute can often be a way to make the y-intercept more interpretable. The code below shows a scatterplot by subtracting the mean from all the values of gestational days. gf_point(birth_weight ~ I(gestational_days - mean(gestational_days)), data = baby, size = 3, alpha = .2) %&gt;% gf_smooth(method = &#39;lm&#39;, linetype = 2, size = 1) %&gt;% gf_smooth(size = 1) ## `geom_smooth()` using method = &#39;gam&#39; Notice that the relationship is the same as before, but now the scale of gestational days is different. It may be more difficult to interpret now as the number of days a women is pregnant is relatively well known, but now the mean gestational days is represented as 0 in the figure and all the values are in reference to that instead of referencing when a women became pregnant. Using this same approach, a linear regression can be fitted to this newly recentered gestational days variable. baby_reg_centered &lt;- lm(birth_weight ~ I(gestational_days - mean(gestational_days)), data = baby) coef(baby_reg_centered) ## (Intercept) ## 119.4625213 ## I(gestational_days - mean(gestational_days)) ## 0.4665569 he new equation would look like: begin{equation} = 119.5 + 0.47 (gestational_days - mean(gestational_days)) \\end{equation} 119.5 + 0.47 * -3 ## [1] 118.09 119.5 + 0.47 * 0 ## [1] 119.5 6.2.4.2 Minimum or Maximum centered gestational days A few other options that are common are to subtract the minimum or maximum values from the x attribute. baby_reg_min &lt;- lm(birth_weight ~ I(gestational_days - min(gestational_days)), data = baby) coef(baby_reg_min) ## (Intercept) ## 58.2962789 ## I(gestational_days - min(gestational_days)) ## 0.4665569 baby_reg_max &lt;- lm(birth_weight ~ I(gestational_days - max(gestational_days)), data = baby) coef(baby_reg_max) ## (Intercept) ## 153.9404386 ## I(gestational_days - max(gestational_days)) ## 0.4665569 6.3 Conditional Means "],
["estimation-bootstrap-uncertainty.html", "Chapter 7 Estimation / Bootstrap / Uncertainty 7.1 Estimating Error 7.2 Categorical Predictor(s) 7.3 More than 2 categorical groups 7.4 Multiple Regression", " Chapter 7 Estimation / Bootstrap / Uncertainty library(tidyverse) library(ggformula) library(mosaic) library(broom) library(statthink) # Set theme for plots theme_set(theme_statthinking()) baby &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/baby.csv&quot;) head(baby) ## [90m# A tibble: 6 x 6[39m ## birth_weight gestational_days maternal_age maternal_height maternal_pregna… ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m 120 284 27 62 100 ## [90m2[39m 113 282 33 64 135 ## [90m3[39m 128 279 28 64 115 ## [90m4[39m 108 282 23 67 125 ## [90m5[39m 136 286 25 62 93 ## [90m6[39m 138 244 33 62 178 ## [90m# … with 1 more variable: maternal_smoker [3m[90m&lt;lgl&gt;[90m[23m[39m Bootstrap and resampling methods can be used to estimate the variability in the estimated effects. 7.1 Estimating Error To get some sense of the amount of error in the estimate of the linear slope here, a bootstrap can be done to provide some evidence of the likely range of slope values. The bootstrap will take the following general steps: Resample the observed data available, with replacement Fit the same linear regression model as above. Save the slope coefficient representing the relationship between birth weight and gestational days Repeat steps 1 - 3 many times Explore the distribution of slope estimates from the many resampled data sets. When this was done with the classification tree, a function was used to do these steps once, then these were repeated many times. Below is a function that does the steps 1 - 3 above a single time. resample_baby &lt;- function(...) { baby_resample &lt;- baby %&gt;% sample_n(nrow(baby), replace = TRUE) baby_resample %&gt;% lm(birth_weight ~ gestational_days, data = .) %&gt;% coef(.) %&gt;% .[2] %&gt;% data.frame() } resample_baby() ## . ## gestational_days 0.4487489 Now that there is a function that does steps 1 - 3, these processes can now be repeated many times. baby_coef &lt;- map(1:10000, resample_baby) %&gt;% bind_rows() names(baby_coef) &lt;- &#39;slope&#39; gf_density(~ slope, data = baby_coef) baby_coef %&gt;% df_stats(~ slope, quantile(c(0.05, 0.5, 0.95))) ## response 5% 50% 95% ## 1 slope 0.3955932 0.4683647 0.5442306 7.2 Categorical Predictor(s) Before, linear regression has been ran with a continuous attribute. In both models, the baby’s birth weight was the outcome of interest and the predictor in one model was the number of gestational days and in the other was the age of the mother at time of birth. What happens when a categorical predictor is used instead of a continuous predictor? This section will introduce that idea with a categorical predictor that has two different levels. 7.2.1 Mother’s smoking It is known that a mother smoking while pregnant can hamper the development of the unborn fetus. Will this transition into lower birth weight for baby’s born to mothers who smoked during the pregnancy? First, let’s explore the distribution and calculate descriptive statistics for birth weight across the two groups. gf_density(~ birth_weight, color = ~ maternal_smoker, size = 1.25, fill = &#39;gray80&#39;, data = baby) %&gt;% gf_labs(x = &#39;Birth Weight (in oz)&#39;, color = &#39;Smoked?&#39;) What are the general take-aways from the distributions above? To give some additional information, a violin plot may be helpful. gf_violin(birth_weight ~ maternal_smoker, data = baby, draw_quantiles = c(0.1, 0.5, 0.9), fill = &#39;gray85&#39;, size = 1) %&gt;% gf_refine(coord_flip()) %&gt;% gf_labs(y = &quot;Birth Weight (in oz)&quot;, x = &quot;Smoker?&quot;) Any additional information shown here that shows differences? To finish the descriptive exploration, let’s compute some descriptive statistics. baby %&gt;% df_stats(birth_weight ~ maternal_smoker, mean, sd, median, quantile(c(0.25, 0.75)), length) ## response maternal_smoker mean sd median 25% 75% length ## 1 birth_weight FALSE 123.0853 17.42370 123 113 134 715 ## 2 birth_weight TRUE 113.8192 18.29501 115 101 126 459 7.2.2 Linear Regression - Categorical Predictor Now it is time to fit a model to the data here to explore if there indeed is a difference in the population. We know descriptively there is a difference in the two group means and medians, but is this difference large enough to be practical? The model is fitted similar to before with the lm() function and a similar formula as before. The outcome (birth weight) is to the left of the ~ and the predictor (maternal smoking status) is to the right. smoker_reg &lt;- lm(birth_weight ~ maternal_smoker, data = baby) coef(smoker_reg) ## (Intercept) maternal_smokerTRUE ## 123.085315 -9.266143 To explore what these coefficients mean in a bit more detail, let’s look at the data a bit more. baby &lt;- baby %&gt;% mutate(smoker = ifelse(maternal_smoker, 1, 0)) head(baby) ## [90m# A tibble: 6 x 7[39m ## birth_weight gestational_days maternal_age maternal_height maternal_pregna… ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m 120 284 27 62 100 ## [90m2[39m 113 282 33 64 135 ## [90m3[39m 128 279 28 64 115 ## [90m4[39m 108 282 23 67 125 ## [90m5[39m 136 286 25 62 93 ## [90m6[39m 138 244 33 62 178 ## [90m# … with 2 more variables: maternal_smoker [3m[90m&lt;lgl&gt;[90m[23m, smoker [3m[90m&lt;dbl&gt;[90m[23m[39m Instead of using the maternal_smoker attribute, instead let’s run the model with the smoker attribute. smoker_reg_new &lt;- lm(birth_weight ~ smoker, data = baby) coef(smoker_reg_new) ## (Intercept) smoker ## 123.085315 -9.266143 Notice that the coefficients for the linear regression are the same no matter which attribute is entered into the model. When a categorical attribute is entered into the regression in R, the attribute is automatically converted into something called an indicator or dummy variable. This means that one of the two values are represented with a 1, the other with a 0. The value that is represented with a 0 is the one that is closer to the letter “A”, meaning that the 0 is the first category in alphabetical order. To again get a better grasp, the descriptive stats and the coefficients from the regression are shown together below. baby %&gt;% df_stats(birth_weight ~ maternal_smoker, mean, sd, median, quantile(c(0.25, 0.75)), length) ## response maternal_smoker mean sd median 25% 75% length ## 1 birth_weight FALSE 123.0853 17.42370 123 113 134 715 ## 2 birth_weight TRUE 113.8192 18.29501 115 101 126 459 coef(smoker_reg) ## (Intercept) maternal_smokerTRUE ## 123.085315 -9.266143 7.2.3 Inference Similar to the continuous predictor, resampling/bootstrap takes a similar method in the case with a single categorical predictor. In order to get some sense of the amount of error in the estimate of the linear slope here, a bootstrap can be done to provide some evidence of the likely range of slope values. The bootstrap will take the following general steps: Resample the observed data available, with replacement Fit the same linear regression model as above. Save the slope coefficient representing the relationship between birth weight and gestational days Repeat steps 1 - 3 many times Explore the distribution of slope estimates from the many resampled data sets. resample_baby &lt;- function(...) { baby_resample &lt;- baby %&gt;% sample_n(nrow(baby), replace = TRUE) baby_resample %&gt;% lm(birth_weight ~ maternal_smoker, data = .) %&gt;% coef(.) %&gt;% .[2] %&gt;% data.frame() } resample_baby() ## . ## maternal_smokerTRUE -10.9041 Now that there is a function that does steps 1 - 3, these processes can now be repeated many times. baby_coef &lt;- map(1:10000, resample_baby) %&gt;% bind_rows() names(baby_coef) &lt;- &#39;slope&#39; gf_density(~ slope, data = baby_coef) baby_coef %&gt;% df_stats(~ slope, quantile(c(0.05, 0.5, 0.95))) ## response 5% 50% 95% ## 1 slope -10.98238 -9.248666 -7.522132 7.3 More than 2 categorical groups Before the model contained one attribute that represented two groups. What happens when there are more than two groups for an attribute? To explore this, the college scorecard data will be used again. college_score &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000) ## Parsed with column specification: ## cols( ## instnm = [31mcol_character()[39m, ## city = [31mcol_character()[39m, ## stabbr = [31mcol_character()[39m, ## preddeg = [31mcol_character()[39m, ## region = [31mcol_character()[39m, ## locale = [31mcol_character()[39m, ## adm_rate = [32mcol_double()[39m, ## actcmmid = [32mcol_double()[39m, ## ugds = [32mcol_double()[39m, ## costt4_a = [32mcol_double()[39m, ## costt4_p = [32mcol_double()[39m, ## tuitionfee_in = [32mcol_double()[39m, ## tuitionfee_out = [32mcol_double()[39m, ## debt_mdn = [32mcol_double()[39m, ## grad_debt_mdn = [32mcol_double()[39m, ## female = [32mcol_double()[39m, ## bachelor_degree = [32mcol_double()[39m ## ) head(college_score) ## [90m# A tibble: 6 x 17[39m ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m Alaba… Norm… AL Bachel… South… City:… 0.903 18 [4m4[24m824 [4m2[24m[4m2[24m886 ## [90m2[39m Unive… Birm… AL Bachel… South… City:… 0.918 25 [4m1[24m[4m2[24m866 [4m2[24m[4m4[24m129 ## [90m3[39m Unive… Hunt… AL Bachel… South… City:… 0.812 28 [4m6[24m917 [4m2[24m[4m2[24m108 ## [90m4[39m Alaba… Mont… AL Bachel… South… City:… 0.979 18 [4m4[24m189 [4m1[24m[4m9[24m413 ## [90m5[39m The U… Tusc… AL Bachel… South… City:… 0.533 28 [4m3[24m[4m2[24m387 [4m2[24m[4m8[24m836 ## [90m6[39m Aubur… Mont… AL Bachel… South… City:… 0.825 22 [4m4[24m211 [4m1[24m[4m9[24m892 ## [90m# … with 7 more variables: costt4_p [3m[90m&lt;dbl&gt;[90m[23m, tuitionfee_in [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# tuitionfee_out [3m[90m&lt;dbl&gt;[90m[23m, debt_mdn [3m[90m&lt;dbl&gt;[90m[23m, grad_debt_mdn [3m[90m&lt;dbl&gt;[90m[23m, female [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bachelor_degree [3m[90m&lt;dbl&gt;[90m[23m[39m 7.3.1 Explore distribution 3 groups Early in the course, the distribution of admission rates by the primary degree that the institution grants was explored. Below is a violin plot that shows these three distributions. gf_violin(adm_rate ~ preddeg, data = college_score, fill = &#39;gray85&#39;, size = 1, draw_quantiles = c(0.1, 0.5, 0.9)) There may be some small differences between these groups, but more formally we can test this to understand the amount of uncertainty in the average of the distributions. This again will make use of the lm() function in R and the formula is very similar to what was done before and mimics the formula from the violin plot above. adm_model &lt;- lm(adm_rate ~ preddeg, data = college_score) coef(adm_model) ## (Intercept) preddegBachelor Degree preddegCertificate Degree ## 0.72296993 -0.05170254 0.02193828 Guesses as to what these coefficients represent? How were the categorical groups turned into the different elements in the model? 7.3.2 Overall model fit There is a measure of overall model fit that is commonly used in the research literature for linear regression models, called R-squared. R-squared represents the proportion of variation in the outcome that is explained by the attributes in the model. The statistic ranges from 0 to 1 where values closer to 1 indicate larger percentages of variation explained. This can be extracted from the model directly. summary(adm_model)$r.squared ## [1] 0.01404376 Another one can be computed from the baby data where the birth weight was the outcome and gestational days was the primary attribute used as a predictor. baby_reg &lt;- lm(birth_weight ~ gestational_days, data = baby) summary(baby_reg)$r.squared ## [1] 0.1660911 For models with a single predictor variable, R-squared is the correlation coefficient squared. For example: cor(birth_weight ~ gestational_days, data = baby) ^ 2 ## [1] 0.1660911 7.4 Multiple Regression What happens if we would like to combine the two predictors? Shown above is that the number of gestational days has a moderate relationship to the baby weight, therefore exploring the effects of smoking, it would be nice to remove the effect of gestational days from the baby weight. More specifically, this essentially allows us to make comparisons on the effect of smoking for the same gestational days. One way to think about this is through conditional means. Exploration of these visually first can be particularly helpful. gf_point(birth_weight ~ gestational_days, data = baby, size = 3) %&gt;% gf_smooth() %&gt;% gf_facet_wrap(~ maternal_smoker) ## `geom_smooth()` using method = &#39;loess&#39; baby_reg_smoker &lt;- lm(birth_weight ~ I(gestational_days - mean(gestational_days)) + maternal_smoker, data = baby) coef(baby_reg_smoker) ## (Intercept) ## 122.7366688 ## I(gestational_days - mean(gestational_days)) ## 0.4511679 ## maternal_smokerTRUE ## -8.3743990 We can write out the regression equation similar to before: \\[\\begin{equation} birth\\_weight = 122.67 + 0.49 (gestational\\_days - mean(gestational\\_days) - 8.17 maternal\\_smoker + \\epsilon \\end{equation}\\] Let’s explore how these are interpreted. ### Distribution of Effects Similar to before, the distribution of effects can be obtained with the following steps: Resample the observed data available, with replacement Estimate linear model coefficients. Save terms of interest Repeat steps 1 - 3 many times Explore the distribution of median differences from the many resampled data sets. resample_baby &lt;- function(...) { baby_resample &lt;- baby %&gt;% sample_n(nrow(baby), replace = TRUE) baby_resample %&gt;% lm(birth_weight ~ I(gestational_days - mean(gestational_days)) + maternal_smoker, data = .) %&gt;% tidy(.) %&gt;% select(term, estimate) } resample_baby() ## [90m# A tibble: 3 x 2[39m ## term estimate ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m (Intercept) 122. ## [90m2[39m I(gestational_days - mean(gestational_days)) 0.503 ## [90m3[39m maternal_smokerTRUE -[31m6[39m[31m.[39m[31m79[39m coef_baby &lt;- map(1:10000, resample_baby) %&gt;% bind_rows() coef_baby %&gt;% gf_density(~ estimate) %&gt;% gf_facet_wrap(~ term, scales = &#39;free&#39;) 7.4.1 Interactions One additional idea that can be quite powerful is the idea of interactions. This was indirectly shown earlier in the course with classification and regression trees, where the models after each split re-evaluated which attributes were most helpful. In this way, the same attribute could be used in different places with different scores identifying the split. A similar idea can be explored in the regression framework, where the idea is that there are differential effects for different groups. This can be shown visually: gf_point(birth_weight ~ gestational_days, data = baby, size = 3) %&gt;% gf_smooth() %&gt;% gf_facet_wrap(~ maternal_smoker) ## `geom_smooth()` using method = &#39;loess&#39; baby_reg_int &lt;- lm(birth_weight ~ I(gestational_days - mean(gestational_days)) * maternal_smoker, data = baby) coef(baby_reg_int) ## (Intercept) ## 122.799690 ## I(gestational_days - mean(gestational_days)) ## 0.369615 ## maternal_smokerTRUE ## -8.257707 ## I(gestational_days - mean(gestational_days)):maternal_smokerTRUE ## 0.230846 resample_baby &lt;- function(...) { baby_resample &lt;- baby %&gt;% sample_n(nrow(baby), replace = TRUE) baby_resample %&gt;% lm(birth_weight ~ I(gestational_days - mean(gestational_days)) * maternal_smoker, data = .) %&gt;% tidy(.) %&gt;% select(term, estimate) } resample_baby() ## [90m# A tibble: 4 x 2[39m ## term estimate ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m (Intercept) 123. ## [90m2[39m I(gestational_days - mean(gestational_days)) 0.331 ## [90m3[39m maternal_smokerTRUE -[31m8[39m[31m.[39m[31m40[39m ## [90m4[39m I(gestational_days - mean(gestational_days)):maternal_smokerTRUE 0.265 coef_baby &lt;- map(1:10000, resample_baby) %&gt;% bind_rows() coef_baby %&gt;% gf_density(~ estimate) %&gt;% gf_facet_wrap(~ term, scales = &#39;free&#39;) 7.4.2 Evaluating model fit As discussed earlier, R-square is a measure of overall model fit. These can be compared across the different models to see which one may be doing the best and explaining the most variation in the baby’s birth weight. summary(baby_reg)$r.square ## [1] 0.1660911 summary(smoker_reg)$r.square ## [1] 0.06091 summary(baby_reg_smoker)$r.square ## [1] 0.215661 summary(baby_reg_int)$r.square ## [1] 0.2249173 "],
["prediction-for-individuals.html", "Chapter 8 Prediction for individuals 8.1 Comparison of classification / linear model 8.2 Compared linear model with median", " Chapter 8 Prediction for individuals 8.1 Comparison of classification / linear model 8.2 Compared linear model with median 8.2.1 Skewed Data - Inference In one example, a skewed distribution was transformed prior to conducting the analysis with a regression tree. Another approach could be to use a more robust statistic such as the median. One limitation of the median, is that a linear regression model as we have covered so far, does not allow you to fit the model while using the median. library(tidyverse) library(ggformula) library(mosaic) library(broom) library(statthink) # Set theme for plots theme_set(theme_statthinking()) college_score &lt;- read_csv(&quot;https://raw.githubusercontent.com/lebebr01/statthink/master/data-raw/College-scorecard-clean.csv&quot;, guess_max = 10000) head(college_score) ## [90m# A tibble: 6 x 17[39m ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m Alaba… Norm… AL Bachel… South… City:… 0.903 18 [4m4[24m824 [4m2[24m[4m2[24m886 ## [90m2[39m Unive… Birm… AL Bachel… South… City:… 0.918 25 [4m1[24m[4m2[24m866 [4m2[24m[4m4[24m129 ## [90m3[39m Unive… Hunt… AL Bachel… South… City:… 0.812 28 [4m6[24m917 [4m2[24m[4m2[24m108 ## [90m4[39m Alaba… Mont… AL Bachel… South… City:… 0.979 18 [4m4[24m189 [4m1[24m[4m9[24m413 ## [90m5[39m The U… Tusc… AL Bachel… South… City:… 0.533 28 [4m3[24m[4m2[24m387 [4m2[24m[4m8[24m836 ## [90m6[39m Aubur… Mont… AL Bachel… South… City:… 0.825 22 [4m4[24m211 [4m1[24m[4m9[24m892 ## [90m# … with 7 more variables: costt4_p [3m[90m&lt;dbl&gt;[90m[23m, tuitionfee_in [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# tuitionfee_out [3m[90m&lt;dbl&gt;[90m[23m, debt_mdn [3m[90m&lt;dbl&gt;[90m[23m, grad_debt_mdn [3m[90m&lt;dbl&gt;[90m[23m, female [3m[90m&lt;dbl&gt;[90m[23m,[39m ## [90m# bachelor_degree [3m[90m&lt;dbl&gt;[90m[23m[39m adm_model &lt;- lm(adm_rate ~ preddeg, data = college_score) coef(adm_model) ## (Intercept) preddegBachelor Degree preddegCertificate Degree ## 0.72296993 -0.05170254 0.02193828 Prior to doing the median, we can bootstrap the mean difference from the model above. resample_admrate &lt;- function(...) { college_resample &lt;- college_score %&gt;% sample_n(nrow(college_score), replace = TRUE) college_resample %&gt;% lm(adm_rate ~ preddeg, data = .) %&gt;% tidy(.) %&gt;% select(term, estimate) } resample_admrate() ## [90m# A tibble: 3 x 2[39m ## term estimate ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m (Intercept) 0.706 ## [90m2[39m preddegBachelor Degree -[31m0[39m[31m.[39m[31m0[39m[31m25[4m1[24m[39m ## [90m3[39m preddegCertificate Degree 0.037[4m2[24m admrate_coef &lt;- map(1:10000, resample_admrate) %&gt;% bind_rows() admrate_coef %&gt;% gf_density(~ estimate) %&gt;% gf_facet_wrap(~ term, scales = &#39;free_x&#39;) 8.2.2 Bootstrap Median he bootstrap for the median will take much of a similar process as before, the major difference being that a model will not be fitted. Instead, we will compute statistics for the median of each group, take differences of the median to represent the median difference between the groups and then replicate. Resample the observed data available, with replacement Estimate median for each group. Calculate median difference between the groups Repeat steps 1 - 3 many times Explore the distribution of median differences from the many resampled data sets. resample_admrate_median &lt;- function(...) { college_resample &lt;- college_score %&gt;% sample_n(nrow(college_score), replace = TRUE) med_est &lt;- college_resample %&gt;% df_stats(adm_rate ~ preddeg, median) %&gt;% pivot_wider(names_from = preddeg, values_from = median_adm_rate) names(med_est) &lt;- c(&quot;Associate&quot;, &quot;Bachelor&quot;, &quot;Certificate&quot;) med_est %&gt;% mutate(bachelor_associate = Bachelor - Associate, certificate_associate = Certificate - Associate, bachelor_certificate = Bachelor - Certificate) %&gt;% pivot_longer(Associate:bachelor_certificate, names_to = &quot;Term&quot;, values_to = &quot;Median_Difference&quot;) } resample_admrate_median() admrate_median &lt;- map(1:10000, resample_admrate_median) %&gt;% bind_rows() admrate_median %&gt;% filter(Term %in% c(&#39;bachelor_associate&#39;, &#39;certificate_associate&#39;, &#39;bachelor_certificate&#39;)) %&gt;% gf_density(~ Median_Difference) %&gt;% gf_facet_wrap(~ Term, scales = &#39;free_x&#39;) "]
]
